[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "The major United States public transit systems contribute significantly to individuals daily transportation, allowing commuters alternative and often more affordable transit options for commutes. The goal of this project is to explore different characteristics of the major transit systems in the United States, based on data from the National Transit Database. More specifically, the analysis includes data from the 2022 Fare Revenue table, the most recent Monthly Ridership table, and the 2022 Operating Expenses reports. This report intends to consider various statistics including farebox recovery, ridership, total trips, total vehicle miles traveled, total revenues and expenses for various locations and transit agencies, to analyze the performance of these transit systems over time. Ultimately, the analysis of certain metrics will assist in determining the most efficient transit system in the United States."
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "The major United States public transit systems contribute significantly to individuals daily transportation, allowing commuters alternative and often more affordable transit options for commutes. The goal of this project is to explore different characteristics of the major transit systems in the United States, based on data from the National Transit Database. More specifically, the analysis includes data from the 2022 Fare Revenue table, the most recent Monthly Ridership table, and the 2022 Operating Expenses reports. This report intends to consider various statistics including farebox recovery, ridership, total trips, total vehicle miles traveled, total revenues and expenses for various locations and transit agencies, to analyze the performance of these transit systems over time. Ultimately, the analysis of certain metrics will assist in determining the most efficient transit system in the United States."
  },
  {
    "objectID": "mp01.html#data-cleaning",
    "href": "mp01.html#data-cleaning",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nBefore starting the analysis, the relevant data files need to be loaded and cleaned into data frames on R. Below consists of the code required to download the data files and create the relevant data frames by merging different tables. From the original data files four data frames are created: FARES, EXPENSES, TRIPS, and MILES. Following this, FARES and EXPENSES are merged into the FINANCIALS data frame and TRIPS and MILES are merged into the USAGE data frame. From this point forward, only the USAGE and FINANCIALS data frames will be necessary to conduct the analysis.\n\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(dplyr)\n\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Agency, City, State, NTD ID, Organization Type, Reporter Type, UZA...\ndbl  (2): Report Year, UACE Code\nnum (10): Primary UZA Population, Agency VOMS, Mode VOMS, Vehicle Operations...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe code below checks for possible lingering null values in the USAGE and FINANCIALS data frames before conducting analysis. An output of 0 for each of the data frames ensures that there are no null values in these data frames.\n\nUSAGE |&gt;\n  is.na() |&gt;\n  sum()\n\n[1] 0\n\n\n\nFINANCIALS |&gt;\n  is.na() |&gt;\n  sum()\n\n[1] 0\n\n\n\nSince, both of these chunks produced an output of 0, there are no null values in these data tables that may alter calculations and analysis in the future.\n\n\nThe USAGE table (shown below) provides transit system information including (but not limited to): agency, mode of transportation, total unlinked passenger trips, and total vehicle revenue miles.\n\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  arrange(`NTD ID`, month) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))"
  },
  {
    "objectID": "mp01.html#data-transformation",
    "href": "mp01.html#data-transformation",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Data Transformation",
    "text": "Data Transformation\nPrior to analyzing data, it is important to make data tables as clear as possible to ensure clarity and accurate interpretation of results. So, renaming the column UZA Name to metro_area clarifies that this column contains the location (city, state) of the transit systems.\n\nUSAGE &lt;- USAGE |&gt; \n  rename(metro_area = 'UZA Name')\n\nAdditionally, the Mode column contains some abbreviated codes, so converting these codes into understandable terms is helpful for future analysis.\n\n\n\n\n\n\nNote\n\n\n\n\nThe code below is used help to streamline the process of converting the codes into the terms, by showing the unique codes in the Mode column. Ultimately making the process of searching for these terms in the National Transit Database (NTD) Glossary much more efficient.\n\nunique(USAGE$Mode)\n\n [1] \"DR\" \"FB\" \"MB\" \"SR\" \"TB\" \"VP\" \"CB\" \"RB\" \"LR\" \"YR\" \"MG\" \"CR\" \"AR\" \"TR\" \"HR\"\n[16] \"IP\" \"PB\" \"CC\"\n\n\n\n\n\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway Transit\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nFurthermore, to clean up the data table even more, the removal of columns NTD ID and 3 Mode could help clear unnecessary information before analysis. Renaming certain columns like UPT and VRM to unlinked_passenger_trips and vehicle_revenue_miles will also provide more clarity for interpretation. For this, a new data table USAGE_clean is created, shown below.\n\nUSAGE_clean &lt;- USAGE |&gt; \n  select(-`NTD ID`, -`3 Mode`) |&gt; \n  rename(unlinked_passenger_trips = UPT, vehicle_revenue_miles = VRM)\n\nsample_n(USAGE_clean, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable(rownames = FALSE,\n                  options = list(pageLength = 5))"
  },
  {
    "objectID": "mp01.html#ridership-analysis",
    "href": "mp01.html#ridership-analysis",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Ridership Analysis",
    "text": "Ridership Analysis\n\n\n\n\n\n\nPreliminary Analysis:\n\n\n\nBelow are some preliminary questions to explore the USAGE_clean data table:\n\nWhat transit agency had the most total VRM in our data set?\nWhat transit mode had the most total VRM in our data set?\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n\n\nQuestion #1\n\ntransit_agency_max_total_VRM &lt;- \n  USAGE_clean |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(Agency)\n\ntransit_agency_max_total_VRM\n\n[1] \"MTA New York City Transit\"\n\ntransit_agency_max_total_VRM_dist &lt;- \n  USAGE_clean |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(total_VRM)\n\ntransit_agency_max_total_VRM_dist\n\n[1] 10832855350\n\n\nThe transit agency that had the most total vehicle revenue miles in the sample was MTA New York City Transit, with a total of about 10.83 billion vehicle revenue miles. Being that the MTA New York City Transit is one of the largest transit systems in the entire world and the largest in the US, it is unsurprising that this transit agency had the most total VRM in the data set.\n\n\nQuestion #2\n\ntransit_mode_max_total_VRM &lt;- \n  USAGE_clean |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(Mode)\n\ntransit_mode_max_total_VRM\n\n[1] \"Bus\"\n\ntransit_mode_max_total_VRM_dist &lt;- \n  USAGE_clean |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(total_VRM)\n\ntransit_mode_max_total_VRM_dist\n\n[1] 49444494088\n\n\nThe transit mode with the most total vehicle revenue miles in this sample was the Bus, having a total of roughly 49.44 billion vehicle revenue miles. Buses make up a large portion of public transportation, especially in cities that don’t have heavy rail trains like NYC, so it is reasonable that buses had the most vehicle revenue miles.\n\n\nQuestion #3\n\nif(!require(\"lubridate\")) install.packages(\"lubridate\")\nlibrary(lubridate)\n\ntotal_trips_NYC_subway_2024 &lt;- USAGE_clean |&gt;\n  filter(Agency == \"MTA New York City Transit\", \n         Mode == \"Heavy Rail\", \n         year(month) == 2024) |&gt;\n  group_by(mon = month(month)) |&gt;\n  summarize(total_trips = sum(unlinked_passenger_trips))\n\ntotal_trips_NYC_subway_2024\n\n# A tibble: 7 × 2\n    mon total_trips\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1   157917292\n2     2   155722143\n3     3   172235628\n4     4   171458466\n5     5   180458819\n6     6   159891215\n7     7   161133245\n\ntotal_trips_NYC_subway &lt;- total_trips_NYC_subway_2024 |&gt;\n  filter(mon == 5) |&gt;\n  pull(total_trips)\n\nThere were about 180.46 million total trips taken on the New York City Subway in May 2024. From the data table total_trips_NYC_subway_2024, it is evident that the average monthly ridership from January 2024 to April 2024, was less than the total trips in May 2024. This could be due to the season change, as the weather gets warmer from Winter into the Spring.\n\n\nQuestion #4\n\nridership_drop_NYC_april &lt;- USAGE_clean |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month(month)==4) |&gt;\n  filter((year(month)==2019) | (year(month)==2020)) |&gt;\n  group_by(year = year(month)) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  arrange(year) |&gt;\n  summarize(ridership_drop = first(total_ridership)-last(total_ridership)) |&gt;\n  pull(ridership_drop)\n\nridership_drop_NYC_april\n\n[1] 211969660\n\n\nFrom April 2019 to April 2020, the NYC subway ridership fell by approxmately 211.97 million. The drastic drop in NYC subway ridership was likely due to the COVID-19 global pandemic which forced everyone to remain indoors and socially distance from others. So, it is understandable that such an extreme drop of 211.97 million occurred.\nAfter analyzing the above statistics from the monthly ridership table, I wanted to find some additional information on the possible impact of COVID-19 on transit system ridership in the US.\n\n\nTransit System Patterns Before and During COVID-19 Analysis\nSeveral years ago, in 2020, the world experienced a global outbreak of COVID-19, impacting local and international economies. This event encouraged social distancing and prevented any significant travel for citizens besides essential workers. I was curious to see how our national transit systems were affected by this global event. More specifically, I wanted to explore the ridership changes for the various metro areas included in the national ridership data set. I created a data frame with the total ridership of each metro area in this time frame, then calculated the percent change of ridership from 2019 to 2020. I expected to find a decrease in ridership in all the cities from 2019 to 2020 and wanted to explore which cities were negatively impacted the most and least.\n\nridership_by_area_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_by_area_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_2019_2020 &lt;- left_join(ridership_by_area_2019, ridership_by_area_2020, join_by(metro_area)) |&gt;\n  rename(total_ridership_2019 = total_ridership.x, total_ridership_2020 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2020-total_ridership_2019, percent_change = round(((change_in_ridership / total_ridership_2019) * 100), digits = 2), decrease = (percent_change &lt; 0))\n\n\nridership_2019_2020 |&gt; \n    DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nGreatest Decrease in Ridership from 2019-2020\n\ngreatest_ridership_drop_place &lt;- ridership_2019_2020 |&gt;\n  slice_min(percent_change, n=1) |&gt;\n  pull(metro_area)\n\ngreatest_ridership_drop_percent &lt;- ridership_2019_2020 |&gt;\n  slice_min(percent_change, n=1) |&gt;\n  pull(percent_change)\n\ngreatest_ridership_drop_place\n\n[1] \"Rome, GA\"\n\ngreatest_ridership_drop_percent\n\n[1] -91.56\n\n\nAfter looking through the data, I found that the city with the greatest ridership drop from 2019 to 2020 was Rome, GA with a percentage change of -91.56%.\nFurthermore, I wanted to explore more about the ridership in Rome, GA and analyze the breakdown of ridership by mode of transportation.\n\nridership_Rome_GA_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, metro_area == \"Rome, GA\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_Rome_GA_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, metro_area == \"Rome, GA\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_Rome_GA_2019_2020 &lt;- left_join(ridership_Rome_GA_2019, ridership_Rome_GA_2020, join_by(Mode)) |&gt;\n  rename(total_ridership_2019 = total_ridership.x, total_ridership_2020 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2020-total_ridership_2019, percent_change = round(((change_in_ridership / total_ridership_2019) * 100), digits = 2), decrease = (percent_change &lt; 0))\n\nridership_Rome_GA_2019_2020 |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\nFrom this table above, there were only two recorded modes of transportation from Rome, GA: bus and demand response. The bus ridership changed by -92.75%, while the demand response ridership changed by -36.57%. The sharp decline in bus ridership could be due to the fear of taking public mass transit. Individuals needing to travel locally would probably have preferred utilizing personal vehicles. Overall, the decrease in ridership was typical for majority of metropolitan areas with transit systems.\n\n\nExploring Peculiar Increases in Ridership from 2019-2020\nSubsequently, I was also curious to see which city was affected the least. When exploring this question, contrary to my assumption, there were a few cities that actually increased their ridership from 2019 to 2020.\n\npositive_change &lt;- ridership_2019_2020 |&gt;\n  filter(decrease == FALSE) |&gt;\n  arrange(desc(percent_change))\n\npositive_change_cities &lt;- positive_change$metro_area\n\npositive_change_cities\n\n[1] \"Victoria, TX\"       \"Port St. Lucie, FL\" \"Las Cruces, NM\"    \n\n\nThe cities that presumably increased their ridership from 2019 to 2020 were Victoria, TX, Port St. Lucie, FL, Las Cruces, NM.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough finding an increase in ridership from 2019 to 2020 is not entirely impossible, it was probably unlikely to occur. So, it was important to do additional investigation to figure out why these calculations came out to be positive.\n\n\nUpon further review of the data for Victoria, TX, it is apparent that no data was collected between January 2019 to August 2019, causing the total ridership for 2019 to be significantly lower than the totals from 2020.\n\ntx_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, \n         metro_area == \"Victoria, TX\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\ntx_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, \n         metro_area == \"Victoria, TX\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\ntx_combined &lt;- full_join(tx_2019, tx_2020) |&gt;\n  mutate(year = year(month),\n         mon = month(month)) |&gt;\n  select(-month) |&gt;\n  pivot_wider(id_cols = c(mon),\n              names_from = year,\n              values_from = total_UPT) |&gt;\n  arrange(mon)\n\nJoining with `by = join_by(month, total_UPT)`\n\ntx_combined |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\ntx_2019_total &lt;- tx_combined$`2019` |&gt;\n  replace(is.na(tx_combined$`2019`), 0) |&gt;\n  sum()\n\ntx_2019_total\n\n[1] 107199\n\ntx_2020_total &lt;- tx_combined$`2020` |&gt;\n  sum()\n\ntx_2020_total\n\n[1] 269979\n\n\nThe total for Victoria, TX in 2019 was 107,199 compared to 2020 with a total of 269,979, a significant difference due to the missing data from 2019. Being that the information for 2019 is not entirely accessible, it is inconclusive whether the ridership in Victoria, TX increased or decreased from 2019 to 2020.\nContrary to Victoria, TX, the data for Las Cruces, NM was not nearly as drastic. Although, there seems to be an unexpected value for the total ridership in January 2019.\n\nnm_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, \n         metro_area == \"Las Cruces, NM\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nnm_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, \n         metro_area == \"Las Cruces, NM\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nnm_combined &lt;- full_join(nm_2019, nm_2020) |&gt;\n  mutate(year = year(month),\n         mon = month(month)) |&gt;\n  select(-month) |&gt;\n  pivot_wider(id_cols = c(mon),\n              names_from = year,\n              values_from = total_UPT)\n\nJoining with `by = join_by(month, total_UPT)`\n\nnm_combined |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\njan_2019_nm &lt;- nm_combined |&gt;\n  filter(mon == 1) |&gt;\n  pull(`2019`)\n\nnm_combined_no_jan &lt;- nm_combined |&gt;\n  filter(mon != 1)\n\nnm_2019_min &lt;- nm_combined_no_jan$`2019` |&gt;\n  min()\n\nnm_2019_max &lt;- nm_combined_no_jan$`2019` |&gt;\n  max()\n\nFrom the table above, Las Cruces, NM, experienced a total ridership of 532 in January 2019, which seems highly unlikely given that the range of values for the remainder of 2019 were between 4,510 and 9,544. Observing the remaining data points, there’s a general increase in ridership from January 2019 until about January 2020. The Las Cruces, NM metro area experiences a steep ridership drop in April 2020, right around the peak of the pandemic. Overall, it seems that the ridership from January 2019 may have been undercounted, so it is unclear whether Las Cruces, NM experienced an increase or decrease in ridership from 2019 to 2020.\nLastly, for Port St. Lucie, FL, there is less of a concern for the 2019 data as the monthly ridership totals seem to fluctuate in the range of 14,000 to 18,000. However, in 2020, there is a noticeable increase in ridership in October 2020.\n\nfl_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, \n         metro_area == \"Port St. Lucie, FL\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nfl_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, \n         metro_area == \"Port St. Lucie, FL\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nfl_combined &lt;- full_join(fl_2019, fl_2020) |&gt;\n  mutate(year = year(month),\n         mon = month(month)) |&gt;\n  select(-month) |&gt;\n  pivot_wider(id_cols = c(mon),\n              names_from = year,\n              values_from = total_UPT)\n\nJoining with `by = join_by(month, total_UPT)`\n\nfl_combined |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nsept_2020_fl &lt;- fl_combined |&gt;\n  filter(mon == 9) |&gt;\n  pull(`2020`)\n\noct_2020_fl &lt;- fl_combined |&gt;\n  filter(mon == 10) |&gt;\n  pull(`2020`)\n\nfl_increase &lt;- oct_2020_fl - sept_2020_fl\n\nfl_increase_percent &lt;- round((fl_increase / sept_2020_fl) * 100, digits = 0)\n\nIn September 2020, Port St. Lucie, FL experienced a ridership total of 6,883. In only one month, Port St. Lucie, FL increased its ridership by 43,535 (or 633%) to a total ridership in October 2020 of 50,418. While this seems a little unbelievable, it is possible that during the latter part of 2020, people were beginning to travel domestically again. Perhaps more people were fleeing bigger urban areas for smaller cities like Port St. Lucie, FL causing a spike in ridership in October 2020 and beyond. This case seems more reasonable, especially since the ridership in Port St. Lucie, FL after October 2020 maintained this high total ridership fluctuating between 46,000 and 52,000 UPT.\nAlthough in most cases, it is unlikely that ridership would have increased from 2019 to 2020, it is possible that certain, more suburban, areas could have had an increase in ridership. One possibility is that people wanted to relocate from more densely populated cities into smaller cities, when travel became more accessible, causing an increase in ridership in cities like Port St. Lucie, FL.\n\n\nNew York City Transit During COVID-19\nAside from the above three cities, the large majority of US city transit systems experienced a decrease in ridership. Being that New York City has the largest transit system in the United States, the Metropolitan Transit Authority (MTA), I was curious to further analyze the changes the city’s transit system experienced from 2019 to 2020.\n\nridership_NYC_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_NYC_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_NYC_2019_2020 &lt;- left_join(ridership_NYC_2019, ridership_NYC_2020, join_by(Mode)) |&gt;\n  rename(total_ridership_2019 = total_ridership.x, total_ridership_2020 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2020-total_ridership_2019, percent_change = round(((change_in_ridership / total_ridership_2019) * 100), digits = 2), decrease = (percent_change &lt; 0))\n\n\nridership_NYC_2019_2020 |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nnyc_change &lt;- round((sum(ridership_NYC_2019_2020$change_in_ridership) / sum(ridership_NYC_2019_2020$total_ridership_2019)) * 100, digits = 0)\n\nnyc_change\n\n[1] -56\n\n\nAs expected, all 5 modes of transportation (bus, bus rapid transit, commuter bus, demand response, and heavy rail) contributed to the overall -56% change in ridership in NYC from 2019 to 2020. During this time, many companies transitioned to remote work allowing employees to work from home and all students transitioned to virtual learning, avoiding travel and public transportation. Additionally, the fear of contracting and spreading the virus led more individuals who needed to travel to use personal vehicles rather than public transport. All of these factors contributed to the severe decrease in ridership in NYC’s transit system in 2020.\n\nridership_by_area_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_by_area_2023 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2023) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_2020_2023 &lt;- left_join(ridership_by_area_2020, ridership_by_area_2023, join_by(metro_area)) |&gt;\n  rename(total_ridership_2020 = total_ridership.x, total_ridership_2023 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2023-total_ridership_2020, percent_change = round(((change_in_ridership / total_ridership_2020) * 100), digits = 2), increase = (percent_change &gt;= 0))\n\nridership_2020_2023 |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\npercentage_positive &lt;- ridership_2020_2023 |&gt;\n  drop_na() |&gt;\n  mutate(total = n()) |&gt;\n  group_by(increase, total) |&gt;\n  summarize(count = n()) |&gt;\n  mutate(percent = round((count / total)*100, digits = 0)) |&gt;\n  filter(increase == TRUE) |&gt;\n  pull(percent)\n\n`summarise()` has grouped output by 'increase'. You can override using the\n`.groups` argument.\n\npercentage_positive\n\n[1] 91\n\n\nOverall, it was interesting to observe the varying effects that COVID-19 had on US transit systems across different metropolitan areas. Each city reacted and adjusted to the global pandemic differently, which led to different patterns of changes in their respective transit systems. Although most transit systems experienced a severe drop in ridership during 2020, it is evident that most transit systems have progressed in the right direction, with approximately 91% of cities demonstrating an increase in ridership from 2020 to 2023."
  },
  {
    "objectID": "mp01.html#financial-and-ridership-analysis-major-transit-systems",
    "href": "mp01.html#financial-and-ridership-analysis-major-transit-systems",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "2022 Financial and Ridership Analysis (Major Transit Systems)",
    "text": "2022 Financial and Ridership Analysis (Major Transit Systems)\nSince the focus of the remaining analysis is on the financials and ridership of the US major transit systems in 2022, the table USAGE must be filtered for data in 2022 and then merged with the FINANCIALS table from earlier. Additionally, to filter for major transit systems, the new table USAGE_AND_FINANCIALS is filtered to only contain data with total unlinked passenger trips greater than 400,000.\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarize(UPT = sum(UPT), VRM = sum(VRM)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'metro_area'. You can\noverride using the `.groups` argument.\n\n\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway Transit\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()\n\n\nUSAGE_AND_FINANCIALS_major_transit &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000)\n\nUSAGE_AND_FINANCIALS_major_transit |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\nEfficiency involves optimizing productivity while minimizing expense. In the below analysis, I take a look at various efficiency metrics to determine which United States transit system was the most efficient in 2022.\n\n\n\n\n\n\nEfficiency Analysis:\n\n\n\nBelow are various metrics that can be used to describe the efficiency of transit systems:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\n\n\n\nQuestion #1\n\ntransit_system_most_UPT_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  slice_max(UPT, n = 1) |&gt;\n  pull(Agency)\n\ntransit_system_most_UPT_agency\n\n[1] \"MTA New York City Transit\"\n\ntransit_system_most_UPT_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  slice_max(UPT, n = 1) |&gt;\n  pull(Mode)\n\ntransit_system_most_UPT_mode\n\n[1] \"Heavy Rail\"\n\ntransit_system_most_UPT &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  slice_max(UPT, n = 1) |&gt;\n  pull(UPT)\n  \ntransit_system_most_UPT\n\n[1] 1793073801\n\n\nIn 2022, the transit system with the highest amount of unlinked passenger trips (UPT) of 1.79 billion was the MTA New York City Transit with the mode of transportation of Heavy Rail. Since MTA New York City Transit subway system is the largest in the US, it is clear that they would have the highest UPT among all major US transit systems.\n\n\nQuestion #2\n\nhighest_farebox_recovery_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(farebox_recovery = `Total Fares` / Expenses) |&gt;\n  slice_max(farebox_recovery, n = 1) |&gt;\n  pull(Agency)\n\nhighest_farebox_recovery_agency\n\n[1] \"Port Imperial Ferry Corporation\"\n\nhighest_farebox_recovery_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(farebox_recovery = `Total Fares` / Expenses) |&gt;\n  slice_max(farebox_recovery, n = 1) |&gt;\n  pull(Mode)\n\nhighest_farebox_recovery_mode\n\n[1] \"Ferryboat\"\n\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(farebox_recovery = `Total Fares` / Expenses) |&gt;\n  slice_max(farebox_recovery, n = 1) |&gt;\n  pull(farebox_recovery)\n\nhighest_farebox_recovery\n\n[1] 1.428146\n\n\nThe transit system with the highest farebox recovery of 1.43 (ratio of total fares to expenses) in 2022 was Port Imperial Ferry Corporation with the mode of transportation of Ferryboat. It is interesting to see that the transit system with the highest farebox recovery is not a popular agency or mode most people would attribute “public transportation” as. However, it does show that efficient transit systems can exist anywhere in the US on any mode of transit.\n\n\nQuestion #3\n\nlowest_expenses_per_UPT_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_UPT = Expenses / UPT) |&gt;\n  slice_min(expenses_per_UPT, n = 1) |&gt;\n  pull(Agency)\n\nlowest_expenses_per_UPT_agency\n\n[1] \"North Carolina State University\"\n\nlowest_expenses_per_UPT_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_UPT = Expenses / UPT) |&gt;\n  slice_min(expenses_per_UPT, n = 1) |&gt;\n  pull(Mode)\n\nlowest_expenses_per_UPT_mode\n\n[1] \"Bus\"\n\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_UPT = Expenses / UPT) |&gt;\n  slice_min(expenses_per_UPT, n = 1) |&gt;\n  pull(expenses_per_UPT)\n\nlowest_expenses_per_UPT\n\n[1] 1.17912\n\n\nThe transit system that had the lowest expenses per UPT (1.18) in 2022 was Bus from North Carolina State University. Although we do not have all the information about the cost of university transit, it is likely that such a large university is transporting sizeable amounts of students daily which could contribute to its low expense per UPT.\n\n\nQuestion #4\n\nhighest_fares_per_UPT_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_UPT = `Total Fares` / UPT) |&gt;\n  slice_max(fares_per_UPT, n = 1) |&gt;\n  pull(Agency)\n\nhighest_fares_per_UPT_agency\n\n[1] \"Hampton Jitney, Inc.\"\n\nhighest_fares_per_UPT_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_UPT = `Total Fares` / UPT) |&gt;\n  slice_max(fares_per_UPT, n = 1) |&gt;\n  pull(Mode)\n\nhighest_fares_per_UPT_mode\n\n[1] \"Commuter Bus\"\n\nhighest_fares_per_UPT &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_UPT = `Total Fares` / UPT) |&gt;\n  slice_max(fares_per_UPT, n = 1) |&gt;\n  pull(fares_per_UPT)\n\nhighest_fares_per_UPT\n\n[1] 41.29628\n\n\nHampton Jitney, Inc. with the transit mode of Commuter Bus had the highest total fares per UPT of 41.3 in 2022. Since Hampton Jitney, Inc. is a commuter bus company that provides coach bus transportation, charter bus and tour bus options, it is likely that passengers are charged higher fares, leading to a high total fare per UPT.\n\n\nQuestion #5\n\nlowest_expenses_per_VRM_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_VRM = Expenses / VRM) |&gt;\n  slice_min(expenses_per_VRM, n = 1) |&gt;\n  pull(Agency)\n\nlowest_expenses_per_VRM_agency\n\n[1] \"Metropolitan Transportation Commission\"\n\nlowest_expenses_per_VRM_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_VRM = Expenses / VRM) |&gt;\n  slice_min(expenses_per_VRM, n = 1) |&gt;\n  pull(Mode)\n\nlowest_expenses_per_VRM_mode\n\n[1] \"Vanpool\"\n\nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_VRM = Expenses / VRM) |&gt;\n  slice_min(expenses_per_VRM, n = 1) |&gt;\n  pull(expenses_per_VRM)\n\nlowest_expenses_per_VRM\n\n[1] 0.4449998\n\n\nThe transit system with the lowest expenses per vehicle revenue mile (VRM) of 0.44 in 2022 was Vanpool from Metropolitan Transportation Commission. Since Vanpool is a form of transportation similar to rideshares, transporting a significantly less amount of people than typical mass transit in a smaller vehicle, the expenses required to operate and maintain it is likely significantly less. Thus, it is understandable that this mode of transportation would have the lowest expenses per VRM.\n\n\nQuestion #6\n\nhighest_fares_per_VRM_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_VRM = `Total Fares` / VRM) |&gt;\n  slice_max(fares_per_VRM, n = 1) |&gt;\n  pull(Agency)\n\nhighest_fares_per_VRM_agency\n\n[1] \"Jacksonville Transportation Authority\"\n\nhighest_fares_per_VRM_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_VRM = `Total Fares` / VRM) |&gt;\n  slice_max(fares_per_VRM, n = 1) |&gt;\n  pull(Mode)\n\nhighest_fares_per_VRM_mode\n\n[1] \"Ferryboat\"\n\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_VRM = `Total Fares` / VRM) |&gt;\n  slice_max(fares_per_VRM, n = 1) |&gt;\n  pull(fares_per_VRM)\n\nhighest_fares_per_VRM\n\n[1] 157.7002\n\n\nLastly, Ferryboat from Jacksonville Transportation Authority had the highest total fares per vehicle revenue mile (VRM) of 157.7 in 2022. The Jacksonville Transportation Authority provides Ferryboat transportation not only for passengers but also charges extra for those who want to transport various vehicles which could contribute to the high fares per VRM."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Conclusion",
    "text": "Conclusion\nTransit system efficiency is subjective and can be measured with different metrics (highest UPT, highest farebox recovery, lowest expenses per UPT, highest total fares per UPT, lowest expenses per VRM, and highest total fares per VRM). Based on the above information, using the farebox recovery ratio, I found that the most efficient transit system in the country in 2022 was the Ferryboat from the Port Imperial Ferry Corporation. This transit system was able to maximize its total fares while minimizing their expenses leading to the highest farebox recovery ratio of 1.43.\nOverall, the National Transit Database provided a lot of useful and insightful data, allowing for a wide range of analysis. More specifically, it was interesting to explore historical transit data leading to thought-provoking insights as well as the various metrics to determine effective and efficient transportation in the United States. As the world continues to accelerate post-COVID, the major US public transit systems will continue to improve and create more efficient means of transportation for commuters around the nation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Elissa",
    "section": "",
    "text": "Hello everyone, my name is Elissa!\nI am a curious graduate student at Baruch College with a passion for problem solving and tackling challenges through new experiences. I’m currently pursuing a Master of Science in Business Analytics with a concentration in Data Analytics in the Zicklin School of Business.\nI am also a recent graduate of Baruch College from the Baruch Scholars Honors program where I obtained a Bachelor of Arts in Mathematics and minored in Communication Studies.\n13 years ago, I discovered my first passion. Basketball has been a large part of my life for as long as I can remember. I have grown significantly since then, adapting to new team environments, learning to become a leader, and overcoming challenges. Four years ago, I was able to live out my dream of playing at the collegiate level for the Baruch Bearcats.\nThree summers ago, I tried something new and different. I had the opportunity of being a Microsoft Data Science Research fellow. Using R, I researched hands-on with large data sets, developed regression models, and created visualizations to analyze data. By the end of this program, I successfully collaborated on a research replication project, analyzing the effects of socioeconomic and mobility factors on Covid-19 positivity rates in New York City. Through this opportunity I found a new passion for data science, a bridge between my interests in mathematics and programming.\nAll these experiences have provided me with foundational skills to foster development in both my personal and professional growth. I hope to continue to explore all my passions and gain new experiences during my pursuit of data science and data analytics careers."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood first launched with the completion of the silent film The Count of Monte Cristo in 1908. Shortly after, Hollywood’s first studio was created, and the rest is history.1 Since then, Hollywood has grown to be synonymous with the entertainment industry. As the film and television industries progressed with the advancement of technology, so did the demands for more creative storytelling and diverse entertainment.\nHollywood development executives are tasked with developing these new and creative movie ideas. Historically, these executives relied on adapting existing material like novels, true stories, or intellectual property (IP). However, this traditional approach has faced criticism by insiders and viewers for its reliance on pre-existing sources. The goal of this project is to develop these data-driven ideas to inspire new movies.\nWe will be utilizing the Internet Movie Database (IMDb) to drive our data-driven insights on the entertainment industry. More specifically, we will use the data sets from the IMDb non-commercial release. With this data, we hope to analyze the significant characteristics of successful films, discover successful filmmakers and actors, and explore unique insights from the entertainment industry."
  },
  {
    "objectID": "mp02.html#initial-exploration",
    "href": "mp02.html#initial-exploration",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Initial Exploration",
    "text": "Initial Exploration\n\nData Cleaning\nBefore beginning any data analysis, it is crucial that the data is cleaned and transformed appropriately to avoid any issues further down the line. Upon first glimpse of the six IMDb data sets, there are columns with incorrect data types. Occasionally when data sets are imported and read in, numeric columns are misread as character (string) data. This is due to the “null” values being represented in a non-standard way. In the case of IMDb, null values are recorded as \\\\N, R does not read this as NA values, misunderstands this and reads these values in as strings.\nSubsequently, these columns need to be adjusted to reflect its quantitative data or logical data for future analysis. To correct this, we can use a combination of the mutate command with the as.numeric (for quantitative values) or as.logical (for factor/level data) command to alter the type of column. Below are the columns I adjusted for each of the data tables from their previous character data type to either logical or numeric.\n\n\nCode\n# NAME_BASICS: birthYear and deathYear need to be corrected to numeric values\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\n# TITLE_BASICS: isAdult needs to be corrected to logical values, while startYear, endYear, and runtimeMinutes need to be corrected to numeric values\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(isAdult = as.logical(isAdult),\n         startYear = as.numeric(startYear),\n         endYear = as.numeric(endYear),\n         runtimeMinutes = as.numeric(isAdult))\n\n# TITLE_EPISODES: seasonNumber and episodeNumber need to be fixed to numeric values\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(seasonNumber = as.numeric(seasonNumber),\n         episodeNumber = as.numeric(episodeNumber))\n\n# TITLE_RATINGS: column types are good\n\n# TITLE_CREW: column types are correct, for character type columns (directors and writers), let's change the '\\\\N' values to the NA values that R understands\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  mutate(directors = na_if(directors, \"\\\\N\"),\n         writers = na_if(writers, \"\\\\N\"))\n\n# TITLE_PRINCIPALS: column types are correct, for character type columns (job, characters), let's change the '\\\\N' values to the NA values that R understands\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  mutate(job = na_if(job, \"\\\\N\"),\n         characters = na_if(characters, \"\\\\N\"))\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are a few columns in some of the data tables that combine several pieces of information into one cell separated with commas (e.g. primaryProfession and knownForTitles in the NAME_BASICS table). In order to break these values into multiple rows, we can use the separate_longer_delim. This will be a useful tool for later when we are taking a closer look at the data.\n\n\n\n\nPreliminary Exploration Questions\nBefore starting any deep-dive analysis, it is important to do some preliminary exploration of our data tables to understand what information we are working with.\n\n\n\n\n\n\nPreliminary Questions\n\n\n\nBelow are some preliminary questions, we would like to answer as an initial exploration of our data tables.\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nWho is the oldest living person in our data set?\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nWhat four projects is the actor Mark Hamill most known for?\nWhat TV series, with more than 12 episodes, has the highest average rating?\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\n\n\nQuestion #1: How many movies are in our data set? How many TV series? How many TV episodes?\nAs a preliminary step, it is useful to take a closer look at the data set to see the amount of data remaining after cleaning such a large data set. More specifically, let’s figure out how many movies, TV series and TV episodes remain in our data set. Before approaching this question, it is important to understand the existing types of titles in our data set.\n\n\nCode\nunique(TITLE_BASICS$titleType)\n\n\n [1] \"short\"        \"movie\"        \"tvSeries\"     \"tvShort\"      \"tvMovie\"     \n [6] \"tvEpisode\"    \"tvMiniSeries\" \"video\"        \"tvSpecial\"    \"videoGame\"   \n\n\nNow that we know how the title types are categorized, we can filter out our data set to determine the count of each type.\n\n\nCode\n# Total titles by title type\n\ntotal_by_title_type &lt;- TITLE_BASICS |&gt;\n  group_by(titleType) |&gt;\n  summarize(total = n())\n\n# Total movies: included both movies and tvMovie categories\n\ntotal_movies &lt;- total_by_title_type |&gt;\n  filter(titleType == \"movie\" | titleType == \"tvMovie\") |&gt;\n  summarize(total = sum(total)) |&gt;\n  pull(total)\n\ncat('Total Movies: ', prettyNum(total_movies, big.mark = \",\"), '\\n')\n\n\nTotal Movies:  147,085 \n\n\nCode\n# Total TV series\n\ntotal_tvSeries &lt;- total_by_title_type |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  pull(total)\n\ncat('Total TV Series: ', prettyNum(total_tvSeries, big.mark = \",\"), '\\n')\n\n\nTotal TV Series:  29,920 \n\n\nCode\n# Total TV Episodes\n\ntotal_tvEpisode &lt;- total_by_title_type |&gt;\n  filter(titleType == \"tvEpisode\") |&gt;\n  pull(total)\n\ncat('Total TV Episodes: ', prettyNum(total_tvEpisode, big.mark = \",\"))\n\n\nTotal TV Episodes:  156,347\n\n\nOur data set currently has information about 147,085 movies (including television movies), 29,920 TV series (or shows), and 156,347 television episodes (more specifically, these are TV series episodes).\nQuestion #2: Who is the oldest living person in our data set?\nIn the NAME_BASICS table, we have information on 3,184,663 people in the data set. However, to find prospective actors/actresses and directors for new movies, it’s important to filter our data down to those that are still alive. By filtering the deathYear column to only show NA values, it removes records of people with listed death years (these are people we know for sure are no longer alive).\n\n\nCode\nno_death_date &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt;\n  mutate(age = 2024 - birthYear) |&gt;\n  arrange(desc(age))\n\n\nThe remaining data set contains 3,019,754 records. However, after creating an “age” column and observing these values, there are 3,830 people in the remaining data set that are over 100 years old, which seems highly unlikely.\nSo, after a little extra research, I found that the current oldest living person is 116 years old (Tomiko Itooka from Japan), so I filtered down the data even further to only contain people who would be at most 116 years old based on the information given.\n\n\nCode\nno_death_date &lt;- no_death_date |&gt;\n  filter(age &lt;= 116) |&gt;\n  arrange(desc(age))\n\n\nWith this new filtered data, there are 111 people that are (or would be) 116 years old, one of which is Angel Acciaresi. Given that some information is missing from the data set, it is unclear based on the data we have who is actually the current oldest living person in the data.\nQuestion #3: There is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nIMDb provides average ratings for titles based on votes from IMDb users, these ratings are out of a possible 10 (with 10/10 being the highest possible score). Now, let’s take a look at the data set and find out which titles have received a perfect 10/10 average rating on IMDb.\n\n\nCode\nbasics_ratings &lt;- inner_join(TITLE_BASICS, TITLE_RATINGS, by = 'tconst')\n\nperfect &lt;- basics_ratings |&gt;\n  filter(averageRating == 10) |&gt;\n  arrange(desc(numVotes))\n\nperfect_count &lt;- perfect |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\nperfect |&gt;\n  select(primaryTitle, titleType, averageRating, numVotes) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nThere are 67 listed TV episodes that have a perfect 10/10 rating in the data set, however, only one of these titles has an overwhelming number of votes, over 200,000 ratings.\n\n\nCode\nperfect_rating &lt;- basics_ratings |&gt;\n  filter(titleType == \"tvEpisode\",\n         averageRating == 10,\n         numVotes &gt;= 200000)\n\nperfect_episode &lt;- left_join(perfect_rating, TITLE_EPISODES, by = 'tconst')\n\nperfect_episode_series &lt;- TITLE_BASICS |&gt;\n  filter(tconst == perfect_episode$parentTconst)\n\ncat(\"Series: \", perfect_episode_series$primaryTitle, \"\\nEpisode Name: \", perfect_episode$primaryTitle, \"\\nSeason: \", perfect_episode$seasonNumber, \"\\nEpisode: \", perfect_episode$episodeNumber)\n\n\nSeries:  Breaking Bad \nEpisode Name:  Ozymandias \nSeason:  5 \nEpisode:  14\n\n\nFrom Breaking Bad, season 5 episode 14, titled “Ozymandias” garnered a 10/10 perfect rating with 228,900 votes. This was the show’s third to last episode, which had overwhelmingly positive feedback from audiences. Not only did this episode score high ratings on over 200,000 votes but, Dan Peeke from Screen Rant says “Ozymandias” is regarded as the series’ best episode and might even be the best episode in all of television history.2\nQuestion #4: What four projects is the actor Mark Hamill most known for?\nMark Hamill, a 73-year old American actor, has been part of many movies and shows over the course of his career. However, there were a slew of movies that highlighted his career. The four titles he is most known for are: “Star Wars: Episode IV - A New Hope”, “Star Wars: Episode VIII - The Last Jedi”, “Star Wars: Episode V - The Empire Strikes Back”, and “Star Wars: Episode VI - Return of the Jedi”. Hamill is best known for his role as Luke Skywalker in the Star Wars franchise (both the original and sequel trilogies).\n\n\nCode\n# knownForTitles column has multiple values within one cell, need to split the information into separate records\n\nNAME_BASICS_split &lt;- NAME_BASICS |&gt; \n  separate_longer_delim(knownForTitles, \",\")\n\n\n\n\nCode\nmark_hamill &lt;- NAME_BASICS_split |&gt;\n  filter(primaryName == \"Mark Hamill\")\n\nmark_hamill_projects &lt;- left_join(mark_hamill, TITLE_BASICS, join_by(knownForTitles == tconst))\n\n\nprint(mark_hamill_projects$primaryTitle)\n\n\n[1] \"Star Wars: Episode IV - A New Hope\"            \n[2] \"Star Wars: Episode VIII - The Last Jedi\"       \n[3] \"Star Wars: Episode V - The Empire Strikes Back\"\n[4] \"Star Wars: Episode VI - Return of the Jedi\"    \n\n\nQuestion #5: What TV series, with more than 12 episodes, has the highest average rating?\nThe IMDb data set not only includes information on films, but TV series as well. So, to take a closer look at some of the most successful TV series, let’s consider only those with more than 12 episodes.\n\n\nCode\ntv_series_12 &lt;- inner_join(TITLE_EPISODES, TITLE_BASICS, join_by(parentTconst == tconst)) |&gt; \n  inner_join(TITLE_RATINGS, join_by(parentTconst == tconst)) |&gt;\n  filter(episodeNumber &gt; 12)\n\ntv_series_12_ID &lt;- tv_series_12 |&gt;\n  select(parentTconst) |&gt;\n  unique()\n\nhigh_rated_series &lt;- inner_join(tv_series_12_ID, TITLE_BASICS, join_by(parentTconst == tconst)) |&gt;\n  inner_join(TITLE_RATINGS, join_by(parentTconst == tconst)) |&gt;\n  arrange(desc(averageRating)) |&gt;\n  slice_max(averageRating, n = 1)\n\nprint(high_rated_series$primaryTitle)\n\n\n[1] \"Cumartesi-Pazar Surpriz\"\n\n\nCode\nprint(high_rated_series$averageRating)\n\n\n[1] 9.8\n\n\nAfter filtering out the data, and sorting by average ratings, I found that a weekly news program called Cumartesi-Pazar Surpriz had the highest rating (among series with more than 12 episodes) with an average rating of 9.8.\nQuestion #6: The TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nSince the controversial episode occurred in the fifth season, I’d like to compare the average ratings of seasons 1-5 against the average ratings of seasons 6-11. To determine whether the quality issue after season 5 was true for Happy Days, I took the average of all the episode ratings from seasons 1-5 versus the average of all the episode ratings from seasons 6-11.\n\n\nCode\nhappy_days_ID &lt;- TITLE_BASICS |&gt;\n  filter(originalTitle == \"Happy Days\", startYear == 1974) |&gt;\n  pull(tconst)\n\nhappy_days_episodes &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst == happy_days_ID) |&gt;\n  left_join(TITLE_RATINGS, by = 'tconst') |&gt;\n  arrange(seasonNumber, episodeNumber) |&gt;\n  mutate(after_s5 = (seasonNumber &gt; 5))\n\nhappy_days_avg_ratings &lt;- happy_days_episodes |&gt;\n  group_by(after_s5) |&gt;\n  summarize(average = mean(averageRating, na.rm = TRUE)) \n\nup_to_5 &lt;- happy_days_avg_ratings |&gt;\n  filter(!after_s5) |&gt;\n  pull(average)\n\nafter_5 &lt;- happy_days_avg_ratings |&gt;\n  filter(after_s5) |&gt;\n  pull(average)\n\ncat(\"Before: \", up_to_5)\n\n\nBefore:  7.470536\n\n\nCode\ncat(\"\\nAfter: \", after_5)\n\n\n\nAfter:  6.868\n\n\nThe average rating for episodes up to and including season 5 was roughly 7.47 versus the average rating of 6.87 after season 5. Based on our data, after season 5, the show did experience a decrease in their average ratings for their episodes, though maybe not as drastic as we may have initially thought."
  },
  {
    "objectID": "mp02.html#quantifying-success",
    "href": "mp02.html#quantifying-success",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Quantifying Success",
    "text": "Quantifying Success\nBy the end of the project, we would like to propose successful movies. Before doing so, we need to have a success measure beyond what IMDb has already provided. We can assume that a successful title would be of high quality (high average rating) and has gained a broad awareness by IMDb users (large number of votes). So, a success metric combining these two metrics would be logical.\nFor my success metric, I decided to normalize the average rating and number of votes to create an index (ranging from 0 to 1) allowing for a simpler and holistic comparison for each of the movies. I wanted to account for both these values in my success metric. However, I noticed the drastic differences in the number of votes for certain movies. So, to mitigate this observation, I included a square root function for the votes proportion. Below is my success metric calculation:\n\\[\nsuccess = (\\frac{averageRating}{max(averageRating)})\\cdot(\\sqrt\\frac{numVotes}{max(numVotes)})\n\\]\nBelow is the code used to create a new success column with my newly defined success metric from above.\n\n\nCode\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(success = round((averageRating / max(averageRating)) * sqrt(numVotes / max(numVotes)), 3))\n\n\n\nSuccess Metric Validation\nBefore proceeding with further analysis, it is important to validate the success metric and ensure that it aligns appropriately with the movies in the data set. So, I will be answering the following questions to provide some validation on my proposed metric.\n\n\n\n\n\n\nValidation Questions\n\n\n\nBelow are some questions I will be answering to provide some validation on the proposed success metric.\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nPerform at least one other form of ‘spot check’ validation.\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value v such that movies above v are all “solid” or better.\n\n\n\nQuestion #1: Choose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nFirst, I would like to check that the top 5 movies based on my success metric were actually box office successes. Before tackling this question, I merged the two tables TITLE_RATINGS and TITLE_BASICS into a new data table movies_ratings_success so that I could work directly with all the title information alongside their IMDb metrics and success indices.\n\n\nCode\n# movies_ratings_success combines title_basics with title_ratings, to include metrics beside their respective movie titles and other information\n\nmovies_ratings_success &lt;- full_join(TITLE_RATINGS, TITLE_BASICS, by = 'tconst') |&gt;\n  filter(titleType == \"movie\")\n\n\nThese top 5 movies were The Shawshank Redemption, The Dark Knight, Inception, Fight Club, and Pulp Fiction. The Shawshank Redemption began as a box office flop, failing to cover its original budget, however, this film has grown in popularity over time, especially with the movie’s release into a regular fixture on TV.3 Fight Club also struggled on its initial release into the theaters, but became a popular film after its DVD release.4 The remaining three films (The Dark Knight, Inception, and Pulp Fiction) were all box office hits and continued with their success even after leaving theaters.\n\n\nCode\ntop_5_movies &lt;- movies_ratings_success |&gt;\n  arrange(desc(success)) |&gt;\n  slice_max(success, n = 5)\n\ntop_5_movies |&gt;\n  select(primaryTitle, averageRating, numVotes, success) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nQuestion #2: Choose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nNext, I wanted to choose 3 movies with a large amount of IMDb votes that scored relatively low on my success metric, and check that they were low quality films. Before beginning, I wanted to assess the distribution of the numVotes and success columns to determine what would be considered as a “large number” and a “poor score”.\n\n\nCode\n# distribution of the number of votes per movie\n\nquantile(movies_ratings_success$numVotes)\n\n\n     0%     25%     50%     75%    100% \n    100     195     459    1665 2948622 \n\n\n\n\nCode\n# distribution of the success index per movie\n\nquantile(movies_ratings_success$success)\n\n\n   0%   25%   50%   75%  100% \n0.001 0.005 0.007 0.014 0.930 \n\n\nFor this question, I decided to filter my data table to include only movies that exceeded the 3rd quartile number of votes 1,665 votes with a success rate lower than the 1st quartile success indices 0.005.\nThe lowest three were: Santa and the Ice Cream Bunny, Ax ’Em, and 2025 - The World Enslaved by a Virus. All three of these movies scored low on the success and had low average ratings from IMDb. Rotten Tomatoes provided some additional insight on these three movies. Santa and the Ice Cream Bunny scored a 12% on the popcornmeter (audience vote metric) out of a scale of 100, Ax ’Em scored a 36% on the popcornmeter, 2025 - The World Enslaved by a Virus scored a 0% on the popcornmeter from Rotten Tomatoes. All of these movies scored generally low on this audience vote metric and thus confirms that these movies were of low quality.\n\n\nCode\nlow_success_3 &lt;- movies_ratings_success |&gt;\n  filter(numVotes &gt;= quantile(movies_ratings_success$numVotes, 0.75) & success &lt;=  quantile(movies_ratings_success$success, 0.25)\n) |&gt;\n  arrange(success) |&gt;\n  slice_head(n = 3)\n\nlow_success_3 |&gt;\n  select(primaryTitle, averageRating, numVotes, success) |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nQuestion #3: Choose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nAfter researching some of the most prestigious directors in the movie business, I found Steven Spielberg among the most popular directors and decided to use his data as a validation source for my success metric. Taking a look at the distribution of success for his works in the past below, I found that a large majority of his works score higher on the success index than about 75% of the entire movie data set (indicated by the red dashed line). This is a good indication that Steven Spielberg’s work is typically more successful.\n\n\nCode\n# grabbing Steven Spielberg's NAME_BASICS information, also want to ensure that there's only one Steven Spielberg\n\nsteven_spielberg_basics &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Steven Spielberg\")\n  \n# want to check the ratings of all of Spielberg's works\n\nsteven_spielberg_works &lt;- left_join(steven_spielberg_basics, TITLE_PRINCIPALS, by = 'nconst') |&gt;\n  left_join(movies_ratings_success, by = 'tconst') |&gt;\n  select(tconst, primaryTitle, startYear, genres, averageRating, numVotes, success) |&gt;\n  drop_na() |&gt;\n  unique()\n\n\n\n\nCode\nsteven_spielberg_works |&gt;\n  ggplot(aes(x=success)) + \n  geom_histogram(bins=30) +\n  geom_vline(xintercept = quantile(movies_ratings_success$success, 0.75), \n             linetype = \"dashed\",\n             color = \"red\", \n             linewidth = 1) +\n  xlab(\"Success\") + \n  ylab(\"Number of Titles\") + \n  ggtitle(\"Distribution of Success for Steven Spielberg's Movies\") + \n  theme_bw() + \n  scale_x_log10(label=scales::comma) + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nQuestion #4: Perform at least one other form of ‘spot check’ validation.\nI decided for my last ‘spot check’ validation I would check the success metric of the top 5 most grossing movies of all time from IMDb’s Top Lifetime Grosses - Box Office Mojo, to ensure that their box office success is reflected in my success index. These 5 movies are Avatar, Avengers: Endgame, Avatar: The Way of Water, Titanic, and Star Wars: Episode VII - The Force Awakens. Each of these movies score relatively high on my success index, with success indices greater than 0.3.\n\n\nCode\navatar_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Avatar\", startYear == 2009) |&gt;\n  pull(success)\n\navengers_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Avengers: Endgame\") |&gt;\n  pull(success)\n\navatar_water_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Avatar: The Way of Water\") |&gt;\n  pull(success)\n\ntitanic_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Titanic\", startYear == 1997) |&gt;\n  pull(success)\n\nstar_wars_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Star Wars: Episode VII - The Force Awakens\") |&gt;\n  pull(success)\n\ncat(\"Avatar: \", avatar_success)\n\n\nAvatar:  0.545\n\n\nCode\ncat(\"\\nAvengers: Endgame: \", avengers_success)\n\n\n\nAvengers: Endgame:  0.558\n\n\nCode\ncat(\"\\nAvatar: The Way of Water: \", avatar_water_success)\n\n\n\nAvatar: The Way of Water:  0.313\n\n\nCode\ncat(\"\\nTitanic: \", titanic_success)\n\n\n\nTitanic:  0.526\n\n\nCode\ncat(\"\\nStar Wars: Episode VII - The Force Awakens: \", star_wars_success)\n\n\n\nStar Wars: Episode VII - The Force Awakens:  0.451\n\n\nQuestion #5: Come up with a numerical threshold for a project to be a ‘success’; that is, determine a value v such that movies above v are all “solid” or better.\nTo drive our analysis on successful movies, we will need to determine a numerical threshold for our success metric that will ensure these films are “solid” or better.\nBefore assessing possible thresholds for the success metric, let’s take a look at the distribution of success for the movie data. About 75% of movies score really low on the success scale with indices less than 0.014.\n\n\nCode\nquantile(movies_ratings_success$success)\n\n\n   0%   25%   50%   75%  100% \n0.001 0.005 0.007 0.014 0.930 \n\n\n\n\nCode\nmovies_ratings_success |&gt;\n    ggplot(aes(x=success)) + \n    geom_histogram(bins=30) +\n    xlab(\"Success\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Distribution of Success\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nTo be an incredibly successful movie, I believe that they have to score better than a large majority of movies, but still have a generally high average rating (at least above 0.5). Initially I chose a success measure of v = 0.1. Upon further observations, I noticed that a threshold of 0.1 still kept movies with average ratings as low as 3.8.\n\n\nCode\nmovies_ratings_success |&gt;\n  filter(success &gt;= 0.1) |&gt;\n  arrange(averageRating) |&gt;\n  slice_head(n = 10) |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\nThen after further adjustment and considering various numeric thresholds for my success metric, I decided on a success threshold of v = 0.2, guaranteeing that the films score above more than 75% of less successful films and still maintaining above or equal to a 5.3 average rating with a substantial number of votes (over 171,448 votes).\nBelow is the code to create our new data table, successful_movies, with our success threshold of v = 0.2.\n\n\nCode\n# Successful Movies Filter\n\nsuccessful_movies &lt;- movies_ratings_success |&gt;\n  filter(success &gt;= 0.2)"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood first launched with the completion of the silent film The Count of Monte Cristo in 1908. Shortly after, Hollywood’s first studio was created, and the rest is history.1 Since then, Hollywood has grown to be synonymous with the entertainment industry. As the film and television industries progressed with the advancement of technology, so did the demands for more creative storytelling and diverse entertainment.\nHollywood development executives are tasked with developing these new and creative movie ideas. Historically, these executives relied on adapting existing material like novels, true stories, or intellectual property (IP). However, this traditional approach has faced criticism by insiders and viewers for its reliance on pre-existing sources. The goal of this project is to develop these data-driven ideas to inspire new movies.\nWe will be utilizing the Internet Movie Database (IMDb) to drive our data-driven insights on the entertainment industry. More specifically, we will use the data sets from the IMDb non-commercial release. With this data, we hope to analyze the significant characteristics of successful films, discover successful filmmakers and actors, and explore unique insights from the entertainment industry."
  },
  {
    "objectID": "mp02.html#data",
    "href": "mp02.html#data",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Data",
    "text": "Data\nThe below code will download and load these files from IMDb into R, which results in a total of 6 data tables: NAME_BASICS, TITLE_BASICS, TITLE_EPISODES, TITLE_RATINGS, TITLE_CREW, TITLE_PRINCIPALS.\n\n\nCode\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\n\n\nCode\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\n\n\nCode\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\n\n\nCode\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\n\n\nCode\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\n\n\nCode\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\n\nBelow are a few packages that will be useful for our IMDb entertainment data analysis.\n\n\nCode\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"DT\")) install.packages(\"DT\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif(!require(\"forcats\")) install.packages(\"forcats\")\nif(!require(\"stringr\")) install.packages(\"stringr\")\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(RColorBrewer) # different color palette options\nlibrary(forcats) # grouping into an \"other\" category\nlibrary(stringr) # filtering on strings"
  },
  {
    "objectID": "mp02.html#data-sub-sampling",
    "href": "mp02.html#data-sub-sampling",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Data Sub-Sampling",
    "text": "Data Sub-Sampling\nSince our data is incredibly large, we need to take a closer look at the data to filter out any unnecessary information to a more reasonable data set to analyze more effectively and efficiently.\nTo restrict our attention to more well-known people in the entertainment industry, we will only keep those that have more than 1 “known for” credit.\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nsample_n(NAME_BASICS, 100) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nAdditionally, taking a look at the distribution of the amount of votes per title, we find that the distribution is heavily skewed right, indicating a significant number of titles with less than 100 votes.\n\n\nCode\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nBreaking this down a little further, we find that nearly 75% of the titles from the data set have less than 100 ratings.\n\n\nCode\nTITLE_RATINGS |&gt;\n  pull(numVotes) |&gt;\n  quantile()\n\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2948622 \n\n\nSo, to prevent our computers from working with such massive files, let’s focus only on titles with more than 100 ratings. After dropping these records, we greatly reduce the size of our data set.\n\n\nCode\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\n\nsample_n(TITLE_RATINGS, 100) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nWe also need to reflect this change on our remaining TITLE_* data tables using semi_joins.\n\n\nCode\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nWe have filtered each of the data tables into a more manageable size and now can begin analysis on the entertainment industry."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore information on the history of Hollywood can be found at: https://www.britannica.com/place/Hollywood-California↩︎\nScreen Rant opinion article about Breaking Bad ‘Ozymandias’ episode, written by Dan Peeke: https://screenrant.com/breaking-bad-reasons-ozymandias-was-best-episode-the-fly-was-worst/↩︎\nMore on The Shawshank Redemption’s rocky road to becoming a classic film: https://www.bbc.com/culture/article/20240919-the-shawshank-redemptions-path-from-flop-to-classic↩︎\nFight Club’s trek to becoming a classic film: https://screenrant.com/how-much-fight-club-made-box-office-adjusted-inflation/#:~:text=Summary,advertising%20and%20box%20office%20success.↩︎"
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decade",
    "href": "mp02.html#examining-success-by-genre-and-decade",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Examining Success by Genre and Decade",
    "text": "Examining Success by Genre and Decade\n\nExploration Questions\nNext, to begin thinking about a movie to pitch, I would like to start with a search for an optimal film genre to pursue.\n\n\n\n\n\n\nFilm Genre Questions\n\n\n\nBy answering the following questions, I hope to narrow down an ideal film genre to explore for my movie remake pitch.\n\nWhat was the genre with the most “successes” in each decade?\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nWhat genre has become more popular in recent years?\n\n\n\nQuestion #1: What was the genre with the most “successes” in each decade?\nFirst, I wanted to find out which genre had the most successes in each decade, figure out whether one genre dominated or if different genres had peaks during different decades.\nBefore proceeding with this, I needed to split out the information from the genres column so that each cell only contained one genre, I created a new data frame successful_movies_split_genres for this information.\n\n\nCode\nsuccessful_movies_split_genres &lt;- successful_movies |&gt;\n  separate_longer_delim(genres, \",\") |&gt;\n  mutate(decade = (startYear %/% 10) * 10)\n\n\nBased on the information below, I found that the Drama genre primarily dominated from the 9 decades between the 1920s to the 2000s, it was not until the 2010s when the Action genre took the lead for the 2010s and the 2020s decades. Since the 2020s decade is less than halfway through, there is still time for other genres to overtake its lead, but with our current information, Action currently leads with the most successes produced.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(max_total = max(total)) |&gt;\n  filter(total == max_total) |&gt;\n  ungroup() |&gt;\n  select(decade, genres, total) |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\nSince there are over 20 movie genres in our data set, it is a little bit difficult to visualize all these data onto one graph, so using the forcats package, I was able to keep the top 5 performing genres and bucket the remaining 16 genres into one “Other” bucket. From this bar plot, we can see that the Drama genre has been dominant for the most part, with Action taking the lead more recently. (Note: The “Other” bucket has the highest amount of movies, however, there are 16 genres combined into one bucket.)\n\n\nCode\n# Kept the top 5 performing genres, bucketed the remaining genres into an \"Other\" column (containing 16 genres)\n\n\nsuccessful_movies_split_genres |&gt;\n  mutate(new_genres = fct_lump_n(genres, 5)) |&gt;\n  ggplot(aes(x = decade, fill = new_genres)) +\n  geom_bar() +\n  labs(title = \"Total Successful Movies per Genre by Decade (1920s-2020s)\",\n       x = \"Decade\",\n       y = \"Total Successful Movies\") +\n  scale_fill_discrete(name = \"Genres\")\n\n\n\n\n\n\n\n\n\nBelow is a more comprehensive view of the amount of successful Drama movies per decade. We can see that the Drama genre has significantly increased their output of successful movies from the 1970s onwards.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres == \"Drama\") |&gt;\n  group_by(decade) |&gt;\n  summarize(total = n()) |&gt;\n  ggplot(aes(x = decade, y = total)) +\n  geom_bar(fill = \"deepskyblue\", stat = \"identity\") +\n  geom_line(aes(y = total), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Successful Drama Movies Over the Decades (1920s-2020s)\",\n       x = \"Decade\",\n       y = \"Total Successful Movies\")\n\n\n\n\n\n\n\n\n\nQuestion #2: What genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nBased on the output and plots from the previous question, I found that the Drama genre consistently had the most successes in each decade. Until the 2010s decade when the Action genre overtook the lead. Despite the change in the most dominant genre, the Drama genre continues to keep a close race with the Action genre for the most amount of successful films.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres == \"Drama\" | genres == \"Action\") |&gt;\n  ggplot(aes(x = genres, fill = genres)) +\n  geom_bar() +\n  facet_wrap(~decade) +\n  labs(title = \"Successful Action Movies vs. Drama Movies Over the Decades (1920s-2020s)\",\n       x = \"Decade\",\n       y = \"Total Successful Movies\") +\n  scale_fill_discrete(name = \"Genres\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_manual(name = \"Genres\",\n                    values = c(\"Action\" = \"#2F3C7E\", \"Drama\" = \"#F96167\"))\n\n\n\n\n\n\n\n\n\nQuestion #3: What genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nLet’s focus our attention to more recent decades, I would like to analyze the performance of genres from 2010s to the present. Below is a bar plot to visualize the distribution of successful movies across genres. Like we mentioned earlier, Action and Drama take first and second, respectively, for the most amount of successful films since 2010. However, now we can more clearly see that Adventure is a close third, with over 160 successful films in the past two decades. The remaining genres have all produced less than 150 successful movies in the same timeframe.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(decade &gt;= 2010) |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  ggplot(aes(x = genres, fill = decade)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n  ggtitle(\"Successful Films Since 2010 by Genre\")\n\n\n\n\n\n\n\n\n\nWhile Action, Drama, and Adventure have all produced over 150 successful movies each, it begs the question of whether these are highly successful genres or if this success is also partially impacted by the quantity of movies produced.\nFirst, I needed to create a new data frame, percentage_successful containing the percentage of successful films per genre calculated by taking the total successful films out of the total films produced.\n\n\nCode\ntotal_movies_genre &lt;- movies_ratings_success |&gt;\n  separate_longer_delim(genres, \",\") |&gt;\n  group_by(genres) |&gt;\n  summarize(total = n())\n\ntotal_successful_movies_genre &lt;- successful_movies_split_genres |&gt;\n  group_by(genres) |&gt;\n  summarize(total = n())\n\npercentage_successful &lt;- inner_join(total_movies_genre, total_successful_movies_genre, by = 'genres') |&gt;\n  mutate(percentage = (total.y / total.x) * 100,\n         highest = (percentage == max(percentage))) |&gt;\n  arrange(desc(percentage))\n\n\nAfter taking a closer look, I found that actually the Sci-Fi genre produced the highest percentage of successes with 3.46% of successful movies, followed by Adventure with 3.16%. While the genres Action and Drama had the most successes, this could have been due to the considerable amount of movies produced in those genres. Sometimes, it is less about the quantity produced and more about the quality being produced. Despite the Sci-Fi genre not producing nearly the same amount of films as the Action and Drama genres, the percentage of successful films to come out of this genre is higher than the others, which is why we need to consider the Sci-Fi genre as a successful genre as well.\n\n\nCode\npercentage_successful |&gt;\n  ggplot(aes(x = genres, y = percentage, fill = highest)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n  labs(title = \"Percentage of Successful Films by Genre Since 2010\",\n       x = \"Genres\",\n       y = \"Percentage of Successful Films\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"darkgrey\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nQuestion #4: What genre has become more popular in recent years?\nIn the below line plot, I limited our focus to 11 genres with 1 additional “Other” bucket to contain the remaining genres. All of these genres experience a jump in the amount of successful films produced between the 1990s and the 2000s, likely due to the increase in advanced movie production. Based on this data, we see that again, Drama takes a commanding lead for number of successful films up until 2010 when the Action genre overtakes the top spot. The Adventure genre is also not too far behind these two dominant genres. We are less than halfway through the 2020s decade, so the race is still close, but following the current pattern, it looks as though Action will continue to lead the genres in total number of successful movies.\n\n\nCode\n# limited to 11 genres with one \"Other\" bucket of genres, utilizing color brewer's suggestion of 12 maximum data classes\n\nsuccessful_movies_split_genres |&gt;\n  mutate(new_genres = fct_lump_n(genres, 11)) |&gt;\n  group_by(new_genres, decade) |&gt;\n  summarize(total = n()) |&gt;\n  ggplot(aes(x = decade, y = total, group = new_genres)) + \n  geom_line(aes(color = new_genres)) +\n  geom_point(aes(color = new_genres)) +\n  labs(title = \"Number of Successful Movies per Decade by Genre\",\n       x = \"Decade\",\n       y = \"Number of Successes\") +\n  scale_fill_discrete(name = \"Genres\") +\n  scale_y_continuous(label=scales::comma) +\n  scale_color_manual(values = c('#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928'))\n\n\n\n\n\n\n\n\n\nBased on our earlier analysis, I found that the Action and Drama genres have been the most popular genres in recent years, producing the most successes out of any categories. I also found that the Sci-Fi and Adventure genres produced a high percentage of successful movies since 2010. So, it may be advantageous to pursue one of these categories in my movie pitch. Taking a closer look at each of these genres, we can see that for the Action, Adventure and Drama genres, the distribution of success is skewed right. While for the Sci-Fi genre, the success distribution is a little more uniform, possibly suggesting that there would be less predictability in the success of a Sci-Fi movie as compared to the other genres which cut close with the success threshold.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres %in% c(\"Action\", \"Drama\", \"Sci-Fi\", \"Adventure\")) |&gt;\n  ggplot(aes(x = success)) + \n  geom_histogram(bins=30) +\n  facet_wrap(~genres) +\n  labs(title = \"Distribution of Success for Each Genre\",\n       x = \"Success\",\n       y = \"Number of Titles\") +\n  scale_x_log10(label=scales::comma) +\n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nFrom the box plots below, each of the quartiles for the Sci-Fi genre edge ahead of the other genres. Despite the Sci-Fi genres low production of films, the genre can still compete by producing a high percentage of successful films.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres %in% c(\"Action\", \"Drama\", \"Sci-Fi\", \"Adventure\")) |&gt;\n  ggplot(aes(x = genres, y = success)) + \n  geom_boxplot() +\n  xlab(\"Genres\") + \n  ylab(\"Success\") + \n  ggtitle(\"Distribution of Success for Each Genre\") + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres == \"Sci-Fi\") |&gt;\n  group_by(decade) |&gt;\n  summarize(total = n()) |&gt;\n  ggplot(aes(x = decade, y = total)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Successful Sci-Fi Movies by Decade\",\n       x = \"Decade\",\n       y = \"Number of Titles\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nGenre Selection\nThe Drama genre’s dominance in the film industry was evident from the earliest days into the 21st century, with a consistently high output of successful movies. Drama movies succeeded in the 20th century, a period of significant social change and historical events, by evoking emotions and connecting with its audience’s experience.\nOver time, as the world experienced a technology boom, the movie industry also reflected this by observing a shift in genre success. Starting in the 2010s decade, the Action genre overtook Drama as the most dominant genre, producing more successful movies. The introduction of new technology like computer-generated imagery (CGI) and advanced stunt technology allowed for bolder and more fantastical movie production, propelling newer genres like Action to overtake traditional movies.\nHowever, despite the high output of successful Action films in more recent decades, its proportion of successful movies in relation to its total output lags behind genres like Sci-Fi and Adventure which each produce more successful movies per movie created in their respective genres. Since the Sci-Fi genre has a high success rate and somewhat of a low production (as compared to other genres), I would like to pursue a movie in the Sci-Fi genre."
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre",
    "href": "mp02.html#successful-personnel-in-the-genre",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Successful Personnel in the Genre",
    "text": "Successful Personnel in the Genre\n\nSuccessful Director\nFor my Sci-Fi movie, I would like to pick Steven Spielberg as my director. Spielberg is an experienced director, having directed many highly successful Sci-Fi movies in the past, one of them being the iconic and classic E.T. the Extra-Terrestrial.\n\n\nCode\n# data tables containing everyone's name basics and titles worked on \n\neveryone_projects &lt;- left_join(NAME_BASICS, TITLE_PRINCIPALS, by = 'nconst') |&gt;\n  left_join(movies_ratings_success, by = 'tconst') |&gt;\n  filter(!is.na(averageRating)) |&gt;\n  group_by(nconst) |&gt;\n  mutate(average_person_success = round(sum(success) / n(), 3),\n         total_movies = n())\n  \neveryones_work &lt;- everyone_projects |&gt;\n  select(nconst, primaryName, average_person_success, total_movies, tconst, category, primaryTitle, averageRating, numVotes, success, genres)\n\n\n\n\nCode\n# checking Steven Spielberg's success relative to all Sci-Fi directors\n\nscifi_directors_genre_split &lt;- everyones_work |&gt;\n  separate_longer_delim(genres, delim = \",\") |&gt;\n  filter(genres == \"Sci-Fi\", category == \"director\") |&gt;\n  arrange(desc(average_person_success))\n\nspielberg_success &lt;- scifi_directors_genre_split |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  slice_head(n = 1) |&gt;\n  pull(average_person_success)\n\nsteven_rank &lt;- scifi_directors_genre_split |&gt;\n  select(primaryName, average_person_success) |&gt;\n  mutate(above_steven = (average_person_success &gt;= spielberg_success)) |&gt;\n  group_by(above_steven) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(percent = total/sum(total)) |&gt;\n  filter(above_steven == TRUE) |&gt;\n  pull(percent)\n\ncat(\"Steven Spielberg Top \", round(steven_rank*100, 2), \"% of Sci-Fi Directors\")\n\n\nSteven Spielberg Top  1.69 % of Sci-Fi Directors\n\n\nIn the histogram below, Steven Spielberg actually ranks in the top 1.69% of Sci-Fi directors, solidifying his position as a highly successful director among the most top tier Sci-Fi directors. The blue dashed line indicates where Spielberg’s average success metric falls in relation to other Sci-Fi directors.\n\n\nCode\nscifi_directors_genre_split |&gt;\n  ggplot(aes(x = success)) +\n  geom_histogram(bins = 30) +\n  geom_vline(xintercept = spielberg_success, \n             color = \"deepskyblue\", \n             linetype = \"dashed\", \n             linewidth = 1) +\n  labs(title = \"Distribution of Success Indices for Sci-Fi Directors\",\n       x = \"Director's Average Success\",\n       y = \"Number of Directors\")\n\n\n\n\n\n\n\n\n\nTaking a closer look at the Sci-Fi films that Spielberg has produced, we can see that a large majority of the Sci-Fi films Spielberg has been a part of have been successes. The red dashed line indicates the success threshold of v = 0.2.\n\n\nCode\nscifi_directors_genre_split |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  ggplot(aes(x = primaryTitle, y = success)) +\n  geom_bar(stat = \"identity\", fill = \"#B2B0EA\")  +\n  geom_hline(yintercept = 0.2,\n             color = \"red\",\n             linetype = \"dashed\",\n             linewidth = 1) +\n  labs(title = \"Steven Spielberg Sci-Fi Movies\",\n       x = \"Movie Title\",\n       y = \"Success\")\n\n\n\n\n\n\n\n\n\nFrom the pie chart below, we can see that of the Sci-Fi movies Steven Spielberg has produced, 80% of them have been successes, indicating him as a great candidate to direct future Sci-Fi films.\n\n\nCode\n# splitting the genre column so that each cell under the genre column contains only one value\n\nsteven_spielberg_works_split_genres &lt;- steven_spielberg_works |&gt;\n  separate_longer_delim(genres, delim = \",\")\n\n# filtering for only Sci-Fi movies\n\nsteven_spielberg_scifi &lt;- steven_spielberg_works_split_genres |&gt;\n  filter(genres == \"Sci-Fi\") |&gt;\n  mutate(outcome = (success &gt;= 0.2))\n\n# pie chart showing percentage of successes vs non-successes\n\nsteven_spielberg_scifi |&gt;\n  group_by(outcome) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_movies = sum(total),\n         percentage = (total/total_movies)*100,\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = outcome)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"Steven Spielberg's Proportion of Successful Sci-Fi Films\") +\n  scale_fill_discrete(name = \"Movie Outcome\", \n                      labels = c(\"Unsuccessful\", \"Successful\"),\n                      guide = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\n\n\nActing Talent\nNext, I would like to pick an actor/actress that has worked with Spielberg on successful films in the past. Given Harrison Ford’s proven success with Steven Spielberg in the iconic Indiana Jones franchise, he would be a strong candidate for another lead role. Ford starred in all five films of the Indiana Jones franchise, the first four of which were directed by Spielberg and the last by James Mangold. Ford’s collaborations with Spielberg on the Indiana Jones franchise have consistently proven to be box office successes, which contrasts the performance of franchise’s fifth installment, directed by James Mangold. As shown in the plot below, the first four films exceeded the success threshold, while the last one fell short. Ford’s established work with Spielberg and their history of creating box office hits makes them a compelling choice.\n\n\nCode\n# filter only for the Indiana Jones franchise\n\nindiana_jones_movies &lt;- everyones_work |&gt;\n  filter(primaryTitle %in% c(\"Raiders of the Lost Ark\",\n                             \"Indiana Jones and the Temple of Doom\",\n                             \"Indiana Jones and the Last Crusade\",\n                             \"Indiana Jones and the Kingdom of the Crystal Skull\",\n                             \"Indiana Jones and the Dial of Destiny\")) |&gt;\n  filter(category == \"director\")\n\nmovie_order &lt;- c(\"Raiders of the Lost Ark\",\n                 \"Indiana Jones and the Temple of Doom\",\n                 \"Indiana Jones and the Last Crusade\",\n                 \"Indiana Jones and the Kingdom of the Crystal Skull\",\n                 \"Indiana Jones and the Dial of Destiny\")\n\n# barplot indicating success of each Indiana Jones film, color classified by director\n\nindiana_jones_movies |&gt;\n  ggplot(aes(x = primaryTitle, y = success, fill = primaryName)) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  geom_hline(yintercept = 0.2, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  labs(title = \"Indiana Jones Movies\",\n       x = \"Movie Title\",\n       y = \"Success Index\",\n       fill = \"Directors\") +\n  theme(legend.position = \"bottom\") +\n  scale_x_discrete(limits = movie_order) +\n  scale_fill_manual(values = c(\"Steven Spielberg\" = \"#519DE9\", \"James Mangold\" = \"#f94449\"),\n                    guide = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\nLastly, I would like to pick a younger and experienced actor to work alongside the talented Harrison Ford and Steven Spielberg. Below is the table breakdown of actors and actresses between the ages of 20 and 30, sorted by their respective average success metrics.\n\n\nCode\n# filtering for actors/actresses that have worked on Sci-Fi films under the age of 30\n\nscifi_young_actors_genre_split &lt;- everyone_projects |&gt;\n  separate_longer_delim(genres, delim = \",\") |&gt;\n  filter(is.na(deathYear)) |&gt;\n  mutate(age = 2024 - birthYear) |&gt;\n  filter(genres == \"Sci-Fi\", \n         category %in% c(\"actor\", \"actress\"),\n         age %in% (20:30)) |&gt;\n  arrange(desc(average_person_success))\n\nscifi_young_actors_genre_split |&gt;\n  select(primaryName, age, category, average_person_success) |&gt;\n  unique() |&gt;\n  arrange(desc(average_person_success)) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 10))\n\n\n\n\n\n\nTaking a look at some of the top actors/actresses below the age of 30, I found that Tom Holland ranks very high up (being 5th in average success out of 526 actors/actresses) in the top 1%, with a success index of 0.234. Additionally, both of the Sci-Fi films he’s starred in have scored very high on the success index, demonstrating his existing experience in this genre.\n\n\nCode\ntom_holland_projects &lt;- scifi_young_actors_genre_split |&gt;\n  filter(primaryName == \"Tom Holland\")\n\ntom_holland_projects |&gt;\n  ggplot(aes(x = primaryTitle, y = success)) +\n  geom_bar(stat = \"identity\", width = 0.5, fill = \"#F4B678\") +\n  labs(title = \"Tom Holland's Sci-Fi Films\",\n       x = \"Movie Title\",\n       y = \"Success Index\")\n\n\n\n\n\n\n\n\n\nI knew I wanted to pick a young talent with experience as a lead in major films. Tom Holland with his impressive résumé, including his starring role as Spider-Man in the Marvel Cinematic Universe, presents him as an ideal candidate. Even with the blockbuster movies he’s already been a part of, Holland can benefit from continuing to work with experienced Hollywood talent, like Harrison Ford."
  },
  {
    "objectID": "mp02.html#nostalgia-and-remakes",
    "href": "mp02.html#nostalgia-and-remakes",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Nostalgia and Remakes",
    "text": "Nostalgia and Remakes\nAfter selecting our genre, director, and star actors for our film, we need to finalize a classic movie to remake for our pitch. Below are some of the top Sci-Fi classics released before the year 2000.\n\n\nCode\nscifi_successful_movies &lt;- successful_movies_split_genres |&gt;\n  filter(genres == \"Sci-Fi\",\n         startYear &lt;= 2000) |&gt;\n  arrange(desc(success))\n\nscifi_successful_movies |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\nAfter taking a look at some of the most successful movies released before 2000 (without recent sequels or remakes in the past 25 years), I decided to choose Back to the Future from 1985 as my classic movie remake. Back to the Future is an Adventure, Comedy, and Sci-Fi film with a high success index of 0.57, high IMDb rating of 8.5 and high number of votes at 1,335,879 votes. There were three Back to the Future movies in the franchise, the first film had the most success with the highest average rating and number of votes. The last film of the franchise was released in 1990 and there have been no remakes or additional sequels since. In my Back to the Future remake, I plan to cast Harrison Ford as the scientist, Doc Brown, and Tom Holland as Marty McFly.\n\n\nCode\n# data frame containing all of the Back to the Future franchise movies\n\nback_to_the_future_franchise &lt;- movies_ratings_success |&gt;\n  filter(str_detect(primaryTitle, \"Back to the Future\")) |&gt;\n  select(primaryTitle, startYear, genres, averageRating, numVotes, success)\n\nback_to_the_future_franchise |&gt;\n  select(primaryTitle, startYear, averageRating, numVotes, success) |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nSince the first film was released 39 years ago in 1985, we will need to check to confirm whether key actors, directors, or writers from the original are still alive. From the Back to the Future IMDb page, I found that the primary contributors to the film were Robert Zemeckis (director and writer), Bob Gale (writer), Michael J. Fox (actor), Christopher Lloyd (actor), and Lea Thompson (actor). Since all five of the primary contributors to the original film are still alive, I will need to contact the legal department to confirm that we can secure the rights to the project before I can proceed. As a fan service, I plan to also provide cameos to the stars of the original film if they are interested in partaking in the movie reboot.\n\n\nCode\noriginal_contributors_bttf &lt;- everyone_projects |&gt;\n  filter(primaryTitle == \"Back to the Future\") |&gt;\n  filter(primaryName %in% c(\"Robert Zemeckis\", \n                            \"Bob Gale\", \n                            \"Michael J. Fox\", \n                            \"Christopher Lloyd\", \n                            \"Lea Thompson\")) |&gt;\n  select(primaryName, birthYear, deathYear) |&gt;\n  unique() |&gt;\n  mutate(age = 2024 - birthYear)\n\noriginal_contributors_bttf |&gt;\n  DT::datatable(rownames = FALSE)"
  },
  {
    "objectID": "mp02.html#final-movie-pitch",
    "href": "mp02.html#final-movie-pitch",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Final Movie Pitch",
    "text": "Final Movie Pitch\n\nMovie Pitch\nThe Sci-Fi genre allows audiences to extend imaginations beyond the possibilities of the present, a genre we should consider. Over the past two decades, Sci-Fi has produced the highest percentage of successful films (3.46%). Furthermore, the Sci-Fi genre has increased the amount of successful movies produced from the 1980s-2010s by about 540%. Sci-Fi, unlike more saturated markets, offers a less crowded landscape, providing opportunities for emerging films.\nSteven Spielberg ranks in the top 1.69% of all-time Sci-Fi directors. Additionally, 80% of the Sci-Fi movies he’s directed have been box office successes. Spielberg’s successful track record in the Sci-Fi genre with movies like E.T. the Extra-Terrestrial, justifies him as a top-tier choice to direct another successful Sci-Fi movie.\nHarrison Ford’s and Steven Spielberg’s collaboration on the Indiana Jones franchise resulted in an 100% success rate with four movie hits. Given their history of success together, their collaboration will likely lead to another hit. Tom Holland, a young and experienced actor, has proven his ability to lead blockbuster films and ranks in the top 1% of successful Sci-Fi actors/actresses between ages 20-30. Ford’s and Holland’s experiences and household names will only boost the movie’s reach and potential.\nBack to the Future is an iconic box office hit from the mid-80s, which is due for a modern day remake. Recreating this classic Sci-Fi film, with the expertise of Steven Spielberg alongside star powers Harrison Ford and Tom Holland will surely produce a box office hit.\n\n\nTrailer\nFrom director Steven Spielberg, the visionary mind behind E.T. the Extra-Terrestrial; and from actor Harrison Ford, Hollywood icon and star of Indiana Jones; and from actor Tom Holland, popular upcoming star of the Sci-Fi genre, comes the timeless tale, Back to the Future. A story of adventure, time travel, and imagination. Coming soon to a theater near you!"
  },
  {
    "objectID": "mp03.html#set-up-and-initial-exploration",
    "href": "mp03.html#set-up-and-initial-exploration",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Set-Up and Initial Exploration",
    "text": "Set-Up and Initial Exploration\nBelow are some useful packages we will need to utilize throughout our US election data analysis.\n\n\nCode\n# Install necessary packages\n\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"sf\")) install.packages(\"sf\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"DT\")) install.packages(\"DT\")\nif(!require(\"gt\")) install.packages(\"gt\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif(!require(\"stringr\")) install.packages(\"stringr\")\nif(!require(\"patchwork\")) install.packages(\"patchwork\")\nif(!require(\"gganimate\")) install.packages(\"gganimate\")\nif(!require(\"zoom\")) install.packages(\"zoom\")\nif(!require(\"gridExtra\")) install.packages(\"gridExtra\")\n\n# Load packages into R\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(haven)\nlibrary(DT)\nlibrary(gt)\nlibrary(ggplot2)\nlibrary(RColorBrewer) # different color palette options\nlibrary(stringr)\nlibrary(patchwork) # inset plots\nlibrary(gganimate)\nlibrary(zoom) # zoom for plots\nlibrary(gridExtra) # labels outside the plot\n\n\n\nData I: US House Election Votes from 1976 to 2022\nFor our analysis, we will be downloading data from the MIT Election Data Science Lab, which collects votes from all biennial congressional races in each state from 1976 to 2022 as well as the statewide presidential vote counts from 1976 to 2020. We will need to download these files from the web and read them in as follows.\n\n\nCode\nhouse_1976_2022 &lt;- read.csv(\"1976-2022-house.csv\")\n\n\n\n\nCode\npresident_1976_2020 &lt;- read.csv(\"1976-2020-president.csv\")\n\n\n\n\nData II: Congressional Boundary Files 1976 to 2012\nNext, to visualize the past election results onto a US map, we will have to download the district shapefiles for the US from 1976 to 2022. We will download the US district shape files from 1976 to 2012 from Lewis et al. automatically with the following code.\n\n\nCode\n# Function to download district shape zip files from Jeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis\n\nget_district_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n    fname_ext &lt;- paste0(fname, \".zip\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n}\n\n# For loop to download district095 to district112 zip files\n\nfor (i in 95:112) {\n  filename &lt;- case_when(i &lt; 100 ~ paste0(\"districts0\", as.character(i)),\n                        i &gt;= 100 ~ paste0(\"districts\", as.character(i)))\n  get_district_file(filename)\n}\n\n\n\n\nData III: Congressional Boundary Files 2014 to Present\nAdditionally, for more recent elections from 2014 to 2022, we can download shapefiles from the US Census Bureau. The following code will automaticall download these congressional district shapefiles.\n\n\nCode\n# Function to download district shape zip files from US Census Bureau\n\nget_district_file_census &lt;- function(fname, year){\n    BASE_URL &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", as.character(year), \"/CD/\")\n    fname_ext &lt;- paste0(fname, \".zip\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n}\n\n# For loop to download congressional shapefiles (zip files) from 2014-2022\n\nfor (i in 2014:2022) {\n  filename &lt;- case_when(i &lt; 2016 ~ paste0(\"tl_\", as.character(i), \"_us_cd114\"),\n                        i &gt;= 2016 & i &lt; 2018 ~ paste0(\"tl_\", as.character(i), \"_us_cd115\"),\n                        i &gt; 2017 ~ paste0(\"tl_\", as.character(i), \"_us_cd116\"))\n  get_district_file_census(filename, i)\n}\n\n\nAdditionally for ease of plotting state geometries later in the project, the below code will download the state shape file from 2020 (the most recent presidential election that we have access to).\n\n\nCode\n# Function to download district shape zip files from US Census Bureau\n\nget_state_file_census &lt;- function(fname, year){\n    BASE_URL &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", as.character(year), \"/STATE/\")\n    fname_ext &lt;- paste0(fname, \".zip\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n}\n\n# Download state shapefiles zip folder from 2020\n\nget_state_file_census(\"tl_2020_us_state\", 2020)"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Every four years, the US holds a presidential election, where citizens go to polling sites to cast their votes for potential presidential candidates. The way these popular votes are handled in each state have varied over time. Though, the following has remained the same: * In each of the 50 states, there are R + 2 electoral college votes (where R is the number of Representatives the state has in the US House of Representatives) * For the purposes of this assignment, we will be counting each distinct district as one Representative * States are able to allocate these ECVs however they want * The candidate that receives the majority of ECVs becomes the President\nThere are essentially no rules in place on how the R + 2 ECVs for each state are allocated in the Constitution. Thus, at different points in time, various states have allocated their ECVs with the following methods: * Direct allocation of ECVs by state legislature (no vote) * Allocation of all ECVs to winner of state-wide popular vote * Allocation of all ECVs to winner of nation-wide popular vote * Allocation of R ECVs to popular vote winner by congressional district + allocation of remaining 2 ECVs to the state-wide popular vote winner\nCurrently, 48 states use the state-wide popular vote ECV allocation method. The only two states that have diverged from this are Maine and Nebraska, which use the final option.\nThe goal for this project is to explore the various electoral college vote (ECV) allocation methods for the presidential elections and assess the outcomes of each election had these methods been different.\nWe will be using data from the MIT Election Data Science Lab1 which has collected votes from all biennial congressional races in all 50 states and the statewide presidential vote counts from 1976 to 2022. Furthermore to assist with our map data visualization, we will be utilizing the congressional and/or state shape files created by Lewis et al. (1976-2012) and the US Census Bureau (2014-2022). With this data, we hope to visualize past election outcomes and assess the fairness of various ECV allocation schemes.\nQuestion 1: Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\nFirst, we would like to take a look at the change in US House seats from 1976 to 2022, to determine if there are any significant changes. There are a total of 435 members of the House of Representatives, each state is allowed a specific number of representatives based on their population size. Over time, certain states have gained and/or lost seats based on population changes. Since the US House seats directly impact the number of electoral college votes a state receives, this is an important metric to take a look at for our analysis later.\nCode\n# Data frame with the difference in seat changes from 1976 to 2022\n\nseat_change_1976_2022 &lt;- house_1976_2022 |&gt;\n  select(c(year, state, district)) |&gt;\n  unique() |&gt;\n  group_by(year, state) |&gt;\n  summarize(total = n()) |&gt;\n  filter(year == 1976 | year == 2022) |&gt;\n  pivot_wider(names_from = year, values_from = total) |&gt;\n  mutate(difference = `2022` - `1976`,\n         positive = (difference &gt;= 0))\n\n# States with no change\n\nno_change &lt;- seat_change_1976_2022 |&gt;\n  filter(difference == 0) |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\nno_change_seats &lt;- seat_change_1976_2022 |&gt;\n  filter(difference == 0) |&gt;\n  pull(state) |&gt;\n  c()\n\n# States that gained seats\n\ngain_seats &lt;- seat_change_1976_2022 |&gt;\n  filter(difference &gt; 0) |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\n# States that lost seats\n\nlost_seats &lt;- seat_change_1976_2022 |&gt;\n  filter(difference &lt; 0) |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\n# State that gained the most seats\n\ngained_most_seats &lt;- seat_change_1976_2022 |&gt;\n  slice_max(difference, n = 1) |&gt;\n  pull(difference)\n\n# State that lost the most seats\n\nlost_most_seats &lt;- seat_change_1976_2022 |&gt;\n  slice_min(difference, n = 1) |&gt;\n  mutate(lost = -difference) |&gt;\n  pull(lost)\n\n\n# Plot of all the states and their respective seat changes from 1976 to 2022\n\nseat_change_1976_2022 |&gt;\n  filter(difference != 0) |&gt;\n  ggplot(aes(x = reorder(state, difference), y = difference)) +\n  geom_bar(aes(fill = positive), stat = \"identity\", show.legend = FALSE) +\n  labs(title = \"States with House Seats Gained or Lost (1976-2022)\",\n       x = \"State\",\n       y = \"Change in Seats\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"TRUE\" = \"seagreen1\", \"FALSE\" = \"firebrick1\")) +\n  theme_bw()\nAfter gathering the house seat change from 1976 to 2022, I found that 16 states had no seat change (ALABAMA, ALASKA, ARKANSAS, DELAWARE, HAWAII, IDAHO, MAINE, MARYLAND, MINNESOTA, MONTANA, NEBRASKA, NEW HAMPSHIRE, NORTH DAKOTA, RHODE ISLAND, VERMONT, WYOMING), while 34 states did. Of these 34 states, 15 states gained seats while 19 states lost seats. Texas gained the most seats from 1976 with 14 gained seats. While New York lost the most seats from 1976 with 13 lost seats.\nQuestion 2: New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent).\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes they received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nThe “fusion” voting system allows candidates that have their name appear on multiple party lines on the ballot to have their votes counted under one total. We will take a look at whether the fusion voting system creates a substantial change in candidates’ success. We will do so by taking a look at election outcomes with and without the fusion voting system.\nFrom the data table below, we find that there were 24 instances when election outcomes differed between the fusion voting system and the single party system. Almost all of these outcomes came from New York elections, with only one from Connecticut in 1992.\nCode\n# Data frame with the highest votes for each years' election per state and district (without fusion) and the candidates that would have won\n\nwinner_no_fusion_votes &lt;- house_1976_2022 |&gt;\n  group_by(year, state, district, candidate, party) |&gt;\n  summarize(party_votes = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year, state, district) |&gt;\n  filter(party_votes == max(party_votes)) |&gt;\n  ungroup() |&gt;\n  select(c(year, state, district, candidate))\n\n# Data frame with the highest votes for each years' election per state and district (with fusion) and the candidates that won\n\nwinner_fusion_votes &lt;- house_1976_2022 |&gt;\n  group_by(year, state, district, candidate) |&gt;\n  summarize(candidate_total = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year, state, district) |&gt;\n  filter(candidate_total == max(candidate_total)) |&gt;\n  ungroup() |&gt;\n  select(c(year, state, district, candidate))\n  \n# Data frame combining above two data tables to compare winners with and without fusion\n\ncomparison_winner &lt;- left_join(winner_fusion_votes, winner_no_fusion_votes, by = c(\"year\", \"state\", \"district\")) |&gt;\n  rename(winner_fusion = candidate.x,\n         winner_no_fusion = candidate.y) |&gt;\n  mutate(same_winner = (winner_fusion == winner_no_fusion)) |&gt;\n  filter(same_winner == FALSE)\n\n# Display a DT() data table of the elections that would've been different without the fusion voting system\n\ncomparison_winner_df &lt;- comparison_winner |&gt;\n  select(c('year', 'state', 'district', 'winner_fusion', 'winner_no_fusion'))\n\nDT::datatable(setNames(comparison_winner_df, c(\"Year\", \"State\", \"District\", \"Fusion Winner\", \"No Fusion Winner\")), \n              caption = \"Table 1: Differences in House Seat Winners with Fusion and Without Fusion\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\nQuestion 3: Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\nLastly, we would like to explore whether the total presidential candidate votes per party exceeds or falls below the total votes of all of their party’s congressional candidates per state.\nBelow is the data frame comparing the difference between presidential candidate votes versus the total votes of their co-partisans per state.\nCode\n# Data frame counting total votes for each party in each state from house data\n\nhouse_party_votes_per_state &lt;- house_1976_2022 |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(house_party_votes = sum(candidatevotes))\n\n# Data frame counting total votes for each party in each state from president data\n\npresident_party_votes_per_state &lt;- president_1976_2020 |&gt;\n  filter(writein == FALSE) |&gt; # otherwise will have candidates that weren't originally on the ballot\n  select(year, state, party_detailed, candidate, candidatevotes) |&gt;\n  rename(party = party_detailed)\n\n# Data frame combining president and house data sets to compare total votes\n# Filtered for the 2 main parties: Democrat and Republican\n\nvote_comparison_president_house &lt;- left_join(president_party_votes_per_state,\n                                             house_party_votes_per_state,\n                                             by = c('year', 'state', 'party')) |&gt;\n  rename(presidential_candidate = candidate,\n         president_candidatevotes = candidatevotes) |&gt;\n  drop_na(house_party_votes) |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  mutate(president_more = (president_candidatevotes &gt; house_party_votes),\n         diff_p_h = president_candidatevotes - house_party_votes) |&gt;\n  filter(presidential_candidate != \"\") # one record has an empty string\n\n# Data frame output: showing the difference in votes\n\nvote_comparison_final &lt;- vote_comparison_president_house |&gt;\n  select(year, state, party, presidential_candidate, diff_p_h)\n  \n\nDT::datatable(setNames(vote_comparison_final, c(\"Year\", \"State\", \"Party\", \"Presidential Candidate\", \"Difference in Votes\")), \n              caption = \"Table 2: Difference in Presidential Votes vs Co-Partisans per State per Election Year\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\nAdditionally, below are animated bar plots transitioning between election years demonstrating the change in amounts of votes for the president versus their co-partisans across all 50 states and the District of Columbia from 1976 to 2020. We use blue to represent the Democratic Party and red to represent the Republican Party.\nCode\n# Animated plot showing the difference between presidential votes versus house votes each year in each state for the Democratic Party\n\ndem &lt;- vote_comparison_president_house |&gt;\n  filter(party == \"DEMOCRAT\") |&gt;\n  ggplot(aes(x = state, y = diff_p_h)) +\n  geom_bar(stat = \"identity\", fill = \"royalblue1\") +\n  coord_flip() +\n  transition_states(year) +\n  theme_bw() +\n  labs(title = \"Voting Patterns Presidential Candidate vs Co-Partisans ({closest_state})\",\n       subtitle = \"Democratic Party\",\n       x = \"State\",\n       y = \"Difference in Votes\")\n\nanimate(dem)\nCode\n# Animated plot showing the difference between presidential votes versus house votes each year in each state for the Democratic Party\n\nrep &lt;- vote_comparison_president_house |&gt;\n  filter(party == \"REPUBLICAN\") |&gt;\n  ggplot(aes(x = state, y = diff_p_h)) +\n  geom_bar(stat = \"identity\", fill = \"tomato\") +\n  coord_flip() +\n  transition_states(year) +\n  theme_bw() +\n  labs(title = \"Voting Patterns Presidential Candidate vs Co-Partisans ({closest_state})\",\n       subtitle = \"Republican Party\",\n       x = \"State\",\n       y = \"Difference in Votes\")\n\nanimate(rep)\nTo get a more general view of this data, we take the average difference among all states for each election year and compare these changes over time for the Republican Party versus the Democratic Party. Below is a dumbbell plot examining these changes over time, the yellow dashed line indicates where on average the difference between presidential votes and the party’s congressional district votes would have been 0 (no difference).\nCode\n# Average difference among all 50 states + DC between presidential votes and house votes in each election year\n\naverage_vote_comparison_president_house &lt;- vote_comparison_president_house |&gt;\n  group_by(year, party, presidential_candidate) |&gt;\n  summarize(average_diff = mean(diff_p_h)) |&gt;\n  ungroup()\n\naverage_vote_comparison_president_house |&gt;\n  slice_min(average_diff) |&gt;\n  pull(average_diff)\n\n\n[1] -110593.7\n\n\nCode\n# Graph the average change over time with a dumbbell plot\n\naverage_vote_comparison_president_house |&gt;\n  ggplot(aes(x = year, y = average_diff, color = party)) +\n  geom_segment(aes(xend = year, yend = -111000), color = \"grey\") +\n  geom_point(aes(color = party)) +\n  geom_hline(linetype = \"dashed\", color = \"gold\", yintercept = 0) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"royalblue1\", \"REPUBLICAN\" = \"tomato\"),\n                     name = \"Party\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Average Difference in Presidential Votes vs House Votes (1976-2020)\",\n       x = \"Election Year\",\n       y = \"Difference in Votes\")\nFrom our plot above, we find that the Democratic Party until 1996 experienced on average less votes for the presidential candidate than its co-partisans did across the 50 states and DC. While the Republican Party only saw a drop in its presidential elections in 1992, 1996, and 2016. In more recent years, it seems that for both parties, the presidential candidate generally has been more popular than its co-partisans."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Every four years, the US holds a presidential election, where citizens go to polling sites to cast their votes for potential presidential candidates. The way these popular votes are handled in each state have varied over time. Though, the following has remained the same: * In each of the 50 states, there are R + 2 electoral college votes (where R is the number of Representatives the state has in the US House of Representatives) * For the purposes of this assignment, we will be counting each distinct district as one Representative * States are able to allocate these ECVs however they want * The candidate that receives the majority of ECVs becomes the President\nThere are essentially no rules in place on how the R + 2 ECVs for each state are allocated in the Constitution. Thus, at different points in time, various states have allocated their ECVs with the following methods: * Direct allocation of ECVs by state legislature (no vote) * Allocation of all ECVs to winner of state-wide popular vote * Allocation of all ECVs to winner of nation-wide popular vote * Allocation of R ECVs to popular vote winner by congressional district + allocation of remaining 2 ECVs to the state-wide popular vote winner\nCurrently, 48 states use the state-wide popular vote ECV allocation method. The only two states that have diverged from this are Maine and Nebraska, which use the final option.\nThe goal for this project is to explore the various electoral college vote (ECV) allocation methods for the presidential elections and assess the outcomes of each election had these methods been different.\nWe will be using data from the MIT Election Data Science Lab1 which has collected votes from all biennial congressional races in all 50 states and the statewide presidential vote counts from 1976 to 2022. Furthermore to assist with our map data visualization, we will be utilizing the congressional and/or state shape files created by Lewis et al. (1976-2012) and the US Census Bureau (2014-2022). With this data, we hope to visualize past election outcomes and assess the fairness of various ECV allocation schemes."
  },
  {
    "objectID": "mp03.html#initial-exploration-of-vote-count-data",
    "href": "mp03.html#initial-exploration-of-vote-count-data",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Initial Exploration of Vote Count Data",
    "text": "Initial Exploration of Vote Count Data\nBefore beginning any specific deep-dive analysis, it is important to conduct some preliminary exploration of our data sets to understand the information that we have."
  },
  {
    "objectID": "mp03.html#preliminary-questions",
    "href": "mp03.html#preliminary-questions",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Preliminary Questions",
    "text": "Preliminary Questions\nBelow are some preliminary questions we will answer as an initial exploration of our data 1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022? 2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes they received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines? 3. Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?"
  },
  {
    "objectID": "mp03.html#importing-and-plotting-shape-file-data",
    "href": "mp03.html#importing-and-plotting-shape-file-data",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Importing and Plotting Shape File Data",
    "text": "Importing and Plotting Shape File Data\nBelow is the code used to read in the shape files from the zip archives we downloaded earlier, this will automatically read in only the shape file (shp) from each of these archives.\n\n\nCode\n# For loop loading all the SHP files from district 95 to 112 from Lewis et al.\n\nfor (i in 95:112) {\n  td &lt;- tempdir(); \n  filename &lt;- case_when(i &lt; 100 ~ paste0(\"districts0\", as.character(i)),\n                        i &gt;= 100 ~ paste0(\"districts\", as.character(i)))\n  zip_contents &lt;- unzip(paste0(filename, \".zip\"), \n                        exdir = td)\n      \n  fname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  assign(paste0(\"districts_\", as.character(i), \"_sf\"), read_sf(fname_shp))\n}\n\n# For loop loading all the Tiger/Line SHP files from US Census Bureau (2014 to 2022)\n\nfor (i in 2014:2022) {\n  td &lt;- tempdir(); \n  filename &lt;- case_when(i &lt; 2016 ~ paste0(\"tl_\", as.character(i), \"_us_cd114\"),\n                        i &gt;= 2016 & i &lt; 2018 ~ paste0(\"tl_\", as.character(i), \"_us_cd115\"),\n                        i &gt; 2017 ~ paste0(\"tl_\", as.character(i), \"_us_cd116\"))\n  zip_contents &lt;- unzip(paste0(filename, \".zip\"), \n                        exdir = td)\n      \n  fname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  assign(paste0(\"t1_\", as.character(i), \"_sf\"), read_sf(fname_shp))\n}\n\n# Loading the state shape file for 2020\n\ntd &lt;- tempdir(); \nfilename &lt;- \"tl_2020_us_state\"\nzip_contents &lt;- unzip(paste0(filename, \".zip\"), \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nassign(paste0(\"state_2020_sf\"), read_sf(fname_shp))\n\n\n\nChoropleth Visualization of the 2000 Presidential Election Electoral College Results\nTypically, during and immediately after election day, we often see choropleth maps depicting the voting results of each state in real time. Next, we would like to recreate one of these choropleth maps for the 2000 election between Al Gore and George W. Bush. In this map, we will use the traditional blue color for the Democratic candidate, Al Gore, and red color to represent the Republican candidate, George W. Bush.\n\n\nCode\n# Data frame filtering out only for the winner of each state in the 2000 election\n\nwinner_election_2000 &lt;- president_1976_2020 |&gt;\n  filter(year == 2000) |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(state) |&gt;\n  mutate(most_votes = max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(win = (candidatevotes == most_votes),\n         winner = case_when(win == TRUE ~ candidate)) |&gt;\n  drop_na(winner)\n\n\nUsing the district 107 shape file from the 2000 election, we get the plot below, where each state is colored by the party that won the most votes in the state. Since we used a district shape file to visualize the election outcome, we can also see the various districts that certain states divide into.\nFor easier viewing, I decided to inset Hawaii and Alaska instead of plotting them at their true map locations. Below is the code to plot the US states on the mainland (everything excluding Alaska and Hawaii).\n\n\nCode\n# Selecting only state and party_simplified columns to simplify when joining with sf\n\nstate_color_2000_district &lt;- winner_election_2000 |&gt;\n  select(state, candidate, party_simplified) |&gt;\n  mutate(candidate_last = case_when(candidate == \"BUSH, GEORGE W.\" ~ \"Bush\",\n                                    candidate == \"GORE, AL\" ~ \"Gore\")) |&gt;\n  mutate(state = str_to_title(state))\n\n# Filtering only for states in the mainland so our plot isn't squished\n\nmainland_district &lt;- districts_107_sf |&gt;\n  filter(STATENAME != \"Alaska\" & STATENAME != \"Hawaii\")\n\n# Merging the two tables so we know which party won the election for each state\n\nmainland_colors_district &lt;- left_join(mainland_district, state_color_2000_district, join_by(\"STATENAME\" == \"state\"))\n\n# Plot of mainland with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nmainland_plot_district &lt;- mainland_colors_district |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"2000 Presidential Election Electoral College Results: Gore vs. Bush\")\n\n\nHere is the code which plots for the remaining two states: Hawaii and Alaska.\n\n\nCode\n# Filtering only for Alaska\n\nalaska_district &lt;- districts_107_sf |&gt;\n  filter(STATENAME == \"Alaska\")\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nalaska_colors_district &lt;- left_join(alaska_district, state_color_2000_district, join_by(\"STATENAME\" == \"state\"))\n\n# Plot of Alaska with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nalaska_plot_district &lt;- alaska_colors_district |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  coord_sf(xlim = c(170, 250)) +\n  theme_void() +\n  guides(fill = FALSE)\n\n# Filtering only for Hawaii\n\nhawaii_district &lt;- districts_107_sf |&gt;\n  filter(STATENAME == \"Hawaii\")\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nhawaii_colors_district &lt;- left_join(hawaii_district, state_color_2000_district, join_by(\"STATENAME\" == \"state\"))\n\n# Plot of Hawaii with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nhawaii_plot_district &lt;- hawaii_colors_district |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  labs(x = NULL, y = NULL) +\n  theme_void() +\n  guides(fill = FALSE)\n\n\nLastly, here is the code that puts all three of our plots together onto one plot for easy viewing.\n\n\nCode\n# Combined mainland plot with insetted elements Alaska and Hawaii plots on the lower left\n\nmainland_plot_district +\n  inset_element(hawaii_plot_district, 0, 0, 0.3, 0.3) +\n  inset_element(alaska_plot_district, 0, 0, 0.2, 0.4)\n\n\n\n\n\n\n\n\n\nTo avoid the overwhelming lines on top of the map, we can use a state shape file to more easily visualize the state outlines without having the district outlines. Below is another choropleth map with the same coloring that includes only the shape outlines and also the ECV counts for each state. The same process from above is repeated to inset Hawaii and Alaska onto our map for easier viewing.\n\n\nCode\n# Selecting only state and party_simplified columns to simplify when joining with sf\n\nstate_color &lt;- winner_election_2000 |&gt;\n  select(state, candidate, party_simplified) |&gt;\n  mutate(candidate_last = case_when(candidate == \"BUSH, GEORGE W.\" ~ \"Bush\",\ncandidate == \"GORE, AL\" ~ \"Gore\"))\n\n# Adding the respective ECV count for each state in 2000\n\n  # Data frame with ECV count from 2000\n\necv_per_state_2000_no_dc &lt;- house_1976_2022 |&gt;\n  select(c(year, state, district)) |&gt;\n  unique() |&gt;\n  filter(year == 2000) |&gt;\n  group_by(state) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(ecv_total = total + 2) |&gt;\n  select(-c(total))\n\n  # Adding DC's 2 ECVs for that election --&gt; One Democratic elector abstained from casting a vote\n  \necv_per_state_2000 &lt;- rbind(ecv_per_state_2000_no_dc, data.frame(state = \"DISTRICT OF COLUMBIA\", ecv_total = 2))\n\n  # Merging data frame state_color with ecv counts\n  \nstate_color_ecv &lt;- left_join(state_color, ecv_per_state_2000, by = \"state\")\n\n# Filtering only for states in the mainland so our plot isn't squished, also filtering out places that aren't in the 50 states\n\nmainland &lt;- state_2020_sf |&gt;\n  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"United States Virgin Islands\", \"Commonwealth of the Northern Mariana Islands\", \"Guam\", \"American Samoa\", \"Puerto Rico\"))) |&gt;\n  mutate(NAME = toupper(NAME))\n\n# Merging the two tables so we know which party won the election for each state\n\nmainland_colors &lt;- left_join(mainland, state_color_ecv, join_by(\"NAME\" == \"state\"))\n\n# Plot of mainland with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nmainland_plot &lt;- mainland_colors |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  geom_sf_text(aes(label = ecv_total), color = \"black\", size = 2) +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"2000 Presidential Election Electoral College Results: Gore vs. Bush\")\n\n\n\n\nCode\n# Filtering only for Alaska\n\nalaska &lt;- state_2020_sf |&gt;\n  filter(NAME == \"Alaska\") |&gt;\n  mutate(NAME = toupper(NAME))\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nalaska_colors &lt;- left_join(alaska, state_color_ecv, join_by(\"NAME\" == \"state\"))\n\n# Plot of Alaska with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nalaska_plot &lt;- alaska_colors |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  geom_sf_text(aes(label = ecv_total), color = \"black\", size = 2) +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  coord_sf(xlim = c(170, 250)) +\n  theme_void() +\n  guides(fill = FALSE)\n\n# Filtering only for Hawaii\n\nhawaii &lt;- state_2020_sf |&gt;\n  filter(NAME == \"Hawaii\") |&gt;\n  mutate(NAME = toupper(NAME))\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nhawaii_colors &lt;- left_join(hawaii, state_color_ecv, join_by(\"NAME\" == \"state\"))\n\n# Plot of Hawaii with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nhawaii_plot &lt;- hawaii_colors |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  geom_sf_text(aes(label = ecv_total), color = \"black\", size = 2) +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  labs(x = NULL, y = NULL) +\n  theme_void() +\n  guides(fill = FALSE)\n\n\n\n\nCode\nmainland_plot +\n  inset_element(hawaii_plot, 0, 0, 0.3, 0.3) +\n  inset_element(alaska_plot, 0, 0, 0.2, 0.4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Count the total ECVs per candidate to confirm the winner of the 2000 Presidential Election\n\nwinner_2000 &lt;- state_color_ecv |&gt;\n  group_by(candidate_last) |&gt;\n  summarize(ecv = sum(ecv_total))\n\n# Pull the last name of the candidate that won the 2000 Election -- Bush\n\nwinner_2000_name &lt;- winner_2000 |&gt;\n  slice_max(ecv, n = 1) |&gt;\n  pull(candidate_last)\n\n# Pull the total ECVs the winning candidate received in the 2000 Election -- 271\n\nwinner_2000_count &lt;- winner_2000 |&gt;\n  slice_max(ecv, n = 1) |&gt;\n  pull(ecv)\n\n# Pull total ECVs of the losing candidate from the 2000 Election -- 266\n\nloser_2000_count &lt;- winner_2000 |&gt;\n  slice_min(ecv, n = 1) |&gt;\n  pull(ecv)\n\n\nFrom our new choropleth plot above, there is a clearer indication of state lines as well as the ECVs allocated per state. At first glance without these ECVs, it seems as though a large majority of the map is colored red, for George W. Bush. Taking a closer look at the total ECV count for each candidate, we find that ultimately Bush won the election by a slim margin, 271 ECVs to Al Gore’s 266.\n\n\nFaceted and Animated Plots Presidental Elections (1976-2020)\nNow let’s expand our findings beyond the 2000 presidential election to all the presidential elections from 1976-2020. We could do so with a faceted plot shown below. Each facet represents one of 12 presidential elections that occurred between this time period. It is interesting to visualize the changes in voting patterns for certain states over time.\n\n\nCode\n# Data frame with the winner for each state in each election\n\nwinner_each_election_per_state &lt;- president_1976_2020 |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state) |&gt;\n  mutate(most_votes = max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(win = (candidatevotes == most_votes),\n         winner = case_when(win == TRUE ~ candidate)) |&gt;\n  drop_na(winner)\n\n# Change the case of the states column in our state shape file\n\nstate_2020_sf_case &lt;- state_2020_sf |&gt;\n  filter(!(NAME %in% c(\"United States Virgin Islands\", \"Commonwealth of the Northern Mariana Islands\", \"Guam\", \"American Samoa\", \"Puerto Rico\"))) |&gt;\n  mutate(NAME = toupper(NAME))\n  \n\n# Data frame merging the winner from each state in each election with the shape file\n\nwinner_colors &lt;- left_join(state_2020_sf_case, winner_each_election_per_state, join_by(\"NAME\" == \"state\"))\n\n\n\n\nCode\nwinner_colors |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = party_simplified)) +\n  geom_sf() +\n  coord_sf(xlim = c(170, 300)) +\n  scale_fill_manual(name = \"Party\", values = c(\"DEMOCRAT\" = \"royalblue1\", \"REPUBLICAN\" = \"tomato\")) +\n  theme_bw() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"Presidential Election Outcomes (1976-2020)\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n\n\nAdditionally, I added an animated version of the plots to demonstrate the election results over time. Each animated state of the plot is a different election, demonstrating how some states have changed their voting patterns over time.\n\n\nCode\nanimated_plot &lt;- winner_colors |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = party_simplified)) +\n  geom_sf() +\n  coord_sf(xlim = c(170, 300)) +\n  scale_fill_manual(name = \"Party\", values = c(\"DEMOCRAT\" = \"royalblue1\", \"REPUBLICAN\" = \"tomato\")) +\n  theme_bw() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"Presidential Election Outcomes ({closest_state})\") +\n  transition_states(year, transition_length = 0, state_length = 1)\n\nanimate(animated_plot, renderer = gifski_renderer(file = paste0(directory, \"/election_animated_plot.gif\")))"
  },
  {
    "objectID": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "href": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Comparing the Effects of ECV Allocation Rules",
    "text": "Comparing the Effects of ECV Allocation Rules\nCurrently, electoral college votes are allocated in a state-wide winner-take-all fashion for 48 states and the District of Columbia. For the remaining two states, Nebraska and Maine, a district-wide winner-take-all + state-wide “at large” votes strategy is utilized. Throughout the course of history, various strategies have been used. This begs the question if election results would have been significantly different if all states used different ECV allocation rules.\nLastly, we will be taking a look at the various ECV allocation strategies to determine if any significant election changes would have occurred with alternative methods. We will be allocating ECVs with the following strategies: * State-Wide Winner-Take-All * District-Wide Winner-Take-All + State-Wide “At Large” Votes * State-Wide Proportional * National Proportional\n\n\nCode\n# Data frame that counts the ECV for each state each year\n\necv_per_state_per_year &lt;- house_1976_2022 |&gt;\n  select(c(year, state, district)) |&gt;\n  unique() |&gt;\n  group_by(year, state) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(ecv_total = total + 2) |&gt;\n  select(-c(\"total\")) |&gt;\n  ungroup()\n\n# Only 2020 includes DC's 3 ECVs, so we needto include DC's 3 votes for the other elections\n\necv_per_state_per_year &lt;- ecv_per_state_per_year |&gt;\n  add_row(year = 1976, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1980, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1984, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1988, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1992, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1996, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2000, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2004, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2008, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2012, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2016, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2020, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  distinct()\n\n\n\nState-Wide Winner-Take-All\n\nBelow is the results of the election with an ECV allocation method of State-Wide Winner-Take-All, where the winner of the popular vote for each states wins all ECVs for that respective state. This is essentially what our country currently has in place and reflects the results of the elections from 1976-2020.\n\n\nCode\n# Create a new data frame that groups by calculates the winner of each state's presidential election in a state-wide winner-take-all format\n\nstate_wide_winner_take_all &lt;- president_1976_2020 |&gt;\n  group_by(year, state, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes)) |&gt;\n  filter(total_votes == max(total_votes)) |&gt;\n  ungroup()\n\n# Include the candidate's parties\n\ncandidate_party &lt;- president_1976_2020 |&gt;\n  select(year, state, candidate, party_simplified) |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  rename(party = party_simplified)\n\n# Merge first two tables so we have the candidate and the respective main party\n\nstate_wide_winner_take_all_results &lt;- left_join(state_wide_winner_take_all, candidate_party, by = c(\"year\", \"state\", \"candidate\"))\n\n# Merging data frame with the ECV counts for each year\n\necv_state_wide_winner &lt;- left_join(state_wide_winner_take_all_results, ecv_per_state_per_year, by = c('year', 'state'))\n\n# Data frame containing the winner of each election by majority ECVs\n\nelection_winner_state_wide &lt;- ecv_state_wide_winner |&gt;\n  group_by(year, candidate, party) |&gt;\n  drop_na(ecv_total) |&gt;\n  summarize(results = sum(ecv_total)) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  filter(results == max(results)) |&gt;\n  ungroup() |&gt;\n  rename(state_wide_winner = candidate,\n         state_wide_party = party,\n         state_wide_results = results)\n\n# Display Table\n\nDT::datatable(setNames(election_winner_state_wide, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")),\n              caption = \"Table 3: State-Wide Winner-Take-All Election Winners\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\n\nNext, we will take a look at the District-Wide Winner-Take-All + State-Wide “At Large” Votes ECV allocation method. This is a relatively complex allocation strategy which counts the ECVs for candidates based on the party winner of congressional district elections, then the remaining 2 ECVs for each state are allocated based on the state popular vote winner. Below is the data frame showing the election winners had this method been applied for each state in each election.\n\n\nCode\n# Data frame counting ECV for each state based on party winner from congressional district voting\n\nwinner_per_district_per_year &lt;- house_1976_2022 |&gt;\n  group_by(year, state, district) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  select(year, state, district, party) |&gt;\n  ungroup() |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(ecv_count = n())\n\n# Data frame with candidate and respective party\n\ncandidate_party &lt;- president_1976_2020 |&gt;\n  select(year, state, candidate, party_simplified) |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  rename(party = party_simplified)\n\n# Merge the two data frames, so we can properly tally up the votes for each candidate each election year\n\nmerge_district_ecv_count &lt;- left_join(winner_per_district_per_year, candidate_party, by = c(\"year\", \"state\", \"party\")) |&gt;\n  drop_na(candidate)\n\n# Need the statewide popular vote winner to allocate the remaining 2 ECVs can use data frame from the previous section ... ecv_state_wide_winner data frame\n\nstate_wide_popular &lt;- ecv_state_wide_winner |&gt;\n  select(year, state, candidate, party) |&gt;\n  rename(state_winner = candidate,\n         state_winner_party = party)\n\n# Merge data frame with our other data frame to include the extra 2 ECVs for the statewide popular vote winner, then find the majority winner by ECV\n\nelection_winner_district_wide &lt;- left_join(merge_district_ecv_count, state_wide_popular, by = c('year', 'state')) |&gt;\n  mutate(ecv_extra = case_when(candidate == state_winner ~ 2,\n                               candidate != state_winner ~ 0),\n         ecv_total = ecv_count + ecv_extra) |&gt;\n  group_by(year, candidate, party) |&gt;\n  summarize(candidate_ecv_total = sum(ecv_total)) |&gt;\n  drop_na(candidate) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  filter(candidate_ecv_total == max(candidate_ecv_total)) |&gt;\n  ungroup() |&gt;\n  rename(district_wide_winner = candidate,\n         district_wide_party = party,\n         district_wide_results = candidate_ecv_total)\n\nDT::datatable(setNames(election_winner_district_wide, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")),\n              caption = \"Table 4: District-Wide Winner-Take-All + State-Wide 'At Large' Votes Election Winners\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\n\nState-Wide Proportional\n\nAnother ECV allocation method is taking the proportion of candidate votes for each party in each state and allocating the state’s ECV appropriately. For instance, if Candidate A received 200,000 votes and Candidate B received 300,000 votes for State C, then Candidate A would be allocated 40% of the ECVs for the state while Candidate B receives 60% of the ECvs for State C. The same is repeated for all 50 states. Then the candidate with the most ECVs would win the election. The data table below reveals the winners had the election utilized a State-Wide Proportional ECV allocation strategy.\n\n\nCode\n# Data frame including the proportion of votes each candidate earned from each state\n\nstate_wide_proportional &lt;- president_1976_2020 |&gt;\n  mutate(prop = candidatevotes / totalvotes)\n\n# Merging above data frame with the ECV counts for each year\n\necv_state_proportion_winner &lt;- left_join(state_wide_proportional, ecv_per_state_per_year, by = c('year', 'state'))\n\n# Data frame with winner after calculating the proportionate ECV per state\n\nelection_winner_state_prop &lt;- ecv_state_proportion_winner |&gt;\n  select(year, state, candidate, party_simplified, prop, ecv_total) |&gt;\n  mutate(ecv_votes = round(prop * ecv_total)) |&gt;\n  filter(ecv_votes != 0) |&gt;\n  group_by(year, candidate, party_simplified) |&gt;\n  summarize(combined_ecv = sum(ecv_votes)) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  filter(combined_ecv == max(combined_ecv)) |&gt;\n  ungroup() |&gt;\n  rename(state_prop_winner = candidate,\n         state_prop_party = party_simplified,\n         state_prop_results = combined_ecv)\n\n# Display Table\n\nDT::datatable(setNames(election_winner_state_prop, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")), \n              caption = \"Table 5: State-Wide Proportional Election Winner\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\n\nNational Proportional\n\nLastly, we have the National Proportional ECV allocation method, which acts similar to the State-Wide Proportional method, but on a nation-wide scale. We take the proportion of nation-wide popular votes for each candidate then allocate ECVs based on these holistic values. The data frame below shows the results of each election if a National Proportional ECV allocation was used.\n\n\nCode\n# Data frame including the proportion of votes each candidate earned from the entire nation\n\nnation_wide_proportional &lt;- president_1976_2020 |&gt;\n  group_by(year, candidate, party_simplified) |&gt;\n  summarize(candidate_total = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  mutate(total_year = sum(candidate_total),\n         prop = candidate_total / total_year) |&gt;\n  ungroup()\n\n# Data frame with the total ECV per year\n\ntotal_ecv_per_year &lt;- ecv_per_state_per_year |&gt;\n  group_by(year) |&gt;\n  summarize(total_ecv = sum(ecv_total))\n\n# Merge data frames\n\nmerge_nation_wide_ecv &lt;- left_join(nation_wide_proportional, total_ecv_per_year, by = c('year'))\n\n\n# Data frame with winner after calculating proportional ECV nation wide\n\nelection_winner_nation_prop &lt;- merge_nation_wide_ecv |&gt;\n  mutate(combined_ecv = round(prop * total_ecv)) |&gt;\n  filter(combined_ecv != 0) |&gt;\n  group_by(year) |&gt;\n  filter(combined_ecv == max(combined_ecv)) |&gt;\n  ungroup() |&gt;\n  select(year, candidate, party_simplified, combined_ecv) |&gt;\n  rename(nation_prop_winner = candidate,\n         nation_prop_party = party_simplified,\n         nation_prop_results = combined_ecv)\n\nDT::datatable(setNames(election_winner_nation_prop, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")), \n              caption = \"Table 6: National Proportional Election Winner\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\nBelow is a data frame that combines the results of all the possible election outcomes for easier comparison.\n\n\nCode\n# Merge all 4 results tables together into one data table with all the aggregated information\n\ncombined_results &lt;- left_join(election_winner_state_wide, election_winner_district_wide, by = \"year\") |&gt;\n  left_join(election_winner_state_prop, by = \"year\") |&gt;\n  left_join(election_winner_nation_prop, by = \"year\")\n\n\nHere are the potential US presidents for elections between 1976 and 2020 had each election’s ECV allocation been different.\n\n\nCode\n# Filter only for the candidates\n\ncombined_results |&gt;\n  select(year, state_wide_winner, district_wide_winner, state_prop_winner, nation_prop_winner) |&gt;\n  rename(Year = year,\n         `State Wide` = state_wide_winner,\n         `District Wide` = district_wide_winner,\n         `State Proportional` = state_prop_winner,\n         `National Proportional` = nation_prop_winner) |&gt;\n  DT::datatable(caption = \"Table 7: Election Winners for Different ECV Allocation Schemes\",\n                rownames = FALSE,\n                options = list(pageLength = 12))\n\n\n\n\n\n\nFrom first glance, the election results didn’t seem too different from the actual outcomes (using the State Wide column). We can observe that for the district allocation method, election outcomes were different for the years 1988, with Michael Dukakis, and 2012, with Mitt Romney, as the winners. In the state-wide proportional voting, the only difference we observe is that Hillary Clinton would have won the 2016 election. Lastly, with the national proportion method, we have that Al Gore and Hillary Clinton would have won the 2000 and 2016 elections, respectively.\nNext, we would like to take a look at whether certain ECV allocation schemes seem to favor certain parties. Below are pie charts showing the distribution of party outcomes for each allocation method.\n\n\nCode\n# Filter only for the parties\n\nstate_wide_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(state_wide_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = state_wide_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"State-Wide Winner-Takes-All\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n  \ndistrict_wide_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(district_wide_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = district_wide_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"District-Wide Winner-Take-All + \\nState-Wide 'At Large' Votes\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n\nstate_prop_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(state_prop_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = state_prop_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"State-Wide Proportional\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n\nnation_prop_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(nation_prop_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = nation_prop_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"National Proportional\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n\ngrid.arrange(state_wide_plot, district_wide_plot, state_prop_plot, nation_prop_plot,\n             top = \"Party Distribution\")\n\n\n\n\n\n\n\n\n\nIn our above pie charts, we find that both of the proportional schemes (state-wide proportional and national proportional) seem to favor the Democratic party over the Republican Party, while the remaining two methods (state-wide winner-takes-all and district-wide winner-take-all + state-wide “at large” votes) seem balanced with a perfect 50/50 split between the two parties. Despite the national proportional method and state-wide proportional method seemingly favoring the Democratic Party, these methods provide a better representation of the voting population, valuing each vote more closely than the other methods, which could perhaps suggest that this is reflective of the nation’s voter preferences."
  },
  {
    "objectID": "mp03.html#final-thoughts",
    "href": "mp03.html#final-thoughts",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nVarious elections would have been different had the electoral college vote allocations. Two examples are the 2000 election and the 2016 election. In the 2000 election, Al Gore won the popular vote and would have won with the national proportional voting scheme but lost in every other method. For the 2016 election, Hillary Clinton also won the national popular vote and would have won with a state-wide proportional vote as well. This leads to the question of fairness among these different allocation schemes.\nEach of the four electoral college vote allocation schemes has its strengths and weaknesses, I will be evaluating the “fairness” of each of these methods below.\nAmong all four options, I believe that the “fairest” ECV allocation method would be the national proportional scheme. The national proportional method takes everyone’s vote into consideration when allocating the electoral college votes. In the past, there have been elections where candidates have won the popular vote (gained the most votes among all US voters), but because of the state-wide winner-take-all allocation scheme, these candidates are left with fewer ECVs and thus, lose the election. The National Proportional allocation scheme would highly reflect voter preferences by allowing the majority popular vote winner to take office. However, to implement this scheme, every state would have to agree to implement this strategy and it may require a change in the Constitution.\nNext, I believe that the state-wide proportional ECV allocation strategy would be the next “fairest” allocation scheme. Similar to the national proportional method, the state-wide proportional method values each person’s vote equally per state. In this scheme, presidential candidates would get votes according to their state-level popularity. Additionally, the impact of swing states would be reduced, rather than a few hundred thousands of votes determining an election, more of an emphasis would be placed on individuals’ votes in each state. As compared to the national proportional vote, the state-wide proportional vote would reflect voter preferences from the respective state, however, since each state may have a different number of ECVs, some people’s votes may weigh less than others in the grand scheme. Implementing the state-wide proportional ECV still would require a state to make legislative changes, but would not be as tedious as what is required in the national proportional method.\nOf the remaining two allocation methods, I believe the district-wide winner-take-all + state-wide “at large” votes would be the next fairest scheme. This method takes the state-wide proportional method a step further by allocating each ECV by the popular vote winner of the respective district, which can better represent a state’s diversity of voting preferences compared to the state-wide winner-take-all method. This method similar to the above two will reduce the chances of candidates winning the popular vote but losing the electoral college, as we saw in some of the past elections. However, with this method of ECV allocation, a possible issue could occur with the bounds of districts (gerrymandering), if certain districts are changed, certain parties may be favored over others, which could skew the outcome of the election.\nLastly, in my opinion, the state-wide winner-take-all approach takes last in the “fairest” rankings. This allocation method only values the votes of the majority of the population in a state, and does not reflect the larger popular vote representation of an entire nation let alone one state. This is especially apparent in swing states, where elections come down to hundreds or even tens of thousands of votes, in which case the candidate that edges out obtains all ECVs. Ultimately, while this ECV allocation method is the simplest to implement, it often does not reflect voter preferences, making it the least “fairest” strategy."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMIT Election Data + Science Lab. (n.d.). MIT Election Lab. MIT Election Data + Science Lab. https://electionlab.mit.edu/↩︎"
  }
]