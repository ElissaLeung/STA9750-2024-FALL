[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "The major United States public transit systems contribute significantly to individuals daily transportation, allowing commuters alternative and often more affordable transit options for commutes. The goal of this project is to explore different characteristics of the major transit systems in the United States, based on data from the National Transit Database. More specifically, the analysis includes data from the 2022 Fare Revenue table, the most recent Monthly Ridership table, and the 2022 Operating Expenses reports. This report intends to consider various statistics including farebox recovery, ridership, total trips, total vehicle miles traveled, total revenues and expenses for various locations and transit agencies, to analyze the performance of these transit systems over time. Ultimately, the analysis of certain metrics will assist in determining the most efficient transit system in the United States."
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "The major United States public transit systems contribute significantly to individuals daily transportation, allowing commuters alternative and often more affordable transit options for commutes. The goal of this project is to explore different characteristics of the major transit systems in the United States, based on data from the National Transit Database. More specifically, the analysis includes data from the 2022 Fare Revenue table, the most recent Monthly Ridership table, and the 2022 Operating Expenses reports. This report intends to consider various statistics including farebox recovery, ridership, total trips, total vehicle miles traveled, total revenues and expenses for various locations and transit agencies, to analyze the performance of these transit systems over time. Ultimately, the analysis of certain metrics will assist in determining the most efficient transit system in the United States."
  },
  {
    "objectID": "mp01.html#data-cleaning",
    "href": "mp01.html#data-cleaning",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nBefore starting the analysis, the relevant data files need to be loaded and cleaned into data frames on R. Below consists of the code required to download the data files and create the relevant data frames by merging different tables. From the original data files four data frames are created: FARES, EXPENSES, TRIPS, and MILES. Following this, FARES and EXPENSES are merged into the FINANCIALS data frame and TRIPS and MILES are merged into the USAGE data frame. From this point forward, only the USAGE and FINANCIALS data frames will be necessary to conduct the analysis.\n\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(dplyr)\n\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Agency, City, State, NTD ID, Organization Type, Reporter Type, UZA...\ndbl  (2): Report Year, UACE Code\nnum (10): Primary UZA Population, Agency VOMS, Mode VOMS, Vehicle Operations...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe code below checks for possible lingering null values in the USAGE and FINANCIALS data frames before conducting analysis. An output of 0 for each of the data frames ensures that there are no null values in these data frames.\n\nUSAGE |&gt;\n  is.na() |&gt;\n  sum()\n\n[1] 0\n\n\n\nFINANCIALS |&gt;\n  is.na() |&gt;\n  sum()\n\n[1] 0\n\n\n\nSince, both of these chunks produced an output of 0, there are no null values in these data tables that may alter calculations and analysis in the future.\n\n\nThe USAGE table (shown below) provides transit system information including (but not limited to): agency, mode of transportation, total unlinked passenger trips, and total vehicle revenue miles.\n\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  arrange(`NTD ID`, month) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))"
  },
  {
    "objectID": "mp01.html#data-transformation",
    "href": "mp01.html#data-transformation",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Data Transformation",
    "text": "Data Transformation\nPrior to analyzing data, it is important to make data tables as clear as possible to ensure clarity and accurate interpretation of results. So, renaming the column UZA Name to metro_area clarifies that this column contains the location (city, state) of the transit systems.\n\nUSAGE &lt;- USAGE |&gt; \n  rename(metro_area = 'UZA Name')\n\nAdditionally, the Mode column contains some abbreviated codes, so converting these codes into understandable terms is helpful for future analysis.\n\n\n\n\n\n\nNote\n\n\n\n\nThe code below is used help to streamline the process of converting the codes into the terms, by showing the unique codes in the Mode column. Ultimately making the process of searching for these terms in the National Transit Database (NTD) Glossary much more efficient.\n\nunique(USAGE$Mode)\n\n [1] \"DR\" \"FB\" \"MB\" \"SR\" \"TB\" \"VP\" \"CB\" \"RB\" \"LR\" \"YR\" \"MG\" \"CR\" \"AR\" \"TR\" \"HR\"\n[16] \"IP\" \"PB\" \"CC\"\n\n\n\n\n\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway Transit\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nFurthermore, to clean up the data table even more, the removal of columns NTD ID and 3 Mode could help clear unnecessary information before analysis. Renaming certain columns like UPT and VRM to unlinked_passenger_trips and vehicle_revenue_miles will also provide more clarity for interpretation. For this, a new data table USAGE_clean is created, shown below.\n\nUSAGE_clean &lt;- USAGE |&gt; \n  select(-`NTD ID`, -`3 Mode`) |&gt; \n  rename(unlinked_passenger_trips = UPT, vehicle_revenue_miles = VRM)\n\nsample_n(USAGE_clean, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable(rownames = FALSE,\n                  options = list(pageLength = 5))"
  },
  {
    "objectID": "mp01.html#ridership-analysis",
    "href": "mp01.html#ridership-analysis",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Ridership Analysis",
    "text": "Ridership Analysis\n\n\n\n\n\n\nPreliminary Analysis:\n\n\n\nBelow are some preliminary questions to explore the USAGE_clean data table:\n\nWhat transit agency had the most total VRM in our data set?\nWhat transit mode had the most total VRM in our data set?\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n\n\nQuestion #1\n\ntransit_agency_max_total_VRM &lt;- \n  USAGE_clean |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(Agency)\n\ntransit_agency_max_total_VRM\n\n[1] \"MTA New York City Transit\"\n\ntransit_agency_max_total_VRM_dist &lt;- \n  USAGE_clean |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(total_VRM)\n\ntransit_agency_max_total_VRM_dist\n\n[1] 10832855350\n\n\nThe transit agency that had the most total vehicle revenue miles in the sample was MTA New York City Transit, with a total of about 10.83 billion vehicle revenue miles. Being that the MTA New York City Transit is one of the largest transit systems in the entire world and the largest in the US, it is unsurprising that this transit agency had the most total VRM in the data set.\n\n\nQuestion #2\n\ntransit_mode_max_total_VRM &lt;- \n  USAGE_clean |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(Mode)\n\ntransit_mode_max_total_VRM\n\n[1] \"Bus\"\n\ntransit_mode_max_total_VRM_dist &lt;- \n  USAGE_clean |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  slice_max(total_VRM, n = 1) |&gt;\n  pull(total_VRM)\n\ntransit_mode_max_total_VRM_dist\n\n[1] 49444494088\n\n\nThe transit mode with the most total vehicle revenue miles in this sample was the Bus, having a total of roughly 49.44 billion vehicle revenue miles. Buses make up a large portion of public transportation, especially in cities that don’t have heavy rail trains like NYC, so it is reasonable that buses had the most vehicle revenue miles.\n\n\nQuestion #3\n\nif(!require(\"lubridate\")) install.packages(\"lubridate\")\nlibrary(lubridate)\n\ntotal_trips_NYC_subway_2024 &lt;- USAGE_clean |&gt;\n  filter(Agency == \"MTA New York City Transit\", \n         Mode == \"Heavy Rail\", \n         year(month) == 2024) |&gt;\n  group_by(mon = month(month)) |&gt;\n  summarize(total_trips = sum(unlinked_passenger_trips))\n\ntotal_trips_NYC_subway_2024\n\n# A tibble: 7 × 2\n    mon total_trips\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1   157917292\n2     2   155722143\n3     3   172235628\n4     4   171458466\n5     5   180458819\n6     6   159891215\n7     7   161133245\n\ntotal_trips_NYC_subway &lt;- total_trips_NYC_subway_2024 |&gt;\n  filter(mon == 5) |&gt;\n  pull(total_trips)\n\nThere were about 180.46 million total trips taken on the New York City Subway in May 2024. From the data table total_trips_NYC_subway_2024, it is evident that the average monthly ridership from January 2024 to April 2024, was less than the total trips in May 2024. This could be due to the season change, as the weather gets warmer from Winter into the Spring.\n\n\nQuestion #4\n\nridership_drop_NYC_april &lt;- USAGE_clean |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month(month)==4) |&gt;\n  filter((year(month)==2019) | (year(month)==2020)) |&gt;\n  group_by(year = year(month)) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  arrange(year) |&gt;\n  summarize(ridership_drop = first(total_ridership)-last(total_ridership)) |&gt;\n  pull(ridership_drop)\n\nridership_drop_NYC_april\n\n[1] 211969660\n\n\nFrom April 2019 to April 2020, the NYC subway ridership fell by approxmately 211.97 million. The drastic drop in NYC subway ridership was likely due to the COVID-19 global pandemic which forced everyone to remain indoors and socially distance from others. So, it is understandable that such an extreme drop of 211.97 million occurred.\nAfter analyzing the above statistics from the monthly ridership table, I wanted to find some additional information on the possible impact of COVID-19 on transit system ridership in the US.\n\n\nTransit System Patterns Before and During COVID-19 Analysis\nSeveral years ago, in 2020, the world experienced a global outbreak of COVID-19, impacting local and international economies. This event encouraged social distancing and prevented any significant travel for citizens besides essential workers. I was curious to see how our national transit systems were affected by this global event. More specifically, I wanted to explore the ridership changes for the various metro areas included in the national ridership data set. I created a data frame with the total ridership of each metro area in this time frame, then calculated the percent change of ridership from 2019 to 2020. I expected to find a decrease in ridership in all the cities from 2019 to 2020 and wanted to explore which cities were negatively impacted the most and least.\n\nridership_by_area_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_by_area_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_2019_2020 &lt;- left_join(ridership_by_area_2019, ridership_by_area_2020, join_by(metro_area)) |&gt;\n  rename(total_ridership_2019 = total_ridership.x, total_ridership_2020 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2020-total_ridership_2019, percent_change = round(((change_in_ridership / total_ridership_2019) * 100), digits = 2), decrease = (percent_change &lt; 0))\n\n\nridership_2019_2020 |&gt; \n    DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nGreatest Decrease in Ridership from 2019-2020\n\ngreatest_ridership_drop_place &lt;- ridership_2019_2020 |&gt;\n  slice_min(percent_change, n=1) |&gt;\n  pull(metro_area)\n\ngreatest_ridership_drop_percent &lt;- ridership_2019_2020 |&gt;\n  slice_min(percent_change, n=1) |&gt;\n  pull(percent_change)\n\ngreatest_ridership_drop_place\n\n[1] \"Rome, GA\"\n\ngreatest_ridership_drop_percent\n\n[1] -91.56\n\n\nAfter looking through the data, I found that the city with the greatest ridership drop from 2019 to 2020 was Rome, GA with a percentage change of -91.56%.\nFurthermore, I wanted to explore more about the ridership in Rome, GA and analyze the breakdown of ridership by mode of transportation.\n\nridership_Rome_GA_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, metro_area == \"Rome, GA\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_Rome_GA_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, metro_area == \"Rome, GA\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_Rome_GA_2019_2020 &lt;- left_join(ridership_Rome_GA_2019, ridership_Rome_GA_2020, join_by(Mode)) |&gt;\n  rename(total_ridership_2019 = total_ridership.x, total_ridership_2020 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2020-total_ridership_2019, percent_change = round(((change_in_ridership / total_ridership_2019) * 100), digits = 2), decrease = (percent_change &lt; 0))\n\nridership_Rome_GA_2019_2020 |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\nFrom this table above, there were only two recorded modes of transportation from Rome, GA: bus and demand response. The bus ridership changed by -92.75%, while the demand response ridership changed by -36.57%. The sharp decline in bus ridership could be due to the fear of taking public mass transit. Individuals needing to travel locally would probably have preferred utilizing personal vehicles. Overall, the decrease in ridership was typical for majority of metropolitan areas with transit systems.\n\n\nExploring Peculiar Increases in Ridership from 2019-2020\nSubsequently, I was also curious to see which city was affected the least. When exploring this question, contrary to my assumption, there were a few cities that actually increased their ridership from 2019 to 2020.\n\npositive_change &lt;- ridership_2019_2020 |&gt;\n  filter(decrease == FALSE) |&gt;\n  arrange(desc(percent_change))\n\npositive_change_cities &lt;- positive_change$metro_area\n\npositive_change_cities\n\n[1] \"Victoria, TX\"       \"Port St. Lucie, FL\" \"Las Cruces, NM\"    \n\n\nThe cities that presumably increased their ridership from 2019 to 2020 were Victoria, TX, Port St. Lucie, FL, Las Cruces, NM.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough finding an increase in ridership from 2019 to 2020 is not entirely impossible, it was probably unlikely to occur. So, it was important to do additional investigation to figure out why these calculations came out to be positive.\n\n\nUpon further review of the data for Victoria, TX, it is apparent that no data was collected between January 2019 to August 2019, causing the total ridership for 2019 to be significantly lower than the totals from 2020.\n\ntx_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, \n         metro_area == \"Victoria, TX\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\ntx_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, \n         metro_area == \"Victoria, TX\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\ntx_combined &lt;- full_join(tx_2019, tx_2020) |&gt;\n  mutate(year = year(month),\n         mon = month(month)) |&gt;\n  select(-month) |&gt;\n  pivot_wider(id_cols = c(mon),\n              names_from = year,\n              values_from = total_UPT) |&gt;\n  arrange(mon)\n\nJoining with `by = join_by(month, total_UPT)`\n\ntx_combined |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\ntx_2019_total &lt;- tx_combined$`2019` |&gt;\n  replace(is.na(tx_combined$`2019`), 0) |&gt;\n  sum()\n\ntx_2019_total\n\n[1] 107199\n\ntx_2020_total &lt;- tx_combined$`2020` |&gt;\n  sum()\n\ntx_2020_total\n\n[1] 269979\n\n\nThe total for Victoria, TX in 2019 was 107,199 compared to 2020 with a total of 269,979, a significant difference due to the missing data from 2019. Being that the information for 2019 is not entirely accessible, it is inconclusive whether the ridership in Victoria, TX increased or decreased from 2019 to 2020.\nContrary to Victoria, TX, the data for Las Cruces, NM was not nearly as drastic. Although, there seems to be an unexpected value for the total ridership in January 2019.\n\nnm_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, \n         metro_area == \"Las Cruces, NM\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nnm_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, \n         metro_area == \"Las Cruces, NM\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nnm_combined &lt;- full_join(nm_2019, nm_2020) |&gt;\n  mutate(year = year(month),\n         mon = month(month)) |&gt;\n  select(-month) |&gt;\n  pivot_wider(id_cols = c(mon),\n              names_from = year,\n              values_from = total_UPT)\n\nJoining with `by = join_by(month, total_UPT)`\n\nnm_combined |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\njan_2019_nm &lt;- nm_combined |&gt;\n  filter(mon == 1) |&gt;\n  pull(`2019`)\n\nnm_combined_no_jan &lt;- nm_combined |&gt;\n  filter(mon != 1)\n\nnm_2019_min &lt;- nm_combined_no_jan$`2019` |&gt;\n  min()\n\nnm_2019_max &lt;- nm_combined_no_jan$`2019` |&gt;\n  max()\n\nFrom the table above, Las Cruces, NM, experienced a total ridership of 532 in January 2019, which seems highly unlikely given that the range of values for the remainder of 2019 were between 4,510 and 9,544. Observing the remaining data points, there’s a general increase in ridership from January 2019 until about January 2020. The Las Cruces, NM metro area experiences a steep ridership drop in April 2020, right around the peak of the pandemic. Overall, it seems that the ridership from January 2019 may have been undercounted, so it is unclear whether Las Cruces, NM experienced an increase or decrease in ridership from 2019 to 2020.\nLastly, for Port St. Lucie, FL, there is less of a concern for the 2019 data as the monthly ridership totals seem to fluctuate in the range of 14,000 to 18,000. However, in 2020, there is a noticeable increase in ridership in October 2020.\n\nfl_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, \n         metro_area == \"Port St. Lucie, FL\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nfl_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, \n         metro_area == \"Port St. Lucie, FL\") |&gt;\n  group_by(month) |&gt;\n  summarize(total_UPT = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nfl_combined &lt;- full_join(fl_2019, fl_2020) |&gt;\n  mutate(year = year(month),\n         mon = month(month)) |&gt;\n  select(-month) |&gt;\n  pivot_wider(id_cols = c(mon),\n              names_from = year,\n              values_from = total_UPT)\n\nJoining with `by = join_by(month, total_UPT)`\n\nfl_combined |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nsept_2020_fl &lt;- fl_combined |&gt;\n  filter(mon == 9) |&gt;\n  pull(`2020`)\n\noct_2020_fl &lt;- fl_combined |&gt;\n  filter(mon == 10) |&gt;\n  pull(`2020`)\n\nfl_increase &lt;- oct_2020_fl - sept_2020_fl\n\nfl_increase_percent &lt;- round((fl_increase / sept_2020_fl) * 100, digits = 0)\n\nIn September 2020, Port St. Lucie, FL experienced a ridership total of 6,883. In only one month, Port St. Lucie, FL increased its ridership by 43,535 (or 633%) to a total ridership in October 2020 of 50,418. While this seems a little unbelievable, it is possible that during the latter part of 2020, people were beginning to travel domestically again. Perhaps more people were fleeing bigger urban areas for smaller cities like Port St. Lucie, FL causing a spike in ridership in October 2020 and beyond. This case seems more reasonable, especially since the ridership in Port St. Lucie, FL after October 2020 maintained this high total ridership fluctuating between 46,000 and 52,000 UPT.\nAlthough in most cases, it is unlikely that ridership would have increased from 2019 to 2020, it is possible that certain, more suburban, areas could have had an increase in ridership. One possibility is that people wanted to relocate from more densely populated cities into smaller cities, when travel became more accessible, causing an increase in ridership in cities like Port St. Lucie, FL.\n\n\nNew York City Transit During COVID-19\nAside from the above three cities, the large majority of US city transit systems experienced a decrease in ridership. Being that New York City has the largest transit system in the United States, the Metropolitan Transit Authority (MTA), I was curious to further analyze the changes the city’s transit system experienced from 2019 to 2020.\n\nridership_NYC_2019 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2019, Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_NYC_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020, Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_NYC_2019_2020 &lt;- left_join(ridership_NYC_2019, ridership_NYC_2020, join_by(Mode)) |&gt;\n  rename(total_ridership_2019 = total_ridership.x, total_ridership_2020 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2020-total_ridership_2019, percent_change = round(((change_in_ridership / total_ridership_2019) * 100), digits = 2), decrease = (percent_change &lt; 0))\n\n\nridership_NYC_2019_2020 |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nnyc_change &lt;- round((sum(ridership_NYC_2019_2020$change_in_ridership) / sum(ridership_NYC_2019_2020$total_ridership_2019)) * 100, digits = 0)\n\nnyc_change\n\n[1] -56\n\n\nAs expected, all 5 modes of transportation (bus, bus rapid transit, commuter bus, demand response, and heavy rail) contributed to the overall -56% change in ridership in NYC from 2019 to 2020. During this time, many companies transitioned to remote work allowing employees to work from home and all students transitioned to virtual learning, avoiding travel and public transportation. Additionally, the fear of contracting and spreading the virus led more individuals who needed to travel to use personal vehicles rather than public transport. All of these factors contributed to the severe decrease in ridership in NYC’s transit system in 2020.\n\nridership_by_area_2020 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2020) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_by_area_2023 &lt;- USAGE_clean |&gt;\n  filter(year(month) == 2023) |&gt;\n  group_by(metro_area) |&gt;\n  summarize(total_ridership = sum(unlinked_passenger_trips)) |&gt;\n  ungroup()\n\nridership_2020_2023 &lt;- left_join(ridership_by_area_2020, ridership_by_area_2023, join_by(metro_area)) |&gt;\n  rename(total_ridership_2020 = total_ridership.x, total_ridership_2023 = total_ridership.y) |&gt;\n  mutate(change_in_ridership = total_ridership_2023-total_ridership_2020, percent_change = round(((change_in_ridership / total_ridership_2020) * 100), digits = 2), increase = (percent_change &gt;= 0))\n\nridership_2020_2023 |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\npercentage_positive &lt;- ridership_2020_2023 |&gt;\n  drop_na() |&gt;\n  mutate(total = n()) |&gt;\n  group_by(increase, total) |&gt;\n  summarize(count = n()) |&gt;\n  mutate(percent = round((count / total)*100, digits = 0)) |&gt;\n  filter(increase == TRUE) |&gt;\n  pull(percent)\n\n`summarise()` has grouped output by 'increase'. You can override using the\n`.groups` argument.\n\npercentage_positive\n\n[1] 91\n\n\nOverall, it was interesting to observe the varying effects that COVID-19 had on US transit systems across different metropolitan areas. Each city reacted and adjusted to the global pandemic differently, which led to different patterns of changes in their respective transit systems. Although most transit systems experienced a severe drop in ridership during 2020, it is evident that most transit systems have progressed in the right direction, with approximately 91% of cities demonstrating an increase in ridership from 2020 to 2023."
  },
  {
    "objectID": "mp01.html#financial-and-ridership-analysis-major-transit-systems",
    "href": "mp01.html#financial-and-ridership-analysis-major-transit-systems",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "2022 Financial and Ridership Analysis (Major Transit Systems)",
    "text": "2022 Financial and Ridership Analysis (Major Transit Systems)\nSince the focus of the remaining analysis is on the financials and ridership of the US major transit systems in 2022, the table USAGE must be filtered for data in 2022 and then merged with the FINANCIALS table from earlier. Additionally, to filter for major transit systems, the new table USAGE_AND_FINANCIALS is filtered to only contain data with total unlinked passenger trips greater than 400,000.\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarize(UPT = sum(UPT), VRM = sum(VRM)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'metro_area'. You can\noverride using the `.groups` argument.\n\n\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway Transit\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()\n\n\nUSAGE_AND_FINANCIALS_major_transit &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000)\n\nUSAGE_AND_FINANCIALS_major_transit |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\nEfficiency involves optimizing productivity while minimizing expense. In the below analysis, I take a look at various efficiency metrics to determine which United States transit system was the most efficient in 2022.\n\n\n\n\n\n\nEfficiency Analysis:\n\n\n\nBelow are various metrics that can be used to describe the efficiency of transit systems:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\n\n\n\nQuestion #1\n\ntransit_system_most_UPT_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  slice_max(UPT, n = 1) |&gt;\n  pull(Agency)\n\ntransit_system_most_UPT_agency\n\n[1] \"MTA New York City Transit\"\n\ntransit_system_most_UPT_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  slice_max(UPT, n = 1) |&gt;\n  pull(Mode)\n\ntransit_system_most_UPT_mode\n\n[1] \"Heavy Rail\"\n\ntransit_system_most_UPT &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  slice_max(UPT, n = 1) |&gt;\n  pull(UPT)\n  \ntransit_system_most_UPT\n\n[1] 1793073801\n\n\nIn 2022, the transit system with the highest amount of unlinked passenger trips (UPT) of 1.79 billion was the MTA New York City Transit with the mode of transportation of Heavy Rail. Since MTA New York City Transit subway system is the largest in the US, it is clear that they would have the highest UPT among all major US transit systems.\n\n\nQuestion #2\n\nhighest_farebox_recovery_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(farebox_recovery = `Total Fares` / Expenses) |&gt;\n  slice_max(farebox_recovery, n = 1) |&gt;\n  pull(Agency)\n\nhighest_farebox_recovery_agency\n\n[1] \"Port Imperial Ferry Corporation\"\n\nhighest_farebox_recovery_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(farebox_recovery = `Total Fares` / Expenses) |&gt;\n  slice_max(farebox_recovery, n = 1) |&gt;\n  pull(Mode)\n\nhighest_farebox_recovery_mode\n\n[1] \"Ferryboat\"\n\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(farebox_recovery = `Total Fares` / Expenses) |&gt;\n  slice_max(farebox_recovery, n = 1) |&gt;\n  pull(farebox_recovery)\n\nhighest_farebox_recovery\n\n[1] 1.428146\n\n\nThe transit system with the highest farebox recovery of 1.43 (ratio of total fares to expenses) in 2022 was Port Imperial Ferry Corporation with the mode of transportation of Ferryboat. It is interesting to see that the transit system with the highest farebox recovery is not a popular agency or mode most people would attribute “public transportation” as. However, it does show that efficient transit systems can exist anywhere in the US on any mode of transit.\n\n\nQuestion #3\n\nlowest_expenses_per_UPT_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_UPT = Expenses / UPT) |&gt;\n  slice_min(expenses_per_UPT, n = 1) |&gt;\n  pull(Agency)\n\nlowest_expenses_per_UPT_agency\n\n[1] \"North Carolina State University\"\n\nlowest_expenses_per_UPT_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_UPT = Expenses / UPT) |&gt;\n  slice_min(expenses_per_UPT, n = 1) |&gt;\n  pull(Mode)\n\nlowest_expenses_per_UPT_mode\n\n[1] \"Bus\"\n\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_UPT = Expenses / UPT) |&gt;\n  slice_min(expenses_per_UPT, n = 1) |&gt;\n  pull(expenses_per_UPT)\n\nlowest_expenses_per_UPT\n\n[1] 1.17912\n\n\nThe transit system that had the lowest expenses per UPT (1.18) in 2022 was Bus from North Carolina State University. Although we do not have all the information about the cost of university transit, it is likely that such a large university is transporting sizeable amounts of students daily which could contribute to its low expense per UPT.\n\n\nQuestion #4\n\nhighest_fares_per_UPT_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_UPT = `Total Fares` / UPT) |&gt;\n  slice_max(fares_per_UPT, n = 1) |&gt;\n  pull(Agency)\n\nhighest_fares_per_UPT_agency\n\n[1] \"Hampton Jitney, Inc.\"\n\nhighest_fares_per_UPT_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_UPT = `Total Fares` / UPT) |&gt;\n  slice_max(fares_per_UPT, n = 1) |&gt;\n  pull(Mode)\n\nhighest_fares_per_UPT_mode\n\n[1] \"Commuter Bus\"\n\nhighest_fares_per_UPT &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_UPT = `Total Fares` / UPT) |&gt;\n  slice_max(fares_per_UPT, n = 1) |&gt;\n  pull(fares_per_UPT)\n\nhighest_fares_per_UPT\n\n[1] 41.29628\n\n\nHampton Jitney, Inc. with the transit mode of Commuter Bus had the highest total fares per UPT of 41.3 in 2022. Since Hampton Jitney, Inc. is a commuter bus company that provides coach bus transportation, charter bus and tour bus options, it is likely that passengers are charged higher fares, leading to a high total fare per UPT.\n\n\nQuestion #5\n\nlowest_expenses_per_VRM_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_VRM = Expenses / VRM) |&gt;\n  slice_min(expenses_per_VRM, n = 1) |&gt;\n  pull(Agency)\n\nlowest_expenses_per_VRM_agency\n\n[1] \"Metropolitan Transportation Commission\"\n\nlowest_expenses_per_VRM_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_VRM = Expenses / VRM) |&gt;\n  slice_min(expenses_per_VRM, n = 1) |&gt;\n  pull(Mode)\n\nlowest_expenses_per_VRM_mode\n\n[1] \"Vanpool\"\n\nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(expenses_per_VRM = Expenses / VRM) |&gt;\n  slice_min(expenses_per_VRM, n = 1) |&gt;\n  pull(expenses_per_VRM)\n\nlowest_expenses_per_VRM\n\n[1] 0.4449998\n\n\nThe transit system with the lowest expenses per vehicle revenue mile (VRM) of 0.44 in 2022 was Vanpool from Metropolitan Transportation Commission. Since Vanpool is a form of transportation similar to rideshares, transporting a significantly less amount of people than typical mass transit in a smaller vehicle, the expenses required to operate and maintain it is likely significantly less. Thus, it is understandable that this mode of transportation would have the lowest expenses per VRM.\n\n\nQuestion #6\n\nhighest_fares_per_VRM_agency &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_VRM = `Total Fares` / VRM) |&gt;\n  slice_max(fares_per_VRM, n = 1) |&gt;\n  pull(Agency)\n\nhighest_fares_per_VRM_agency\n\n[1] \"Jacksonville Transportation Authority\"\n\nhighest_fares_per_VRM_mode &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_VRM = `Total Fares` / VRM) |&gt;\n  slice_max(fares_per_VRM, n = 1) |&gt;\n  pull(Mode)\n\nhighest_fares_per_VRM_mode\n\n[1] \"Ferryboat\"\n\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS_major_transit |&gt;\n  mutate(fares_per_VRM = `Total Fares` / VRM) |&gt;\n  slice_max(fares_per_VRM, n = 1) |&gt;\n  pull(fares_per_VRM)\n\nhighest_fares_per_VRM\n\n[1] 157.7002\n\n\nLastly, Ferryboat from Jacksonville Transportation Authority had the highest total fares per vehicle revenue mile (VRM) of 157.7 in 2022. The Jacksonville Transportation Authority provides Ferryboat transportation not only for passengers but also charges extra for those who want to transport various vehicles which could contribute to the high fares per VRM."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Mini Project #1: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Conclusion",
    "text": "Conclusion\nTransit system efficiency is subjective and can be measured with different metrics (highest UPT, highest farebox recovery, lowest expenses per UPT, highest total fares per UPT, lowest expenses per VRM, and highest total fares per VRM). Based on the above information, using the farebox recovery ratio, I found that the most efficient transit system in the country in 2022 was the Ferryboat from the Port Imperial Ferry Corporation. This transit system was able to maximize its total fares while minimizing their expenses leading to the highest farebox recovery ratio of 1.43.\nOverall, the National Transit Database provided a lot of useful and insightful data, allowing for a wide range of analysis. More specifically, it was interesting to explore historical transit data leading to thought-provoking insights as well as the various metrics to determine effective and efficient transportation in the United States. As the world continues to accelerate post-COVID, the major US public transit systems will continue to improve and create more efficient means of transportation for commuters around the nation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Elissa",
    "section": "",
    "text": "Hello everyone, my name is Elissa!\nI am a curious graduate student at Baruch College with a passion for problem solving and tackling challenges through new experiences. I’m currently pursuing a Master of Science in Business Analytics with a concentration in Data Analytics in the Zicklin School of Business.\nI am also a recent graduate of Baruch College from the Baruch Scholars Honors program where I obtained a Bachelor of Arts in Mathematics and minored in Communication Studies.\n13 years ago, I discovered my first passion. Basketball has been a large part of my life for as long as I can remember. I have grown significantly since then, adapting to new team environments, learning to become a leader, and overcoming challenges. Four years ago, I was able to live out my dream of playing at the collegiate level for the Baruch Bearcats.\nThree summers ago, I tried something new and different. I had the opportunity of being a Microsoft Data Science Research fellow. Using R, I researched hands-on with large data sets, developed regression models, and created visualizations to analyze data. By the end of this program, I successfully collaborated on a research replication project, analyzing the effects of socioeconomic and mobility factors on Covid-19 positivity rates in New York City. Through this opportunity I found a new passion for data science, a bridge between my interests in mathematics and programming.\nAll these experiences have provided me with foundational skills to foster development in both my personal and professional growth. I hope to continue to explore all my passions and gain new experiences during my pursuit of data science and data analytics careers."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood first launched with the completion of the silent film The Count of Monte Cristo in 1908. Shortly after, Hollywood’s first studio was created, and the rest is history.1 Since then, Hollywood has grown to be synonymous with the entertainment industry. As the film and television industries progressed with the advancement of technology, so did the demands for more creative storytelling and diverse entertainment.\nHollywood development executives are tasked with developing these new and creative movie ideas. Historically, these executives relied on adapting existing material like novels, true stories, or intellectual property (IP). However, this traditional approach has faced criticism by insiders and viewers for its reliance on pre-existing sources. The goal of this project is to develop these data-driven ideas to inspire new movies.\nWe will be utilizing the Internet Movie Database (IMDb) to drive our data-driven insights on the entertainment industry. More specifically, we will use the data sets from the IMDb non-commercial release. With this data, we hope to analyze the significant characteristics of successful films, discover successful filmmakers and actors, and explore unique insights from the entertainment industry."
  },
  {
    "objectID": "mp02.html#initial-exploration",
    "href": "mp02.html#initial-exploration",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Initial Exploration",
    "text": "Initial Exploration\n\nData Cleaning\nBefore beginning any data analysis, it is crucial that the data is cleaned and transformed appropriately to avoid any issues further down the line. Upon first glimpse of the six IMDb data sets, there are columns with incorrect data types. Occasionally when data sets are imported and read in, numeric columns are misread as character (string) data. This is due to the “null” values being represented in a non-standard way. In the case of IMDb, null values are recorded as \\\\N, R does not read this as NA values, misunderstands this and reads these values in as strings.\nSubsequently, these columns need to be adjusted to reflect its quantitative data or logical data for future analysis. To correct this, we can use a combination of the mutate command with the as.numeric (for quantitative values) or as.logical (for factor/level data) command to alter the type of column. Below are the columns I adjusted for each of the data tables from their previous character data type to either logical or numeric.\n\n\nCode\n# NAME_BASICS: birthYear and deathYear need to be corrected to numeric values\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\n# TITLE_BASICS: isAdult needs to be corrected to logical values, while startYear, endYear, and runtimeMinutes need to be corrected to numeric values\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(isAdult = as.logical(isAdult),\n         startYear = as.numeric(startYear),\n         endYear = as.numeric(endYear),\n         runtimeMinutes = as.numeric(isAdult))\n\n# TITLE_EPISODES: seasonNumber and episodeNumber need to be fixed to numeric values\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(seasonNumber = as.numeric(seasonNumber),\n         episodeNumber = as.numeric(episodeNumber))\n\n# TITLE_RATINGS: column types are good\n\n# TITLE_CREW: column types are correct, for character type columns (directors and writers), let's change the '\\\\N' values to the NA values that R understands\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  mutate(directors = na_if(directors, \"\\\\N\"),\n         writers = na_if(writers, \"\\\\N\"))\n\n# TITLE_PRINCIPALS: column types are correct, for character type columns (job, characters), let's change the '\\\\N' values to the NA values that R understands\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  mutate(job = na_if(job, \"\\\\N\"),\n         characters = na_if(characters, \"\\\\N\"))\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are a few columns in some of the data tables that combine several pieces of information into one cell separated with commas (e.g. primaryProfession and knownForTitles in the NAME_BASICS table). In order to break these values into multiple rows, we can use the separate_longer_delim. This will be a useful tool for later when we are taking a closer look at the data.\n\n\n\n\nPreliminary Exploration Questions\nBefore starting any deep-dive analysis, it is important to do some preliminary exploration of our data tables to understand what information we are working with.\n\n\n\n\n\n\nPreliminary Questions\n\n\n\nBelow are some preliminary questions, we would like to answer as an initial exploration of our data tables.\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nWho is the oldest living person in our data set?\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nWhat four projects is the actor Mark Hamill most known for?\nWhat TV series, with more than 12 episodes, has the highest average rating?\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\n\n\nQuestion #1: How many movies are in our data set? How many TV series? How many TV episodes?\nAs a preliminary step, it is useful to take a closer look at the data set to see the amount of data remaining after cleaning such a large data set. More specifically, let’s figure out how many movies, TV series and TV episodes remain in our data set. Before approaching this question, it is important to understand the existing types of titles in our data set.\n\n\nCode\nunique(TITLE_BASICS$titleType)\n\n\n [1] \"short\"        \"movie\"        \"tvSeries\"     \"tvShort\"      \"tvMovie\"     \n [6] \"tvEpisode\"    \"tvMiniSeries\" \"video\"        \"tvSpecial\"    \"videoGame\"   \n\n\nNow that we know how the title types are categorized, we can filter out our data set to determine the count of each type.\n\n\nCode\n# Total titles by title type\n\ntotal_by_title_type &lt;- TITLE_BASICS |&gt;\n  group_by(titleType) |&gt;\n  summarize(total = n())\n\n# Total movies: included both movies and tvMovie categories\n\ntotal_movies &lt;- total_by_title_type |&gt;\n  filter(titleType == \"movie\" | titleType == \"tvMovie\") |&gt;\n  summarize(total = sum(total)) |&gt;\n  pull(total)\n\ncat('Total Movies: ', prettyNum(total_movies, big.mark = \",\"), '\\n')\n\n\nTotal Movies:  147,085 \n\n\nCode\n# Total TV series\n\ntotal_tvSeries &lt;- total_by_title_type |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  pull(total)\n\ncat('Total TV Series: ', prettyNum(total_tvSeries, big.mark = \",\"), '\\n')\n\n\nTotal TV Series:  29,920 \n\n\nCode\n# Total TV Episodes\n\ntotal_tvEpisode &lt;- total_by_title_type |&gt;\n  filter(titleType == \"tvEpisode\") |&gt;\n  pull(total)\n\ncat('Total TV Episodes: ', prettyNum(total_tvEpisode, big.mark = \",\"))\n\n\nTotal TV Episodes:  156,347\n\n\nOur data set currently has information about 147,085 movies (including television movies), 29,920 TV series (or shows), and 156,347 television episodes (more specifically, these are TV series episodes).\nQuestion #2: Who is the oldest living person in our data set?\nIn the NAME_BASICS table, we have information on 3,184,663 people in the data set. However, to find prospective actors/actresses and directors for new movies, it’s important to filter our data down to those that are still alive. By filtering the deathYear column to only show NA values, it removes records of people with listed death years (these are people we know for sure are no longer alive).\n\n\nCode\nno_death_date &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt;\n  mutate(age = 2024 - birthYear) |&gt;\n  arrange(desc(age))\n\n\nThe remaining data set contains 3,019,754 records. However, after creating an “age” column and observing these values, there are 3,830 people in the remaining data set that are over 100 years old, which seems highly unlikely.\nSo, after a little extra research, I found that the current oldest living person is 116 years old (Tomiko Itooka from Japan), so I filtered down the data even further to only contain people who would be at most 116 years old based on the information given.\n\n\nCode\nno_death_date &lt;- no_death_date |&gt;\n  filter(age &lt;= 116) |&gt;\n  arrange(desc(age))\n\n\nWith this new filtered data, there are 111 people that are (or would be) 116 years old, one of which is Angel Acciaresi. Given that some information is missing from the data set, it is unclear based on the data we have who is actually the current oldest living person in the data.\nQuestion #3: There is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nIMDb provides average ratings for titles based on votes from IMDb users, these ratings are out of a possible 10 (with 10/10 being the highest possible score). Now, let’s take a look at the data set and find out which titles have received a perfect 10/10 average rating on IMDb.\n\n\nCode\nbasics_ratings &lt;- inner_join(TITLE_BASICS, TITLE_RATINGS, by = 'tconst')\n\nperfect &lt;- basics_ratings |&gt;\n  filter(averageRating == 10) |&gt;\n  arrange(desc(numVotes))\n\nperfect_count &lt;- perfect |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\nperfect |&gt;\n  select(primaryTitle, titleType, averageRating, numVotes) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nThere are 67 listed TV episodes that have a perfect 10/10 rating in the data set, however, only one of these titles has an overwhelming number of votes, over 200,000 ratings.\n\n\nCode\nperfect_rating &lt;- basics_ratings |&gt;\n  filter(titleType == \"tvEpisode\",\n         averageRating == 10,\n         numVotes &gt;= 200000)\n\nperfect_episode &lt;- left_join(perfect_rating, TITLE_EPISODES, by = 'tconst')\n\nperfect_episode_series &lt;- TITLE_BASICS |&gt;\n  filter(tconst == perfect_episode$parentTconst)\n\ncat(\"Series: \", perfect_episode_series$primaryTitle, \"\\nEpisode Name: \", perfect_episode$primaryTitle, \"\\nSeason: \", perfect_episode$seasonNumber, \"\\nEpisode: \", perfect_episode$episodeNumber)\n\n\nSeries:  Breaking Bad \nEpisode Name:  Ozymandias \nSeason:  5 \nEpisode:  14\n\n\nFrom Breaking Bad, season 5 episode 14, titled “Ozymandias” garnered a 10/10 perfect rating with 228,900 votes. This was the show’s third to last episode, which had overwhelmingly positive feedback from audiences. Not only did this episode score high ratings on over 200,000 votes but, Dan Peeke from Screen Rant says “Ozymandias” is regarded as the series’ best episode and might even be the best episode in all of television history.2\nQuestion #4: What four projects is the actor Mark Hamill most known for?\nMark Hamill, a 73-year old American actor, has been part of many movies and shows over the course of his career. However, there were a slew of movies that highlighted his career. The four titles he is most known for are: “Star Wars: Episode IV - A New Hope”, “Star Wars: Episode VIII - The Last Jedi”, “Star Wars: Episode V - The Empire Strikes Back”, and “Star Wars: Episode VI - Return of the Jedi”. Hamill is best known for his role as Luke Skywalker in the Star Wars franchise (both the original and sequel trilogies).\n\n\nCode\n# knownForTitles column has multiple values within one cell, need to split the information into separate records\n\nNAME_BASICS_split &lt;- NAME_BASICS |&gt; \n  separate_longer_delim(knownForTitles, \",\")\n\n\n\n\nCode\nmark_hamill &lt;- NAME_BASICS_split |&gt;\n  filter(primaryName == \"Mark Hamill\")\n\nmark_hamill_projects &lt;- left_join(mark_hamill, TITLE_BASICS, join_by(knownForTitles == tconst))\n\n\nprint(mark_hamill_projects$primaryTitle)\n\n\n[1] \"Star Wars: Episode IV - A New Hope\"            \n[2] \"Star Wars: Episode VIII - The Last Jedi\"       \n[3] \"Star Wars: Episode V - The Empire Strikes Back\"\n[4] \"Star Wars: Episode VI - Return of the Jedi\"    \n\n\nQuestion #5: What TV series, with more than 12 episodes, has the highest average rating?\nThe IMDb data set not only includes information on films, but TV series as well. So, to take a closer look at some of the most successful TV series, let’s consider only those with more than 12 episodes.\n\n\nCode\ntv_series_12 &lt;- inner_join(TITLE_EPISODES, TITLE_BASICS, join_by(parentTconst == tconst)) |&gt; \n  inner_join(TITLE_RATINGS, join_by(parentTconst == tconst)) |&gt;\n  filter(episodeNumber &gt; 12)\n\ntv_series_12_ID &lt;- tv_series_12 |&gt;\n  select(parentTconst) |&gt;\n  unique()\n\nhigh_rated_series &lt;- inner_join(tv_series_12_ID, TITLE_BASICS, join_by(parentTconst == tconst)) |&gt;\n  inner_join(TITLE_RATINGS, join_by(parentTconst == tconst)) |&gt;\n  arrange(desc(averageRating)) |&gt;\n  slice_max(averageRating, n = 1)\n\nprint(high_rated_series$primaryTitle)\n\n\n[1] \"Cumartesi-Pazar Surpriz\"\n\n\nCode\nprint(high_rated_series$averageRating)\n\n\n[1] 9.8\n\n\nAfter filtering out the data, and sorting by average ratings, I found that a weekly news program called Cumartesi-Pazar Surpriz had the highest rating (among series with more than 12 episodes) with an average rating of 9.8.\nQuestion #6: The TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nSince the controversial episode occurred in the fifth season, I’d like to compare the average ratings of seasons 1-5 against the average ratings of seasons 6-11. To determine whether the quality issue after season 5 was true for Happy Days, I took the average of all the episode ratings from seasons 1-5 versus the average of all the episode ratings from seasons 6-11.\n\n\nCode\nhappy_days_ID &lt;- TITLE_BASICS |&gt;\n  filter(originalTitle == \"Happy Days\", startYear == 1974) |&gt;\n  pull(tconst)\n\nhappy_days_episodes &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst == happy_days_ID) |&gt;\n  left_join(TITLE_RATINGS, by = 'tconst') |&gt;\n  arrange(seasonNumber, episodeNumber) |&gt;\n  mutate(after_s5 = (seasonNumber &gt; 5))\n\nhappy_days_avg_ratings &lt;- happy_days_episodes |&gt;\n  group_by(after_s5) |&gt;\n  summarize(average = mean(averageRating, na.rm = TRUE)) \n\nup_to_5 &lt;- happy_days_avg_ratings |&gt;\n  filter(!after_s5) |&gt;\n  pull(average)\n\nafter_5 &lt;- happy_days_avg_ratings |&gt;\n  filter(after_s5) |&gt;\n  pull(average)\n\ncat(\"Before: \", up_to_5)\n\n\nBefore:  7.470536\n\n\nCode\ncat(\"\\nAfter: \", after_5)\n\n\n\nAfter:  6.868\n\n\nThe average rating for episodes up to and including season 5 was roughly 7.47 versus the average rating of 6.87 after season 5. Based on our data, after season 5, the show did experience a decrease in their average ratings for their episodes, though maybe not as drastic as we may have initially thought."
  },
  {
    "objectID": "mp02.html#quantifying-success",
    "href": "mp02.html#quantifying-success",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Quantifying Success",
    "text": "Quantifying Success\nBy the end of the project, we would like to propose successful movies. Before doing so, we need to have a success measure beyond what IMDb has already provided. We can assume that a successful title would be of high quality (high average rating) and has gained a broad awareness by IMDb users (large number of votes). So, a success metric combining these two metrics would be logical.\nFor my success metric, I decided to normalize the average rating and number of votes to create an index (ranging from 0 to 1) allowing for a simpler and holistic comparison for each of the movies. I wanted to account for both these values in my success metric. However, I noticed the drastic differences in the number of votes for certain movies. So, to mitigate this observation, I included a square root function for the votes proportion. Below is my success metric calculation:\n\nsuccess = (\\frac{averageRating}{max(averageRating)})\\cdot(\\sqrt\\frac{numVotes}{max(numVotes)})\n\nBelow is the code used to create a new success column with my newly defined success metric from above.\n\n\nCode\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(success = round((averageRating / max(averageRating)) * sqrt(numVotes / max(numVotes)), 3))\n\n\n\nSuccess Metric Validation\nBefore proceeding with further analysis, it is important to validate the success metric and ensure that it aligns appropriately with the movies in the data set. So, I will be answering the following questions to provide some validation on my proposed metric.\n\n\n\n\n\n\nValidation Questions\n\n\n\nBelow are some questions I will be answering to provide some validation on the proposed success metric.\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nPerform at least one other form of ‘spot check’ validation.\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value v such that movies above v are all “solid” or better.\n\n\n\nQuestion #1: Choose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nFirst, I would like to check that the top 5 movies based on my success metric were actually box office successes. Before tackling this question, I merged the two tables TITLE_RATINGS and TITLE_BASICS into a new data table movies_ratings_success so that I could work directly with all the title information alongside their IMDb metrics and success indices.\n\n\nCode\n# movies_ratings_success combines title_basics with title_ratings, to include metrics beside their respective movie titles and other information\n\nmovies_ratings_success &lt;- full_join(TITLE_RATINGS, TITLE_BASICS, by = 'tconst') |&gt;\n  filter(titleType == \"movie\")\n\n\nThese top 5 movies were The Shawshank Redemption, The Dark Knight, Inception, Fight Club, and Pulp Fiction. The Shawshank Redemption began as a box office flop, failing to cover its original budget, however, this film has grown in popularity over time, especially with the movie’s release into a regular fixture on TV.3 Fight Club also struggled on its initial release into the theaters, but became a popular film after its DVD release.4 The remaining three films (The Dark Knight, Inception, and Pulp Fiction) were all box office hits and continued with their success even after leaving theaters.\n\n\nCode\ntop_5_movies &lt;- movies_ratings_success |&gt;\n  arrange(desc(success)) |&gt;\n  slice_max(success, n = 5)\n\ntop_5_movies |&gt;\n  select(primaryTitle, averageRating, numVotes, success) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nQuestion #2: Choose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nNext, I wanted to choose 3 movies with a large amount of IMDb votes that scored relatively low on my success metric, and check that they were low quality films. Before beginning, I wanted to assess the distribution of the numVotes and success columns to determine what would be considered as a “large number” and a “poor score”.\n\n\nCode\n# distribution of the number of votes per movie\n\nquantile(movies_ratings_success$numVotes)\n\n\n     0%     25%     50%     75%    100% \n    100     195     459    1665 2948622 \n\n\n\n\nCode\n# distribution of the success index per movie\n\nquantile(movies_ratings_success$success)\n\n\n   0%   25%   50%   75%  100% \n0.001 0.005 0.007 0.014 0.930 \n\n\nFor this question, I decided to filter my data table to include only movies that exceeded the 3rd quartile number of votes 1,665 votes with a success rate lower than the 1st quartile success indices 0.005.\nThe lowest three were: Santa and the Ice Cream Bunny, Ax ’Em, and 2025 - The World Enslaved by a Virus. All three of these movies scored low on the success and had low average ratings from IMDb. Rotten Tomatoes provided some additional insight on these three movies. Santa and the Ice Cream Bunny scored a 12% on the popcornmeter (audience vote metric) out of a scale of 100, Ax ’Em scored a 36% on the popcornmeter, 2025 - The World Enslaved by a Virus scored a 0% on the popcornmeter from Rotten Tomatoes. All of these movies scored generally low on this audience vote metric and thus confirms that these movies were of low quality.\n\n\nCode\nlow_success_3 &lt;- movies_ratings_success |&gt;\n  filter(numVotes &gt;= quantile(movies_ratings_success$numVotes, 0.75) & success &lt;=  quantile(movies_ratings_success$success, 0.25)\n) |&gt;\n  arrange(success) |&gt;\n  slice_head(n = 3)\n\nlow_success_3 |&gt;\n  select(primaryTitle, averageRating, numVotes, success) |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nQuestion #3: Choose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nAfter researching some of the most prestigious directors in the movie business, I found Steven Spielberg among the most popular directors and decided to use his data as a validation source for my success metric. Taking a look at the distribution of success for his works in the past below, I found that a large majority of his works score higher on the success index than about 75% of the entire movie data set (indicated by the red dashed line). This is a good indication that Steven Spielberg’s work is typically more successful.\n\n\nCode\n# grabbing Steven Spielberg's NAME_BASICS information, also want to ensure that there's only one Steven Spielberg\n\nsteven_spielberg_basics &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Steven Spielberg\")\n  \n# want to check the ratings of all of Spielberg's works\n\nsteven_spielberg_works &lt;- left_join(steven_spielberg_basics, TITLE_PRINCIPALS, by = 'nconst') |&gt;\n  left_join(movies_ratings_success, by = 'tconst') |&gt;\n  select(tconst, primaryTitle, startYear, genres, averageRating, numVotes, success) |&gt;\n  drop_na() |&gt;\n  unique()\n\n\n\n\nCode\nsteven_spielberg_works |&gt;\n  ggplot(aes(x=success)) + \n  geom_histogram(bins=30) +\n  geom_vline(xintercept = quantile(movies_ratings_success$success, 0.75), \n             linetype = \"dashed\",\n             color = \"red\", \n             linewidth = 1) +\n  xlab(\"Success\") + \n  ylab(\"Number of Titles\") + \n  ggtitle(\"Distribution of Success for Steven Spielberg's Movies\") + \n  theme_bw() + \n  scale_x_log10(label=scales::comma) + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nQuestion #4: Perform at least one other form of ‘spot check’ validation.\nI decided for my last ‘spot check’ validation I would check the success metric of the top 5 most grossing movies of all time from IMDb’s Top Lifetime Grosses - Box Office Mojo, to ensure that their box office success is reflected in my success index. These 5 movies are Avatar, Avengers: Endgame, Avatar: The Way of Water, Titanic, and Star Wars: Episode VII - The Force Awakens. Each of these movies score relatively high on my success index, with success indices greater than 0.3.\n\n\nCode\navatar_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Avatar\", startYear == 2009) |&gt;\n  pull(success)\n\navengers_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Avengers: Endgame\") |&gt;\n  pull(success)\n\navatar_water_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Avatar: The Way of Water\") |&gt;\n  pull(success)\n\ntitanic_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Titanic\", startYear == 1997) |&gt;\n  pull(success)\n\nstar_wars_success &lt;- movies_ratings_success |&gt;\n  filter(primaryTitle == \"Star Wars: Episode VII - The Force Awakens\") |&gt;\n  pull(success)\n\ncat(\"Avatar: \", avatar_success)\n\n\nAvatar:  0.545\n\n\nCode\ncat(\"\\nAvengers: Endgame: \", avengers_success)\n\n\n\nAvengers: Endgame:  0.558\n\n\nCode\ncat(\"\\nAvatar: The Way of Water: \", avatar_water_success)\n\n\n\nAvatar: The Way of Water:  0.313\n\n\nCode\ncat(\"\\nTitanic: \", titanic_success)\n\n\n\nTitanic:  0.526\n\n\nCode\ncat(\"\\nStar Wars: Episode VII - The Force Awakens: \", star_wars_success)\n\n\n\nStar Wars: Episode VII - The Force Awakens:  0.451\n\n\nQuestion #5: Come up with a numerical threshold for a project to be a ‘success’; that is, determine a value v such that movies above v are all “solid” or better.\nTo drive our analysis on successful movies, we will need to determine a numerical threshold for our success metric that will ensure these films are “solid” or better.\nBefore assessing possible thresholds for the success metric, let’s take a look at the distribution of success for the movie data. About 75% of movies score really low on the success scale with indices less than 0.014.\n\n\nCode\nquantile(movies_ratings_success$success)\n\n\n   0%   25%   50%   75%  100% \n0.001 0.005 0.007 0.014 0.930 \n\n\n\n\nCode\nmovies_ratings_success |&gt;\n    ggplot(aes(x=success)) + \n    geom_histogram(bins=30) +\n    xlab(\"Success\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Distribution of Success\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nTo be an incredibly successful movie, I believe that they have to score better than a large majority of movies, but still have a generally high average rating (at least above 0.5). Initially I chose a success measure of v = 0.1. Upon further observations, I noticed that a threshold of 0.1 still kept movies with average ratings as low as 3.8.\n\n\nCode\nmovies_ratings_success |&gt;\n  filter(success &gt;= 0.1) |&gt;\n  arrange(averageRating) |&gt;\n  slice_head(n = 10) |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\nThen after further adjustment and considering various numeric thresholds for my success metric, I decided on a success threshold of v = 0.2, guaranteeing that the films score above more than 75% of less successful films and still maintaining above or equal to a 5.3 average rating with a substantial number of votes (over 171,448 votes).\nBelow is the code to create our new data table, successful_movies, with our success threshold of v = 0.2.\n\n\nCode\n# Successful Movies Filter\n\nsuccessful_movies &lt;- movies_ratings_success |&gt;\n  filter(success &gt;= 0.2)"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood first launched with the completion of the silent film The Count of Monte Cristo in 1908. Shortly after, Hollywood’s first studio was created, and the rest is history.1 Since then, Hollywood has grown to be synonymous with the entertainment industry. As the film and television industries progressed with the advancement of technology, so did the demands for more creative storytelling and diverse entertainment.\nHollywood development executives are tasked with developing these new and creative movie ideas. Historically, these executives relied on adapting existing material like novels, true stories, or intellectual property (IP). However, this traditional approach has faced criticism by insiders and viewers for its reliance on pre-existing sources. The goal of this project is to develop these data-driven ideas to inspire new movies.\nWe will be utilizing the Internet Movie Database (IMDb) to drive our data-driven insights on the entertainment industry. More specifically, we will use the data sets from the IMDb non-commercial release. With this data, we hope to analyze the significant characteristics of successful films, discover successful filmmakers and actors, and explore unique insights from the entertainment industry."
  },
  {
    "objectID": "mp02.html#data",
    "href": "mp02.html#data",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Data",
    "text": "Data\nThe below code will download and load these files from IMDb into R, which results in a total of 6 data tables: NAME_BASICS, TITLE_BASICS, TITLE_EPISODES, TITLE_RATINGS, TITLE_CREW, TITLE_PRINCIPALS.\n\n\nCode\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\n\n\nCode\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\n\n\nCode\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\n\n\nCode\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\n\n\nCode\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\n\n\nCode\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\n\nBelow are a few packages that will be useful for our IMDb entertainment data analysis.\n\n\nCode\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"DT\")) install.packages(\"DT\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif(!require(\"forcats\")) install.packages(\"forcats\")\nif(!require(\"stringr\")) install.packages(\"stringr\")\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(RColorBrewer) # different color palette options\nlibrary(forcats) # grouping into an \"other\" category\nlibrary(stringr) # filtering on strings"
  },
  {
    "objectID": "mp02.html#data-sub-sampling",
    "href": "mp02.html#data-sub-sampling",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Data Sub-Sampling",
    "text": "Data Sub-Sampling\nSince our data is incredibly large, we need to take a closer look at the data to filter out any unnecessary information to a more reasonable data set to analyze more effectively and efficiently.\nTo restrict our attention to more well-known people in the entertainment industry, we will only keep those that have more than 1 “known for” credit.\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nsample_n(NAME_BASICS, 100) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nAdditionally, taking a look at the distribution of the amount of votes per title, we find that the distribution is heavily skewed right, indicating a significant number of titles with less than 100 votes.\n\n\nCode\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nBreaking this down a little further, we find that nearly 75% of the titles from the data set have less than 100 ratings.\n\n\nCode\nTITLE_RATINGS |&gt;\n  pull(numVotes) |&gt;\n  quantile()\n\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2948622 \n\n\nSo, to prevent our computers from working with such massive files, let’s focus only on titles with more than 100 ratings. After dropping these records, we greatly reduce the size of our data set.\n\n\nCode\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\n\nsample_n(TITLE_RATINGS, 100) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 5))\n\n\n\n\n\n\nWe also need to reflect this change on our remaining TITLE_* data tables using semi_joins.\n\n\nCode\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nWe have filtered each of the data tables into a more manageable size and now can begin analysis on the entertainment industry."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore information on the history of Hollywood can be found at: https://www.britannica.com/place/Hollywood-California↩︎\nScreen Rant opinion article about Breaking Bad ‘Ozymandias’ episode, written by Dan Peeke: https://screenrant.com/breaking-bad-reasons-ozymandias-was-best-episode-the-fly-was-worst/↩︎\nMore on The Shawshank Redemption’s rocky road to becoming a classic film: https://www.bbc.com/culture/article/20240919-the-shawshank-redemptions-path-from-flop-to-classic↩︎\nFight Club’s trek to becoming a classic film: https://screenrant.com/how-much-fight-club-made-box-office-adjusted-inflation/#:~:text=Summary,advertising%20and%20box%20office%20success.↩︎"
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decade",
    "href": "mp02.html#examining-success-by-genre-and-decade",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Examining Success by Genre and Decade",
    "text": "Examining Success by Genre and Decade\n\nExploration Questions\nNext, to begin thinking about a movie to pitch, I would like to start with a search for an optimal film genre to pursue.\n\n\n\n\n\n\nFilm Genre Questions\n\n\n\nBy answering the following questions, I hope to narrow down an ideal film genre to explore for my movie remake pitch.\n\nWhat was the genre with the most “successes” in each decade?\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nWhat genre has become more popular in recent years?\n\n\n\nQuestion #1: What was the genre with the most “successes” in each decade?\nFirst, I wanted to find out which genre had the most successes in each decade, figure out whether one genre dominated or if different genres had peaks during different decades.\nBefore proceeding with this, I needed to split out the information from the genres column so that each cell only contained one genre, I created a new data frame successful_movies_split_genres for this information.\n\n\nCode\nsuccessful_movies_split_genres &lt;- successful_movies |&gt;\n  separate_longer_delim(genres, \",\") |&gt;\n  mutate(decade = (startYear %/% 10) * 10)\n\n\nBased on the information below, I found that the Drama genre primarily dominated from the 9 decades between the 1920s to the 2000s, it was not until the 2010s when the Action genre took the lead for the 2010s and the 2020s decades. Since the 2020s decade is less than halfway through, there is still time for other genres to overtake its lead, but with our current information, Action currently leads with the most successes produced.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(max_total = max(total)) |&gt;\n  filter(total == max_total) |&gt;\n  ungroup() |&gt;\n  select(decade, genres, total) |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\nSince there are over 20 movie genres in our data set, it is a little bit difficult to visualize all these data onto one graph, so using the forcats package, I was able to keep the top 5 performing genres and bucket the remaining 16 genres into one “Other” bucket. From this bar plot, we can see that the Drama genre has been dominant for the most part, with Action taking the lead more recently. (Note: The “Other” bucket has the highest amount of movies, however, there are 16 genres combined into one bucket.)\n\n\nCode\n# Kept the top 5 performing genres, bucketed the remaining genres into an \"Other\" column (containing 16 genres)\n\n\nsuccessful_movies_split_genres |&gt;\n  mutate(new_genres = fct_lump_n(genres, 5)) |&gt;\n  ggplot(aes(x = decade, fill = new_genres)) +\n  geom_bar() +\n  labs(title = \"Total Successful Movies per Genre by Decade (1920s-2020s)\",\n       x = \"Decade\",\n       y = \"Total Successful Movies\") +\n  scale_fill_discrete(name = \"Genres\")\n\n\n\n\n\n\n\n\n\nBelow is a more comprehensive view of the amount of successful Drama movies per decade. We can see that the Drama genre has significantly increased their output of successful movies from the 1970s onwards.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres == \"Drama\") |&gt;\n  group_by(decade) |&gt;\n  summarize(total = n()) |&gt;\n  ggplot(aes(x = decade, y = total)) +\n  geom_bar(fill = \"deepskyblue\", stat = \"identity\") +\n  geom_line(aes(y = total), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Successful Drama Movies Over the Decades (1920s-2020s)\",\n       x = \"Decade\",\n       y = \"Total Successful Movies\")\n\n\n\n\n\n\n\n\n\nQuestion #2: What genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nBased on the output and plots from the previous question, I found that the Drama genre consistently had the most successes in each decade. Until the 2010s decade when the Action genre overtook the lead. Despite the change in the most dominant genre, the Drama genre continues to keep a close race with the Action genre for the most amount of successful films.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres == \"Drama\" | genres == \"Action\") |&gt;\n  ggplot(aes(x = genres, fill = genres)) +\n  geom_bar() +\n  facet_wrap(~decade) +\n  labs(title = \"Successful Action Movies vs. Drama Movies Over the Decades (1920s-2020s)\",\n       x = \"Decade\",\n       y = \"Total Successful Movies\") +\n  scale_fill_discrete(name = \"Genres\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_manual(name = \"Genres\",\n                    values = c(\"Action\" = \"#2F3C7E\", \"Drama\" = \"#F96167\"))\n\n\n\n\n\n\n\n\n\nQuestion #3: What genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nLet’s focus our attention to more recent decades, I would like to analyze the performance of genres from 2010s to the present. Below is a bar plot to visualize the distribution of successful movies across genres. Like we mentioned earlier, Action and Drama take first and second, respectively, for the most amount of successful films since 2010. However, now we can more clearly see that Adventure is a close third, with over 160 successful films in the past two decades. The remaining genres have all produced less than 150 successful movies in the same timeframe.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(decade &gt;= 2010) |&gt;\n  mutate(decade = as.character(decade)) |&gt;\n  ggplot(aes(x = genres, fill = decade)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n  ggtitle(\"Successful Films Since 2010 by Genre\")\n\n\n\n\n\n\n\n\n\nWhile Action, Drama, and Adventure have all produced over 150 successful movies each, it begs the question of whether these are highly successful genres or if this success is also partially impacted by the quantity of movies produced.\nFirst, I needed to create a new data frame, percentage_successful containing the percentage of successful films per genre calculated by taking the total successful films out of the total films produced.\n\n\nCode\ntotal_movies_genre &lt;- movies_ratings_success |&gt;\n  separate_longer_delim(genres, \",\") |&gt;\n  group_by(genres) |&gt;\n  summarize(total = n())\n\ntotal_successful_movies_genre &lt;- successful_movies_split_genres |&gt;\n  group_by(genres) |&gt;\n  summarize(total = n())\n\npercentage_successful &lt;- inner_join(total_movies_genre, total_successful_movies_genre, by = 'genres') |&gt;\n  mutate(percentage = (total.y / total.x) * 100,\n         highest = (percentage == max(percentage))) |&gt;\n  arrange(desc(percentage))\n\n\nAfter taking a closer look, I found that actually the Sci-Fi genre produced the highest percentage of successes with 3.46% of successful movies, followed by Adventure with 3.16%. While the genres Action and Drama had the most successes, this could have been due to the considerable amount of movies produced in those genres. Sometimes, it is less about the quantity produced and more about the quality being produced. Despite the Sci-Fi genre not producing nearly the same amount of films as the Action and Drama genres, the percentage of successful films to come out of this genre is higher than the others, which is why we need to consider the Sci-Fi genre as a successful genre as well.\n\n\nCode\npercentage_successful |&gt;\n  ggplot(aes(x = genres, y = percentage, fill = highest)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n  labs(title = \"Percentage of Successful Films by Genre Since 2010\",\n       x = \"Genres\",\n       y = \"Percentage of Successful Films\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"darkgrey\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nQuestion #4: What genre has become more popular in recent years?\nIn the below line plot, I limited our focus to 11 genres with 1 additional “Other” bucket to contain the remaining genres. All of these genres experience a jump in the amount of successful films produced between the 1990s and the 2000s, likely due to the increase in advanced movie production. Based on this data, we see that again, Drama takes a commanding lead for number of successful films up until 2010 when the Action genre overtakes the top spot. The Adventure genre is also not too far behind these two dominant genres. We are less than halfway through the 2020s decade, so the race is still close, but following the current pattern, it looks as though Action will continue to lead the genres in total number of successful movies.\n\n\nCode\n# limited to 11 genres with one \"Other\" bucket of genres, utilizing color brewer's suggestion of 12 maximum data classes\n\nsuccessful_movies_split_genres |&gt;\n  mutate(new_genres = fct_lump_n(genres, 11)) |&gt;\n  group_by(new_genres, decade) |&gt;\n  summarize(total = n()) |&gt;\n  ggplot(aes(x = decade, y = total, group = new_genres)) + \n  geom_line(aes(color = new_genres)) +\n  geom_point(aes(color = new_genres)) +\n  labs(title = \"Number of Successful Movies per Decade by Genre\",\n       x = \"Decade\",\n       y = \"Number of Successes\") +\n  scale_fill_discrete(name = \"Genres\") +\n  scale_y_continuous(label=scales::comma) +\n  scale_color_manual(values = c('#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928'))\n\n\n\n\n\n\n\n\n\nBased on our earlier analysis, I found that the Action and Drama genres have been the most popular genres in recent years, producing the most successes out of any categories. I also found that the Sci-Fi and Adventure genres produced a high percentage of successful movies since 2010. So, it may be advantageous to pursue one of these categories in my movie pitch. Taking a closer look at each of these genres, we can see that for the Action, Adventure and Drama genres, the distribution of success is skewed right. While for the Sci-Fi genre, the success distribution is a little more uniform, possibly suggesting that there would be less predictability in the success of a Sci-Fi movie as compared to the other genres which cut close with the success threshold.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres %in% c(\"Action\", \"Drama\", \"Sci-Fi\", \"Adventure\")) |&gt;\n  ggplot(aes(x = success)) + \n  geom_histogram(bins=30) +\n  facet_wrap(~genres) +\n  labs(title = \"Distribution of Success for Each Genre\",\n       x = \"Success\",\n       y = \"Number of Titles\") +\n  scale_x_log10(label=scales::comma) +\n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\nFrom the box plots below, each of the quartiles for the Sci-Fi genre edge ahead of the other genres. Despite the Sci-Fi genres low production of films, the genre can still compete by producing a high percentage of successful films.\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres %in% c(\"Action\", \"Drama\", \"Sci-Fi\", \"Adventure\")) |&gt;\n  ggplot(aes(x = genres, y = success)) + \n  geom_boxplot() +\n  xlab(\"Genres\") + \n  ylab(\"Success\") + \n  ggtitle(\"Distribution of Success for Each Genre\") + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsuccessful_movies_split_genres |&gt;\n  filter(genres == \"Sci-Fi\") |&gt;\n  group_by(decade) |&gt;\n  summarize(total = n()) |&gt;\n  ggplot(aes(x = decade, y = total)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Successful Sci-Fi Movies by Decade\",\n       x = \"Decade\",\n       y = \"Number of Titles\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nGenre Selection\nThe Drama genre’s dominance in the film industry was evident from the earliest days into the 21st century, with a consistently high output of successful movies. Drama movies succeeded in the 20th century, a period of significant social change and historical events, by evoking emotions and connecting with its audience’s experience.\nOver time, as the world experienced a technology boom, the movie industry also reflected this by observing a shift in genre success. Starting in the 2010s decade, the Action genre overtook Drama as the most dominant genre, producing more successful movies. The introduction of new technology like computer-generated imagery (CGI) and advanced stunt technology allowed for bolder and more fantastical movie production, propelling newer genres like Action to overtake traditional movies.\nHowever, despite the high output of successful Action films in more recent decades, its proportion of successful movies in relation to its total output lags behind genres like Sci-Fi and Adventure which each produce more successful movies per movie created in their respective genres. Since the Sci-Fi genre has a high success rate and somewhat of a low production (as compared to other genres), I would like to pursue a movie in the Sci-Fi genre."
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre",
    "href": "mp02.html#successful-personnel-in-the-genre",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Successful Personnel in the Genre",
    "text": "Successful Personnel in the Genre\n\nSuccessful Director\nFor my Sci-Fi movie, I would like to pick Steven Spielberg as my director. Spielberg is an experienced director, having directed many highly successful Sci-Fi movies in the past, one of them being the iconic and classic E.T. the Extra-Terrestrial.\n\n\nCode\n# data tables containing everyone's name basics and titles worked on \n\neveryone_projects &lt;- left_join(NAME_BASICS, TITLE_PRINCIPALS, by = 'nconst') |&gt;\n  left_join(movies_ratings_success, by = 'tconst') |&gt;\n  filter(!is.na(averageRating)) |&gt;\n  group_by(nconst) |&gt;\n  mutate(average_person_success = round(sum(success) / n(), 3),\n         total_movies = n())\n  \neveryones_work &lt;- everyone_projects |&gt;\n  select(nconst, primaryName, average_person_success, total_movies, tconst, category, primaryTitle, averageRating, numVotes, success, genres)\n\n\n\n\nCode\n# checking Steven Spielberg's success relative to all Sci-Fi directors\n\nscifi_directors_genre_split &lt;- everyones_work |&gt;\n  separate_longer_delim(genres, delim = \",\") |&gt;\n  filter(genres == \"Sci-Fi\", category == \"director\") |&gt;\n  arrange(desc(average_person_success))\n\nspielberg_success &lt;- scifi_directors_genre_split |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  slice_head(n = 1) |&gt;\n  pull(average_person_success)\n\nsteven_rank &lt;- scifi_directors_genre_split |&gt;\n  select(primaryName, average_person_success) |&gt;\n  mutate(above_steven = (average_person_success &gt;= spielberg_success)) |&gt;\n  group_by(above_steven) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(percent = total/sum(total)) |&gt;\n  filter(above_steven == TRUE) |&gt;\n  pull(percent)\n\ncat(\"Steven Spielberg Top \", round(steven_rank*100, 2), \"% of Sci-Fi Directors\")\n\n\nSteven Spielberg Top  1.69 % of Sci-Fi Directors\n\n\nIn the histogram below, Steven Spielberg actually ranks in the top 1.69% of Sci-Fi directors, solidifying his position as a highly successful director among the most top tier Sci-Fi directors. The blue dashed line indicates where Spielberg’s average success metric falls in relation to other Sci-Fi directors.\n\n\nCode\nscifi_directors_genre_split |&gt;\n  ggplot(aes(x = success)) +\n  geom_histogram(bins = 30) +\n  geom_vline(xintercept = spielberg_success, \n             color = \"deepskyblue\", \n             linetype = \"dashed\", \n             linewidth = 1) +\n  labs(title = \"Distribution of Success Indices for Sci-Fi Directors\",\n       x = \"Director's Average Success\",\n       y = \"Number of Directors\")\n\n\n\n\n\n\n\n\n\nTaking a closer look at the Sci-Fi films that Spielberg has produced, we can see that a large majority of the Sci-Fi films Spielberg has been a part of have been successes. The red dashed line indicates the success threshold of v = 0.2.\n\n\nCode\nscifi_directors_genre_split |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  ggplot(aes(x = primaryTitle, y = success)) +\n  geom_bar(stat = \"identity\", fill = \"#B2B0EA\")  +\n  geom_hline(yintercept = 0.2,\n             color = \"red\",\n             linetype = \"dashed\",\n             linewidth = 1) +\n  labs(title = \"Steven Spielberg Sci-Fi Movies\",\n       x = \"Movie Title\",\n       y = \"Success\")\n\n\n\n\n\n\n\n\n\nFrom the pie chart below, we can see that of the Sci-Fi movies Steven Spielberg has produced, 80% of them have been successes, indicating him as a great candidate to direct future Sci-Fi films.\n\n\nCode\n# splitting the genre column so that each cell under the genre column contains only one value\n\nsteven_spielberg_works_split_genres &lt;- steven_spielberg_works |&gt;\n  separate_longer_delim(genres, delim = \",\")\n\n# filtering for only Sci-Fi movies\n\nsteven_spielberg_scifi &lt;- steven_spielberg_works_split_genres |&gt;\n  filter(genres == \"Sci-Fi\") |&gt;\n  mutate(outcome = (success &gt;= 0.2))\n\n# pie chart showing percentage of successes vs non-successes\n\nsteven_spielberg_scifi |&gt;\n  group_by(outcome) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_movies = sum(total),\n         percentage = (total/total_movies)*100,\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = outcome)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"Steven Spielberg's Proportion of Successful Sci-Fi Films\") +\n  scale_fill_discrete(name = \"Movie Outcome\", \n                      labels = c(\"Unsuccessful\", \"Successful\"),\n                      guide = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\n\n\nActing Talent\nNext, I would like to pick an actor/actress that has worked with Spielberg on successful films in the past. Given Harrison Ford’s proven success with Steven Spielberg in the iconic Indiana Jones franchise, he would be a strong candidate for another lead role. Ford starred in all five films of the Indiana Jones franchise, the first four of which were directed by Spielberg and the last by James Mangold. Ford’s collaborations with Spielberg on the Indiana Jones franchise have consistently proven to be box office successes, which contrasts the performance of franchise’s fifth installment, directed by James Mangold. As shown in the plot below, the first four films exceeded the success threshold, while the last one fell short. Ford’s established work with Spielberg and their history of creating box office hits makes them a compelling choice.\n\n\nCode\n# filter only for the Indiana Jones franchise\n\nindiana_jones_movies &lt;- everyones_work |&gt;\n  filter(primaryTitle %in% c(\"Raiders of the Lost Ark\",\n                             \"Indiana Jones and the Temple of Doom\",\n                             \"Indiana Jones and the Last Crusade\",\n                             \"Indiana Jones and the Kingdom of the Crystal Skull\",\n                             \"Indiana Jones and the Dial of Destiny\")) |&gt;\n  filter(category == \"director\")\n\nmovie_order &lt;- c(\"Raiders of the Lost Ark\",\n                 \"Indiana Jones and the Temple of Doom\",\n                 \"Indiana Jones and the Last Crusade\",\n                 \"Indiana Jones and the Kingdom of the Crystal Skull\",\n                 \"Indiana Jones and the Dial of Destiny\")\n\n# barplot indicating success of each Indiana Jones film, color classified by director\n\nindiana_jones_movies |&gt;\n  ggplot(aes(x = primaryTitle, y = success, fill = primaryName)) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  geom_hline(yintercept = 0.2, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  labs(title = \"Indiana Jones Movies\",\n       x = \"Movie Title\",\n       y = \"Success Index\",\n       fill = \"Directors\") +\n  theme(legend.position = \"bottom\") +\n  scale_x_discrete(limits = movie_order) +\n  scale_fill_manual(values = c(\"Steven Spielberg\" = \"#519DE9\", \"James Mangold\" = \"#f94449\"),\n                    guide = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\nLastly, I would like to pick a younger and experienced actor to work alongside the talented Harrison Ford and Steven Spielberg. Below is the table breakdown of actors and actresses between the ages of 20 and 30, sorted by their respective average success metrics.\n\n\nCode\n# filtering for actors/actresses that have worked on Sci-Fi films under the age of 30\n\nscifi_young_actors_genre_split &lt;- everyone_projects |&gt;\n  separate_longer_delim(genres, delim = \",\") |&gt;\n  filter(is.na(deathYear)) |&gt;\n  mutate(age = 2024 - birthYear) |&gt;\n  filter(genres == \"Sci-Fi\", \n         category %in% c(\"actor\", \"actress\"),\n         age %in% (20:30)) |&gt;\n  arrange(desc(average_person_success))\n\nscifi_young_actors_genre_split |&gt;\n  select(primaryName, age, category, average_person_success) |&gt;\n  unique() |&gt;\n  arrange(desc(average_person_success)) |&gt;\n  DT::datatable(rownames = FALSE, \n                options = list(pageLength = 10))\n\n\n\n\n\n\nTaking a look at some of the top actors/actresses below the age of 30, I found that Tom Holland ranks very high up (being 5th in average success out of 526 actors/actresses) in the top 1%, with a success index of 0.234. Additionally, both of the Sci-Fi films he’s starred in have scored very high on the success index, demonstrating his existing experience in this genre.\n\n\nCode\ntom_holland_projects &lt;- scifi_young_actors_genre_split |&gt;\n  filter(primaryName == \"Tom Holland\")\n\ntom_holland_projects |&gt;\n  ggplot(aes(x = primaryTitle, y = success)) +\n  geom_bar(stat = \"identity\", width = 0.5, fill = \"#F4B678\") +\n  labs(title = \"Tom Holland's Sci-Fi Films\",\n       x = \"Movie Title\",\n       y = \"Success Index\")\n\n\n\n\n\n\n\n\n\nI knew I wanted to pick a young talent with experience as a lead in major films. Tom Holland with his impressive résumé, including his starring role as Spider-Man in the Marvel Cinematic Universe, presents him as an ideal candidate. Even with the blockbuster movies he’s already been a part of, Holland can benefit from continuing to work with experienced Hollywood talent, like Harrison Ford."
  },
  {
    "objectID": "mp02.html#nostalgia-and-remakes",
    "href": "mp02.html#nostalgia-and-remakes",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Nostalgia and Remakes",
    "text": "Nostalgia and Remakes\nAfter selecting our genre, director, and star actors for our film, we need to finalize a classic movie to remake for our pitch. Below are some of the top Sci-Fi classics released before the year 2000.\n\n\nCode\nscifi_successful_movies &lt;- successful_movies_split_genres |&gt;\n  filter(genres == \"Sci-Fi\",\n         startYear &lt;= 2000) |&gt;\n  arrange(desc(success))\n\nscifi_successful_movies |&gt;\n  DT::datatable(rownames = FALSE,\n                options = list(pageLength = 5))\n\n\n\n\n\n\nAfter taking a look at some of the most successful movies released before 2000 (without recent sequels or remakes in the past 25 years), I decided to choose Back to the Future from 1985 as my classic movie remake. Back to the Future is an Adventure, Comedy, and Sci-Fi film with a high success index of 0.57, high IMDb rating of 8.5 and high number of votes at 1,335,879 votes. There were three Back to the Future movies in the franchise, the first film had the most success with the highest average rating and number of votes. The last film of the franchise was released in 1990 and there have been no remakes or additional sequels since. In my Back to the Future remake, I plan to cast Harrison Ford as the scientist, Doc Brown, and Tom Holland as Marty McFly.\n\n\nCode\n# data frame containing all of the Back to the Future franchise movies\n\nback_to_the_future_franchise &lt;- movies_ratings_success |&gt;\n  filter(str_detect(primaryTitle, \"Back to the Future\")) |&gt;\n  select(primaryTitle, startYear, genres, averageRating, numVotes, success)\n\nback_to_the_future_franchise |&gt;\n  select(primaryTitle, startYear, averageRating, numVotes, success) |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\nSince the first film was released 39 years ago in 1985, we will need to check to confirm whether key actors, directors, or writers from the original are still alive. From the Back to the Future IMDb page, I found that the primary contributors to the film were Robert Zemeckis (director and writer), Bob Gale (writer), Michael J. Fox (actor), Christopher Lloyd (actor), and Lea Thompson (actor). Since all five of the primary contributors to the original film are still alive, I will need to contact the legal department to confirm that we can secure the rights to the project before I can proceed. As a fan service, I plan to also provide cameos to the stars of the original film if they are interested in partaking in the movie reboot.\n\n\nCode\noriginal_contributors_bttf &lt;- everyone_projects |&gt;\n  filter(primaryTitle == \"Back to the Future\") |&gt;\n  filter(primaryName %in% c(\"Robert Zemeckis\", \n                            \"Bob Gale\", \n                            \"Michael J. Fox\", \n                            \"Christopher Lloyd\", \n                            \"Lea Thompson\")) |&gt;\n  select(primaryName, birthYear, deathYear) |&gt;\n  unique() |&gt;\n  mutate(age = 2024 - birthYear)\n\noriginal_contributors_bttf |&gt;\n  DT::datatable(rownames = FALSE)"
  },
  {
    "objectID": "mp02.html#final-movie-pitch",
    "href": "mp02.html#final-movie-pitch",
    "title": "Mini Project #02: The Business of Show Business",
    "section": "Final Movie Pitch",
    "text": "Final Movie Pitch\n\nMovie Pitch\nThe Sci-Fi genre allows audiences to extend imaginations beyond the possibilities of the present, a genre we should consider. Over the past two decades, Sci-Fi has produced the highest percentage of successful films (3.46%). Furthermore, the Sci-Fi genre has increased the amount of successful movies produced from the 1980s-2010s by about 540%. Sci-Fi, unlike more saturated markets, offers a less crowded landscape, providing opportunities for emerging films.\nSteven Spielberg ranks in the top 1.69% of all-time Sci-Fi directors. Additionally, 80% of the Sci-Fi movies he’s directed have been box office successes. Spielberg’s successful track record in the Sci-Fi genre with movies like E.T. the Extra-Terrestrial, justifies him as a top-tier choice to direct another successful Sci-Fi movie.\nHarrison Ford’s and Steven Spielberg’s collaboration on the Indiana Jones franchise resulted in an 100% success rate with four movie hits. Given their history of success together, their collaboration will likely lead to another hit. Tom Holland, a young and experienced actor, has proven his ability to lead blockbuster films and ranks in the top 1% of successful Sci-Fi actors/actresses between ages 20-30. Ford’s and Holland’s experiences and household names will only boost the movie’s reach and potential.\nBack to the Future is an iconic box office hit from the mid-80s, which is due for a modern day remake. Recreating this classic Sci-Fi film, with the expertise of Steven Spielberg alongside star powers Harrison Ford and Tom Holland will surely produce a box office hit.\n\n\nTrailer\nFrom director Steven Spielberg, the visionary mind behind E.T. the Extra-Terrestrial; and from actor Harrison Ford, Hollywood icon and star of Indiana Jones; and from actor Tom Holland, popular upcoming star of the Sci-Fi genre, comes the timeless tale, Back to the Future. A story of adventure, time travel, and imagination. Coming soon to a theater near you!"
  },
  {
    "objectID": "mp03.html#set-up-and-initial-exploration",
    "href": "mp03.html#set-up-and-initial-exploration",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Set-Up and Initial Exploration",
    "text": "Set-Up and Initial Exploration\nBelow are some useful packages we will need to utilize throughout our US election data analysis.\n\n\nCode\n# Install necessary packages\n\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"sf\")) install.packages(\"sf\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"DT\")) install.packages(\"DT\")\nif(!require(\"gt\")) install.packages(\"gt\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif(!require(\"stringr\")) install.packages(\"stringr\")\nif(!require(\"patchwork\")) install.packages(\"patchwork\")\nif(!require(\"gganimate\")) install.packages(\"gganimate\")\nif(!require(\"zoom\")) install.packages(\"zoom\")\nif(!require(\"gridExtra\")) install.packages(\"gridExtra\")\n\n# Load packages into R\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(haven)\nlibrary(DT)\nlibrary(gt)\nlibrary(ggplot2)\nlibrary(RColorBrewer) # different color palette options\nlibrary(stringr)\nlibrary(patchwork) # inset plots\nlibrary(gganimate)\nlibrary(zoom) # zoom for plots\nlibrary(gridExtra) # labels outside the plot\n\n\n\nData I: US House Election Votes from 1976 to 2022\nFor our analysis, we will be downloading data from the MIT Election Data Science Lab, which collects votes from all biennial congressional races in each state from 1976 to 2022 as well as the statewide presidential vote counts from 1976 to 2020. We will need to download these files from the web and read them in as follows.\n\n\nCode\nhouse_1976_2022 &lt;- read.csv(\"1976-2022-house.csv\")\n\n\n\n\nCode\npresident_1976_2020 &lt;- read.csv(\"1976-2020-president.csv\")\n\n\n\n\nData II: Congressional Boundary Files 1976 to 2012\nNext, to visualize the past election results onto a US map, we will have to download the district shapefiles for the US from 1976 to 2022. We will download the US district shape files from 1976 to 2012 from Lewis et al. automatically with the following code.\n\n\nCode\n# Function to download district shape zip files from Jeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis\n\nget_district_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n    fname_ext &lt;- paste0(fname, \".zip\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n}\n\n# For loop to download district095 to district112 zip files\n\nfor (i in 95:112) {\n  filename &lt;- case_when(i &lt; 100 ~ paste0(\"districts0\", as.character(i)),\n                        i &gt;= 100 ~ paste0(\"districts\", as.character(i)))\n  get_district_file(filename)\n}\n\n\n\n\nData III: Congressional Boundary Files 2014 to Present\nAdditionally, for more recent elections from 2014 to 2022, we can download shapefiles from the US Census Bureau. The following code will automatically download these congressional district shapefiles.\n\n\nCode\n# Function to download district shape zip files from US Census Bureau\n\nget_district_file_census &lt;- function(fname, year){\n    BASE_URL &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", as.character(year), \"/CD/\")\n    fname_ext &lt;- paste0(fname, \".zip\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n}\n\n# For loop to download congressional shapefiles (zip files) from 2014-2022\n\nfor (i in 2014:2022) {\n  filename &lt;- case_when(i &lt; 2016 ~ paste0(\"tl_\", as.character(i), \"_us_cd114\"),\n                        i &gt;= 2016 & i &lt; 2018 ~ paste0(\"tl_\", as.character(i), \"_us_cd115\"),\n                        i &gt; 2017 ~ paste0(\"tl_\", as.character(i), \"_us_cd116\"))\n  get_district_file_census(filename, i)\n}\n\n\nAdditionally for ease of plotting state geometries later in the project, the below code will download the state shape file from 2020 (the most recent presidential election that we have access to).\n\n\nCode\n# Function to download district shape zip files from US Census Bureau\n\nget_state_file_census &lt;- function(fname, year){\n    BASE_URL &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", as.character(year), \"/STATE/\")\n    fname_ext &lt;- paste0(fname, \".zip\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n}\n\n# Download state shapefiles zip folder from 2020\n\nget_state_file_census(\"tl_2020_us_state\", 2020)"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Every four years, the US holds a presidential election, where citizens go to polling sites to cast their votes for potential presidential candidates. The way these popular votes are handled in each state have varied over time. Though, the following has remained the same:\n\nIn each of the 50 states, there are R + 2 electoral college votes (where R is the number of Representatives the state has in the US House of Representatives)\nFor the purposes of this assignment, we will be counting each distinct district as one Representative\nStates are able to allocate these ECVs however they want\nThe candidate that receives the majority of ECVs becomes the President\n\nThere are essentially no rules in place on how the R + 2 ECVs for each state are allocated in the Constitution. Thus, at different points in time, various states have allocated their ECVs with the following methods:\n\nDirect allocation of ECVs by state legislature (no vote)\nAllocation of all ECVs to winner of state-wide popular vote\nAllocation of all ECVs to winner of nation-wide popular vote\nAllocation of R ECVs to popular vote winner by congressional district + allocation of remaining 2 ECVs to the state-wide popular vote winner\n\nCurrently, 48 states use the state-wide popular vote ECV allocation method. The only two states that have diverged from this are Maine and Nebraska, which use the final option.\nThe goal for this project is to explore the various electoral college vote (ECV) allocation methods for the presidential elections and assess the outcomes of each election had these methods been different.\nWe will be using data from the MIT Election Data Science Lab1 which has collected votes from all biennial congressional races in all 50 states and the statewide presidential vote counts from 1976 to 2022. Furthermore to assist with our map data visualization, we will be utilizing the congressional and/or state shape files created by Lewis et al. (1976-2012) and the US Census Bureau (2014-2022). With this data, we hope to visualize past election outcomes and assess the fairness of various ECV allocation schemes."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Every four years, the US holds a presidential election, where citizens go to polling sites to cast their votes for potential presidential candidates. The way these popular votes are handled in each state have varied over time. Though, the following has remained the same:\n\nIn each of the 50 states, there are R + 2 electoral college votes (where R is the number of Representatives the state has in the US House of Representatives)\nFor the purposes of this assignment, we will be counting each distinct district as one Representative\nStates are able to allocate these ECVs however they want\nThe candidate that receives the majority of ECVs becomes the President\n\nThere are essentially no rules in place on how the R + 2 ECVs for each state are allocated in the Constitution. Thus, at different points in time, various states have allocated their ECVs with the following methods:\n\nDirect allocation of ECVs by state legislature (no vote)\nAllocation of all ECVs to winner of state-wide popular vote\nAllocation of all ECVs to winner of nation-wide popular vote\nAllocation of R ECVs to popular vote winner by congressional district + allocation of remaining 2 ECVs to the state-wide popular vote winner\n\nCurrently, 48 states use the state-wide popular vote ECV allocation method. The only two states that have diverged from this are Maine and Nebraska, which use the final option.\nThe goal for this project is to explore the various electoral college vote (ECV) allocation methods for the presidential elections and assess the outcomes of each election had these methods been different.\nWe will be using data from the MIT Election Data Science Lab1 which has collected votes from all biennial congressional races in all 50 states and the statewide presidential vote counts from 1976 to 2022. Furthermore to assist with our map data visualization, we will be utilizing the congressional and/or state shape files created by Lewis et al. (1976-2012) and the US Census Bureau (2014-2022). With this data, we hope to visualize past election outcomes and assess the fairness of various ECV allocation schemes."
  },
  {
    "objectID": "mp03.html#initial-exploration-of-vote-count-data",
    "href": "mp03.html#initial-exploration-of-vote-count-data",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Initial Exploration of Vote Count Data",
    "text": "Initial Exploration of Vote Count Data\nBefore beginning any specific deep-dive analysis, it is important to conduct some preliminary exploration of our data sets to understand the information that we have.\n\n\n\n\n\n\nPreliminary Questions\n\n\n\nBelow are some preliminary questions we will answer as an initial exploration of our data\n\nWhich states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\nNew York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes they received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nDo presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n\n\nQuestion 1: Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\nFirst, we would like to take a look at the change in US House seats from 1976 to 2022, to determine if there are any significant changes. There are a total of 435 members of the House of Representatives, each state is allowed a specific number of representatives based on their population size. Over time, certain states have gained and/or lost seats based on population changes. Since the US House seats directly impact the number of electoral college votes a state receives, this is an important metric to take a look at for our analysis later.\n\n\nCode\n# Data frame with the difference in seat changes from 1976 to 2022\n\nseat_change_1976_2022 &lt;- house_1976_2022 |&gt;\n  select(c(year, state, district)) |&gt;\n  unique() |&gt;\n  group_by(year, state) |&gt;\n  summarize(total = n()) |&gt;\n  filter(year == 1976 | year == 2022) |&gt;\n  pivot_wider(names_from = year, values_from = total) |&gt;\n  mutate(difference = `2022` - `1976`,\n         positive = (difference &gt;= 0))\n\n# States with no change\n\nno_change &lt;- seat_change_1976_2022 |&gt;\n  filter(difference == 0) |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\nno_change_seats &lt;- seat_change_1976_2022 |&gt;\n  filter(difference == 0) |&gt;\n  pull(state) |&gt;\n  c()\n\n# States that gained seats\n\ngain_seats &lt;- seat_change_1976_2022 |&gt;\n  filter(difference &gt; 0) |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\n# States that lost seats\n\nlost_seats &lt;- seat_change_1976_2022 |&gt;\n  filter(difference &lt; 0) |&gt;\n  summarize(total = n()) |&gt;\n  pull(total)\n\n# State that gained the most seats\n\ngained_most_seats &lt;- seat_change_1976_2022 |&gt;\n  slice_max(difference, n = 1) |&gt;\n  pull(difference)\n\n# State that lost the most seats\n\nlost_most_seats &lt;- seat_change_1976_2022 |&gt;\n  slice_min(difference, n = 1) |&gt;\n  mutate(lost = -difference) |&gt;\n  pull(lost)\n\n\n# Plot of all the states and their respective seat changes from 1976 to 2022\n\nseat_change_1976_2022 |&gt;\n  filter(difference != 0) |&gt;\n  ggplot(aes(x = reorder(state, difference), y = difference)) +\n  geom_bar(aes(fill = positive), stat = \"identity\", show.legend = FALSE) +\n  labs(title = \"States with House Seats Gained or Lost (1976-2022)\",\n       x = \"State\",\n       y = \"Change in Seats\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"TRUE\" = \"seagreen1\", \"FALSE\" = \"firebrick1\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nAfter gathering the house seat change from 1976 to 2022, I found that 16 states had no seat change (Alabama, Alaska, Arkansas, Delaware, Hawaii, Idaho, Maine, Maryland, Minnesota, Montana, Nebraska, New Hampshire, North Dakota, Rhode Island, Vermont, Wyoming), while 34 states did. Of these 34 states, 15 states gained seats while 19 states lost seats. Texas gained the most seats from 1976 with 14 gained seats. While New York lost the most seats from 1976 with 13 lost seats.\nQuestion 2: New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent).\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes they received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nThe “fusion” voting system allows candidates that have their name appear on multiple party lines on the ballot to have their votes counted under one total. We will take a look at whether the fusion voting system creates a substantial change in candidates’ success. We will do so by taking a look at election outcomes with and without the fusion voting system.\nFrom the data table below, we find that there were 24 instances when election outcomes differed between the fusion voting system and the single party system. Almost all of these outcomes came from New York elections, with only one from Connecticut in 1992.\n\n\nCode\n# Data frame with the highest votes for each years' election per state and district (without fusion) and the candidates that would have won\n\nwinner_no_fusion_votes &lt;- house_1976_2022 |&gt;\n  group_by(year, state, district, candidate, party) |&gt;\n  summarize(party_votes = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year, state, district) |&gt;\n  filter(party_votes == max(party_votes)) |&gt;\n  ungroup() |&gt;\n  select(c(year, state, district, candidate))\n\n# Data frame with the highest votes for each years' election per state and district (with fusion) and the candidates that won\n\nwinner_fusion_votes &lt;- house_1976_2022 |&gt;\n  group_by(year, state, district, candidate) |&gt;\n  summarize(candidate_total = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year, state, district) |&gt;\n  filter(candidate_total == max(candidate_total)) |&gt;\n  ungroup() |&gt;\n  select(c(year, state, district, candidate))\n  \n# Data frame combining above two data tables to compare winners with and without fusion\n\ncomparison_winner &lt;- left_join(winner_fusion_votes, winner_no_fusion_votes, by = c(\"year\", \"state\", \"district\")) |&gt;\n  rename(winner_fusion = candidate.x,\n         winner_no_fusion = candidate.y) |&gt;\n  mutate(same_winner = (winner_fusion == winner_no_fusion)) |&gt;\n  filter(same_winner == FALSE)\n\n# Display a DT() data table of the elections that would've been different without the fusion voting system\n\ncomparison_winner_df &lt;- comparison_winner |&gt;\n  select(c('year', 'state', 'district', 'winner_fusion', 'winner_no_fusion'))\n\nDT::datatable(setNames(comparison_winner_df, c(\"Year\", \"State\", \"District\", \"Fusion Winner\", \"No Fusion Winner\")), \n              caption = \"Table 1: Differences in House Seat Winners with Fusion and Without Fusion\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\nQuestion 3: Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\nLastly, we would like to explore whether the total presidential candidate votes per party exceeds or falls below the total votes of all of their party’s congressional candidates per state.\nBelow is the data frame comparing the difference between presidential candidate votes versus the total votes of their co-partisans per state.\n\n\nCode\n# Data frame counting total votes for each party in each state from house data\n\nhouse_party_votes_per_state &lt;- house_1976_2022 |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(house_party_votes = sum(candidatevotes))\n\n# Data frame counting total votes for each party in each state from president data\n\npresident_party_votes_per_state &lt;- president_1976_2020 |&gt;\n  filter(writein == FALSE) |&gt; # otherwise will have candidates that weren't originally on the ballot\n  select(year, state, party_detailed, candidate, candidatevotes) |&gt;\n  rename(party = party_detailed)\n\n# Data frame combining president and house data sets to compare total votes\n# Filtered for the 2 main parties: Democrat and Republican\n\nvote_comparison_president_house &lt;- left_join(president_party_votes_per_state,\n                                             house_party_votes_per_state,\n                                             by = c('year', 'state', 'party')) |&gt;\n  rename(presidential_candidate = candidate,\n         president_candidatevotes = candidatevotes) |&gt;\n  drop_na(house_party_votes) |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  mutate(president_more = (president_candidatevotes &gt; house_party_votes),\n         diff_p_h = president_candidatevotes - house_party_votes) |&gt;\n  filter(presidential_candidate != \"\") # one record has an empty string\n\n# Data frame output: showing the difference in votes\n\nvote_comparison_final &lt;- vote_comparison_president_house |&gt;\n  select(year, state, party, presidential_candidate, diff_p_h)\n  \n\nDT::datatable(setNames(vote_comparison_final, c(\"Year\", \"State\", \"Party\", \"Presidential Candidate\", \"Difference in Votes\")), \n              caption = \"Table 2: Difference in Presidential Votes vs Co-Partisans per State per Election Year\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\nAdditionally, below are animated bar plots transitioning between election years demonstrating the change in amounts of votes for the president versus their co-partisans across all 50 states and the District of Columbia from 1976 to 2020. We use blue to represent the Democratic Party and red to represent the Republican Party.\n\n\nCode\n# Animated plot showing the difference between presidential votes versus house votes each year in each state for the Democratic Party\n\ndem &lt;- vote_comparison_president_house |&gt;\n  filter(party == \"DEMOCRAT\") |&gt;\n  ggplot(aes(x = state, y = diff_p_h)) +\n  geom_bar(stat = \"identity\", fill = \"royalblue1\") +\n  coord_flip() +\n  transition_states(year) +\n  theme_bw() +\n  labs(title = \"Voting Patterns Presidential Candidate vs Co-Partisans ({closest_state})\",\n       subtitle = \"Democratic Party\",\n       x = \"State\",\n       y = \"Difference in Votes\")\n\nanimate(dem)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Animated plot showing the difference between presidential votes versus house votes each year in each state for the Democratic Party\n\nrep &lt;- vote_comparison_president_house |&gt;\n  filter(party == \"REPUBLICAN\") |&gt;\n  ggplot(aes(x = state, y = diff_p_h)) +\n  geom_bar(stat = \"identity\", fill = \"tomato\") +\n  coord_flip() +\n  transition_states(year) +\n  theme_bw() +\n  labs(title = \"Voting Patterns Presidential Candidate vs Co-Partisans ({closest_state})\",\n       subtitle = \"Republican Party\",\n       x = \"State\",\n       y = \"Difference in Votes\")\n\nanimate(rep)\n\n\n\n\n\n\n\n\n\nTo get a more general view of this data, we take the average difference among all states for each election year and compare these changes over time for the Republican Party versus the Democratic Party. Below is a dumbbell plot examining these changes over time, the yellow dashed line indicates where on average the difference between presidential votes and the party’s congressional district votes would have been 0 (no difference).\n\n\nCode\n# Average difference among all 50 states + DC between presidential votes and house votes in each election year\n\naverage_vote_comparison_president_house &lt;- vote_comparison_president_house |&gt;\n  group_by(year, party, presidential_candidate) |&gt;\n  summarize(average_diff = mean(diff_p_h)) |&gt;\n  ungroup()\n\naverage_vote_comparison_president_house |&gt;\n  slice_min(average_diff) |&gt;\n  pull(average_diff)\n\n\n[1] -110593.7\n\n\nCode\n# Graph the average change over time with a dumbbell plot\n\naverage_vote_comparison_president_house |&gt;\n  ggplot(aes(x = year, y = average_diff, color = party)) +\n  geom_segment(aes(xend = year, yend = -111000), color = \"grey\") +\n  geom_point(aes(color = party)) +\n  geom_hline(linetype = \"dashed\", color = \"gold\", yintercept = 0) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"royalblue1\", \"REPUBLICAN\" = \"tomato\"),\n                     name = \"Party\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Average Difference in Presidential Votes vs House Votes (1976-2020)\",\n       x = \"Election Year\",\n       y = \"Difference in Votes\")\n\n\n\n\n\n\n\n\n\nFrom our plot above, we find that the Democratic Party until 1996 experienced on average less votes for the presidential candidate than its co-partisans did across the 50 states and DC. While the Republican Party only saw a drop in its presidential elections in 1992, 1996, and 2016. In more recent years, it seems that for both parties, the presidential candidate generally has been more popular than its co-partisans."
  },
  {
    "objectID": "mp03.html#preliminary-questions",
    "href": "mp03.html#preliminary-questions",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Preliminary Questions",
    "text": "Preliminary Questions\nBelow are some preliminary questions we will answer as an initial exploration of our data 1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022? 2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes they received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines? 3. Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?"
  },
  {
    "objectID": "mp03.html#importing-and-plotting-shape-file-data",
    "href": "mp03.html#importing-and-plotting-shape-file-data",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Importing and Plotting Shape File Data",
    "text": "Importing and Plotting Shape File Data\nBelow is the code used to read in the shape files from the zip archives we downloaded earlier, this will automatically read in only the shape file (shp) from each of these archives.\n\n\nCode\n# For loop loading all the SHP files from district 95 to 112 from Lewis et al.\n\nfor (i in 95:112) {\n  td &lt;- tempdir(); \n  filename &lt;- case_when(i &lt; 100 ~ paste0(\"districts0\", as.character(i)),\n                        i &gt;= 100 ~ paste0(\"districts\", as.character(i)))\n  zip_contents &lt;- unzip(paste0(filename, \".zip\"), \n                        exdir = td)\n      \n  fname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  assign(paste0(\"districts_\", as.character(i), \"_sf\"), read_sf(fname_shp))\n}\n\n# For loop loading all the Tiger/Line SHP files from US Census Bureau (2014 to 2022)\n\nfor (i in 2014:2022) {\n  td &lt;- tempdir(); \n  filename &lt;- case_when(i &lt; 2016 ~ paste0(\"tl_\", as.character(i), \"_us_cd114\"),\n                        i &gt;= 2016 & i &lt; 2018 ~ paste0(\"tl_\", as.character(i), \"_us_cd115\"),\n                        i &gt; 2017 ~ paste0(\"tl_\", as.character(i), \"_us_cd116\"))\n  zip_contents &lt;- unzip(paste0(filename, \".zip\"), \n                        exdir = td)\n      \n  fname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  assign(paste0(\"t1_\", as.character(i), \"_sf\"), read_sf(fname_shp))\n}\n\n# Loading the state shape file for 2020\n\ntd &lt;- tempdir(); \nfilename &lt;- \"tl_2020_us_state\"\nzip_contents &lt;- unzip(paste0(filename, \".zip\"), \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nassign(paste0(\"state_2020_sf\"), read_sf(fname_shp))\n\n\n\nChoropleth Visualization of the 2000 Presidential Election Electoral College Results\nTypically, during and immediately after election day, we often see choropleth maps depicting the voting results of each state in real time. Next, we would like to recreate one of these choropleth maps for the 2000 election between Al Gore and George W. Bush. In this map, we will use the traditional blue color for the Democratic candidate, Al Gore, and red color to represent the Republican candidate, George W. Bush.\n\n\nCode\n# Data frame filtering out only for the winner of each state in the 2000 election\n\nwinner_election_2000 &lt;- president_1976_2020 |&gt;\n  filter(year == 2000) |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(state) |&gt;\n  mutate(most_votes = max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(win = (candidatevotes == most_votes),\n         winner = case_when(win == TRUE ~ candidate)) |&gt;\n  drop_na(winner)\n\n\nUsing the district 107 shape file from the 2000 election, we get the plot below, where each state is colored by the party that won the most votes in the state. Since we used a district shape file to visualize the election outcome, we can also see the various districts that certain states divide into.\nFor easier viewing, I decided to inset Hawaii and Alaska instead of plotting them at their true map locations. Below is the code to plot the US states on the mainland (everything excluding Alaska and Hawaii).\n\n\nCode\n# Selecting only state and party_simplified columns to simplify when joining with sf\n\nstate_color_2000_district &lt;- winner_election_2000 |&gt;\n  select(state, candidate, party_simplified) |&gt;\n  mutate(candidate_last = case_when(candidate == \"BUSH, GEORGE W.\" ~ \"Bush\",\n                                    candidate == \"GORE, AL\" ~ \"Gore\")) |&gt;\n  mutate(state = str_to_title(state))\n\n# Filtering only for states in the mainland so our plot isn't squished\n\nmainland_district &lt;- districts_107_sf |&gt;\n  filter(STATENAME != \"Alaska\" & STATENAME != \"Hawaii\")\n\n# Merging the two tables so we know which party won the election for each state\n\nmainland_colors_district &lt;- left_join(mainland_district, state_color_2000_district, join_by(\"STATENAME\" == \"state\"))\n\n# Plot of mainland with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nmainland_plot_district &lt;- mainland_colors_district |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"2000 Presidential Election Electoral College Results: Gore vs. Bush\")\n\n\nHere is the code which plots for the remaining two states: Hawaii and Alaska.\n\n\nCode\n# Filtering only for Alaska\n\nalaska_district &lt;- districts_107_sf |&gt;\n  filter(STATENAME == \"Alaska\")\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nalaska_colors_district &lt;- left_join(alaska_district, state_color_2000_district, join_by(\"STATENAME\" == \"state\"))\n\n# Plot of Alaska with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nalaska_plot_district &lt;- alaska_colors_district |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  coord_sf(xlim = c(170, 250)) +\n  theme_void() +\n  guides(fill = FALSE)\n\n# Filtering only for Hawaii\n\nhawaii_district &lt;- districts_107_sf |&gt;\n  filter(STATENAME == \"Hawaii\")\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nhawaii_colors_district &lt;- left_join(hawaii_district, state_color_2000_district, join_by(\"STATENAME\" == \"state\"))\n\n# Plot of Hawaii with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nhawaii_plot_district &lt;- hawaii_colors_district |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  labs(x = NULL, y = NULL) +\n  theme_void() +\n  guides(fill = FALSE)\n\n\nLastly, here is the code that puts all three of our plots together onto one plot for easy viewing.\n\n\nCode\n# Combined mainland plot with insetted elements Alaska and Hawaii plots on the lower left\n\nmainland_plot_district +\n  inset_element(hawaii_plot_district, 0, 0, 0.3, 0.3) +\n  inset_element(alaska_plot_district, 0, 0, 0.2, 0.4)\n\n\n\n\n\n\n\n\n\nTo avoid the overwhelming lines on top of the map, we can use a state shape file to more easily visualize the state outlines without having the district outlines. Below is another choropleth map with the same coloring that includes only the shape outlines and also the ECV counts for each state. The same process from above is repeated to inset Hawaii and Alaska onto our map for easier viewing.\n\n\nCode\n# Selecting only state and party_simplified columns to simplify when joining with sf\n\nstate_color &lt;- winner_election_2000 |&gt;\n  select(state, candidate, party_simplified) |&gt;\n  mutate(candidate_last = case_when(candidate == \"BUSH, GEORGE W.\" ~ \"Bush\",\ncandidate == \"GORE, AL\" ~ \"Gore\"))\n\n# Adding the respective ECV count for each state in 2000\n\n  # Data frame with ECV count from 2000\n\necv_per_state_2000_no_dc &lt;- house_1976_2022 |&gt;\n  select(c(year, state, district)) |&gt;\n  unique() |&gt;\n  filter(year == 2000) |&gt;\n  group_by(state) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(ecv_total = total + 2) |&gt;\n  select(-c(total))\n\n  # Adding DC's 2 ECVs for that election --&gt; One Democratic elector abstained from casting a vote\n  \necv_per_state_2000 &lt;- rbind(ecv_per_state_2000_no_dc, data.frame(state = \"DISTRICT OF COLUMBIA\", ecv_total = 2))\n\n  # Merging data frame state_color with ecv counts\n  \nstate_color_ecv &lt;- left_join(state_color, ecv_per_state_2000, by = \"state\")\n\n# Filtering only for states in the mainland so our plot isn't squished, also filtering out places that aren't in the 50 states\n\nmainland &lt;- state_2020_sf |&gt;\n  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"United States Virgin Islands\", \"Commonwealth of the Northern Mariana Islands\", \"Guam\", \"American Samoa\", \"Puerto Rico\"))) |&gt;\n  mutate(NAME = toupper(NAME))\n\n# Merging the two tables so we know which party won the election for each state\n\nmainland_colors &lt;- left_join(mainland, state_color_ecv, join_by(\"NAME\" == \"state\"))\n\n# Plot of mainland with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nmainland_plot &lt;- mainland_colors |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  geom_sf_text(aes(label = ecv_total), color = \"black\", size = 2) +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"2000 Presidential Election Electoral College Results: Gore vs. Bush\")\n\n\n\n\nCode\n# Filtering only for Alaska\n\nalaska &lt;- state_2020_sf |&gt;\n  filter(NAME == \"Alaska\") |&gt;\n  mutate(NAME = toupper(NAME))\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nalaska_colors &lt;- left_join(alaska, state_color_ecv, join_by(\"NAME\" == \"state\"))\n\n# Plot of Alaska with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nalaska_plot &lt;- alaska_colors |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  geom_sf_text(aes(label = ecv_total), color = \"black\", size = 2) +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  coord_sf(xlim = c(170, 250)) +\n  theme_void() +\n  guides(fill = FALSE)\n\n# Filtering only for Hawaii\n\nhawaii &lt;- state_2020_sf |&gt;\n  filter(NAME == \"Hawaii\") |&gt;\n  mutate(NAME = toupper(NAME))\n\n# Merging the two tables (alaska and state_color_2000) so we know which party won the election for Alaska\n\nhawaii_colors &lt;- left_join(hawaii, state_color_ecv, join_by(\"NAME\" == \"state\"))\n\n# Plot of Hawaii with colors corresponding to the parties (Democrat = Blue, Republican = Red)\n\nhawaii_plot &lt;- hawaii_colors |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = candidate_last)) +\n  geom_sf() +\n  geom_sf_text(aes(label = ecv_total), color = \"black\", size = 2) +\n  scale_fill_manual(values = c(\"Gore\" = \"royalblue1\", \"Bush\" = \"tomato\")) +\n  labs(x = NULL, y = NULL) +\n  theme_void() +\n  guides(fill = FALSE)\n\n\n\n\nCode\nmainland_plot +\n  inset_element(hawaii_plot, 0, 0, 0.3, 0.3) +\n  inset_element(alaska_plot, 0, 0, 0.2, 0.4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Count the total ECVs per candidate to confirm the winner of the 2000 Presidential Election\n\nwinner_2000 &lt;- state_color_ecv |&gt;\n  group_by(candidate_last) |&gt;\n  summarize(ecv = sum(ecv_total))\n\n# Pull the last name of the candidate that won the 2000 Election -- Bush\n\nwinner_2000_name &lt;- winner_2000 |&gt;\n  slice_max(ecv, n = 1) |&gt;\n  pull(candidate_last)\n\n# Pull the total ECVs the winning candidate received in the 2000 Election -- 271\n\nwinner_2000_count &lt;- winner_2000 |&gt;\n  slice_max(ecv, n = 1) |&gt;\n  pull(ecv)\n\n# Pull total ECVs of the losing candidate from the 2000 Election -- 266\n\nloser_2000_count &lt;- winner_2000 |&gt;\n  slice_min(ecv, n = 1) |&gt;\n  pull(ecv)\n\n\nFrom our new choropleth plot above, there is a clearer indication of state lines as well as the ECVs allocated per state. At first glance without these ECVs, it seems as though a large majority of the map is colored red, for George W. Bush. Taking a closer look at the total ECV count for each candidate, we find that ultimately Bush won the election by a slim margin, 271 ECVs to Al Gore’s 266.\n\n\nFaceted and Animated Plots Presidental Elections (1976-2020)\nNow let’s expand our findings beyond the 2000 presidential election to all the presidential elections from 1976-2020. We could do so with a faceted plot shown below. Each facet represents one of 12 presidential elections that occurred between this time period. It is interesting to visualize the changes in voting patterns for certain states over time.\n\n\nCode\n# Data frame with the winner for each state in each election\n\nwinner_each_election_per_state &lt;- president_1976_2020 |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state) |&gt;\n  mutate(most_votes = max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(win = (candidatevotes == most_votes),\n         winner = case_when(win == TRUE ~ candidate)) |&gt;\n  drop_na(winner)\n\n# Change the case of the states column in our state shape file\n\nstate_2020_sf_case &lt;- state_2020_sf |&gt;\n  filter(!(NAME %in% c(\"United States Virgin Islands\", \"Commonwealth of the Northern Mariana Islands\", \"Guam\", \"American Samoa\", \"Puerto Rico\"))) |&gt;\n  mutate(NAME = toupper(NAME))\n  \n\n# Data frame merging the winner from each state in each election with the shape file\n\nwinner_colors &lt;- left_join(state_2020_sf_case, winner_each_election_per_state, join_by(\"NAME\" == \"state\"))\n\n\n\n\nCode\nwinner_colors |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = party_simplified)) +\n  geom_sf() +\n  coord_sf(xlim = c(170, 300)) +\n  scale_fill_manual(name = \"Party\", values = c(\"DEMOCRAT\" = \"royalblue1\", \"REPUBLICAN\" = \"tomato\")) +\n  theme_bw() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"Presidential Election Outcomes (1976-2020)\") +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n\n\nAdditionally, I added an animated version of the plots to demonstrate the election results over time. Each animated state of the plot is a different election, demonstrating how some states have changed their voting patterns over time.\n\n\nCode\nanimated_plot &lt;- winner_colors |&gt;\n  st_shift_longitude() |&gt;\n  ggplot(aes(geometry = geometry,\n             fill = party_simplified)) +\n  geom_sf() +\n  coord_sf(xlim = c(170, 300)) +\n  scale_fill_manual(name = \"Party\", values = c(\"DEMOCRAT\" = \"royalblue1\", \"REPUBLICAN\" = \"tomato\")) +\n  theme_bw() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"Presidential Election Outcomes ({closest_state})\") +\n  transition_states(year, transition_length = 0, state_length = 1)\n\nanimate(animated_plot, renderer = gifski_renderer(file = paste0(directory, \"/election_animated_plot.gif\")))"
  },
  {
    "objectID": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "href": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Comparing the Effects of ECV Allocation Rules",
    "text": "Comparing the Effects of ECV Allocation Rules\nCurrently, electoral college votes are allocated in a state-wide winner-take-all fashion for 48 states and the District of Columbia. For the remaining two states, Nebraska and Maine, a district-wide winner-take-all + state-wide “at large” votes strategy is utilized. Throughout the course of history, various strategies have been used. This begs the question if election results would have been significantly different if all states used different ECV allocation rules.\nLastly, we will be taking a look at the various ECV allocation strategies to determine if any significant election changes would have occurred with alternative methods. We will be allocating ECVs with the following strategies:\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\n\n\nCode\n# Data frame that counts the ECV for each state each year\n\necv_per_state_per_year &lt;- house_1976_2022 |&gt;\n  select(c(year, state, district)) |&gt;\n  unique() |&gt;\n  group_by(year, state) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(ecv_total = total + 2) |&gt;\n  select(-c(\"total\")) |&gt;\n  ungroup()\n\n# Only 2020 includes DC's 3 ECVs, so we needto include DC's 3 votes for the other elections\n\necv_per_state_per_year &lt;- ecv_per_state_per_year |&gt;\n  add_row(year = 1976, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1980, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1984, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1988, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1992, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 1996, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2000, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2004, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2008, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2012, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2016, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  add_row(year = 2020, state = \"DISTRICT OF COLUMBIA\", ecv_total = 3) |&gt;\n  distinct()\n\n\n\nState-Wide Winner-Take-All\n\nBelow is the results of the election with an ECV allocation method of State-Wide Winner-Take-All, where the winner of the popular vote for each states wins all ECVs for that respective state. This is essentially what our country currently has in place and reflects the results of the elections from 1976-2020.\n\n\nCode\n# Create a new data frame that groups by calculates the winner of each state's presidential election in a state-wide winner-take-all format\n\nstate_wide_winner_take_all &lt;- president_1976_2020 |&gt;\n  group_by(year, state, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes)) |&gt;\n  filter(total_votes == max(total_votes)) |&gt;\n  ungroup()\n\n# Include the candidate's parties\n\ncandidate_party &lt;- president_1976_2020 |&gt;\n  select(year, state, candidate, party_simplified) |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  rename(party = party_simplified)\n\n# Merge first two tables so we have the candidate and the respective main party\n\nstate_wide_winner_take_all_results &lt;- left_join(state_wide_winner_take_all, candidate_party, by = c(\"year\", \"state\", \"candidate\"))\n\n# Merging data frame with the ECV counts for each year\n\necv_state_wide_winner &lt;- left_join(state_wide_winner_take_all_results, ecv_per_state_per_year, by = c('year', 'state'))\n\n# Data frame containing the winner of each election by majority ECVs\n\nelection_winner_state_wide &lt;- ecv_state_wide_winner |&gt;\n  group_by(year, candidate, party) |&gt;\n  drop_na(ecv_total) |&gt;\n  summarize(results = sum(ecv_total)) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  filter(results == max(results)) |&gt;\n  ungroup() |&gt;\n  rename(state_wide_winner = candidate,\n         state_wide_party = party,\n         state_wide_results = results)\n\n# Display Table\n\nDT::datatable(setNames(election_winner_state_wide, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")),\n              caption = \"Table 3: State-Wide Winner-Take-All Election Winners\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\n\nNext, we will take a look at the District-Wide Winner-Take-All + State-Wide “At Large” Votes ECV allocation method. This is a relatively complex allocation strategy which counts the ECVs for candidates based on the party winner of congressional district elections, then the remaining 2 ECVs for each state are allocated based on the state popular vote winner. Below is the data frame showing the election winners had this method been applied for each state in each election.\n\n\nCode\n# Data frame counting ECV for each state based on party winner from congressional district voting\n\nwinner_per_district_per_year &lt;- house_1976_2022 |&gt;\n  group_by(year, state, district) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  select(year, state, district, party) |&gt;\n  ungroup() |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(ecv_count = n())\n\n# Data frame with candidate and respective party\n\ncandidate_party &lt;- president_1976_2020 |&gt;\n  select(year, state, candidate, party_simplified) |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  rename(party = party_simplified)\n\n# Merge the two data frames, so we can properly tally up the votes for each candidate each election year\n\nmerge_district_ecv_count &lt;- left_join(winner_per_district_per_year, candidate_party, by = c(\"year\", \"state\", \"party\")) |&gt;\n  drop_na(candidate)\n\n# Need the statewide popular vote winner to allocate the remaining 2 ECVs can use data frame from the previous section ... ecv_state_wide_winner data frame\n\nstate_wide_popular &lt;- ecv_state_wide_winner |&gt;\n  select(year, state, candidate, party) |&gt;\n  rename(state_winner = candidate,\n         state_winner_party = party)\n\n# Merge data frame with our other data frame to include the extra 2 ECVs for the statewide popular vote winner, then find the majority winner by ECV\n\nelection_winner_district_wide &lt;- left_join(merge_district_ecv_count, state_wide_popular, by = c('year', 'state')) |&gt;\n  mutate(ecv_extra = case_when(candidate == state_winner ~ 2,\n                               candidate != state_winner ~ 0),\n         ecv_total = ecv_count + ecv_extra) |&gt;\n  group_by(year, candidate, party) |&gt;\n  summarize(candidate_ecv_total = sum(ecv_total)) |&gt;\n  drop_na(candidate) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  filter(candidate_ecv_total == max(candidate_ecv_total)) |&gt;\n  ungroup() |&gt;\n  rename(district_wide_winner = candidate,\n         district_wide_party = party,\n         district_wide_results = candidate_ecv_total)\n\nDT::datatable(setNames(election_winner_district_wide, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")),\n              caption = \"Table 4: District-Wide Winner-Take-All + State-Wide 'At Large' Votes Election Winners\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\n\nState-Wide Proportional\n\nAnother ECV allocation method is taking the proportion of candidate votes for each party in each state and allocating the state’s ECV appropriately. For instance, if Candidate A received 200,000 votes and Candidate B received 300,000 votes for State C, then Candidate A would be allocated 40% of the ECVs for the state while Candidate B receives 60% of the ECvs for State C. The same is repeated for all 50 states. Then the candidate with the most ECVs would win the election. The data table below reveals the winners had the election utilized a State-Wide Proportional ECV allocation strategy.\n\n\nCode\n# Data frame including the proportion of votes each candidate earned from each state\n\nstate_wide_proportional &lt;- president_1976_2020 |&gt;\n  mutate(prop = candidatevotes / totalvotes)\n\n# Merging above data frame with the ECV counts for each year\n\necv_state_proportion_winner &lt;- left_join(state_wide_proportional, ecv_per_state_per_year, by = c('year', 'state'))\n\n# Data frame with winner after calculating the proportionate ECV per state\n\nelection_winner_state_prop &lt;- ecv_state_proportion_winner |&gt;\n  select(year, state, candidate, party_simplified, prop, ecv_total) |&gt;\n  mutate(ecv_votes = round(prop * ecv_total)) |&gt;\n  filter(ecv_votes != 0) |&gt;\n  group_by(year, candidate, party_simplified) |&gt;\n  summarize(combined_ecv = sum(ecv_votes)) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  filter(combined_ecv == max(combined_ecv)) |&gt;\n  ungroup() |&gt;\n  rename(state_prop_winner = candidate,\n         state_prop_party = party_simplified,\n         state_prop_results = combined_ecv)\n\n# Display Table\n\nDT::datatable(setNames(election_winner_state_prop, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")), \n              caption = \"Table 5: State-Wide Proportional Election Winner\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\n\nNational Proportional\n\nLastly, we have the National Proportional ECV allocation method, which acts similar to the State-Wide Proportional method, but on a nation-wide scale. We take the proportion of nation-wide popular votes for each candidate then allocate ECVs based on these holistic values. The data frame below shows the results of each election if a National Proportional ECV allocation was used.\n\n\nCode\n# Data frame including the proportion of votes each candidate earned from the entire nation\n\nnation_wide_proportional &lt;- president_1976_2020 |&gt;\n  group_by(year, candidate, party_simplified) |&gt;\n  summarize(candidate_total = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  mutate(total_year = sum(candidate_total),\n         prop = candidate_total / total_year) |&gt;\n  ungroup()\n\n# Data frame with the total ECV per year\n\ntotal_ecv_per_year &lt;- ecv_per_state_per_year |&gt;\n  group_by(year) |&gt;\n  summarize(total_ecv = sum(ecv_total))\n\n# Merge data frames\n\nmerge_nation_wide_ecv &lt;- left_join(nation_wide_proportional, total_ecv_per_year, by = c('year'))\n\n\n# Data frame with winner after calculating proportional ECV nation wide\n\nelection_winner_nation_prop &lt;- merge_nation_wide_ecv |&gt;\n  mutate(combined_ecv = round(prop * total_ecv)) |&gt;\n  filter(combined_ecv != 0) |&gt;\n  group_by(year) |&gt;\n  filter(combined_ecv == max(combined_ecv)) |&gt;\n  ungroup() |&gt;\n  select(year, candidate, party_simplified, combined_ecv) |&gt;\n  rename(nation_prop_winner = candidate,\n         nation_prop_party = party_simplified,\n         nation_prop_results = combined_ecv)\n\nDT::datatable(setNames(election_winner_nation_prop, c(\"Year\", \"Winner\", \"Party\", \"Electoral College Votes\")), \n              caption = \"Table 6: National Proportional Election Winner\",\n              rownames = FALSE,\n              options = list(pageLength = 12))\n\n\n\n\n\n\nBelow is a data frame that combines the results of all the possible election outcomes for easier comparison.\n\n\nCode\n# Merge all 4 results tables together into one data table with all the aggregated information\n\ncombined_results &lt;- left_join(election_winner_state_wide, election_winner_district_wide, by = \"year\") |&gt;\n  left_join(election_winner_state_prop, by = \"year\") |&gt;\n  left_join(election_winner_nation_prop, by = \"year\")\n\n\nHere are the potential US presidents for elections between 1976 and 2020 had each election’s ECV allocation been different.\n\n\nCode\n# Filter only for the candidates\n\ncombined_results |&gt;\n  select(year, state_wide_winner, district_wide_winner, state_prop_winner, nation_prop_winner) |&gt;\n  rename(Year = year,\n         `State Wide` = state_wide_winner,\n         `District Wide` = district_wide_winner,\n         `State Proportional` = state_prop_winner,\n         `National Proportional` = nation_prop_winner) |&gt;\n  DT::datatable(caption = \"Table 7: Election Winners for Different ECV Allocation Schemes\",\n                rownames = FALSE,\n                options = list(pageLength = 12))\n\n\n\n\n\n\nFrom first glance, the election results didn’t seem too different from the actual outcomes (using the State Wide column). We can observe that for the district allocation method, election outcomes were different for the years 1988, with Michael Dukakis, and 2012, with Mitt Romney, as the winners. In the state-wide proportional voting, the only difference we observe is that Hillary Clinton would have won the 2016 election. Lastly, with the national proportion method, we have that Al Gore and Hillary Clinton would have won the 2000 and 2016 elections, respectively.\nNext, we would like to take a look at whether certain ECV allocation schemes seem to favor certain parties. Below are pie charts showing the distribution of party outcomes for each allocation method.\n\n\nCode\n# Filter only for the parties\n\nstate_wide_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(state_wide_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = state_wide_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"State-Wide Winner-Takes-All\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n  \ndistrict_wide_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(district_wide_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = district_wide_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"District-Wide Winner-Take-All + \\nState-Wide 'At Large' Votes\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n\nstate_prop_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(state_prop_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = state_prop_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"State-Wide Proportional\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n\nnation_prop_plot &lt;- combined_results |&gt;\n  select(year, state_wide_party, district_wide_party, nation_prop_party, state_prop_party) |&gt;\n  group_by(nation_prop_party) |&gt;\n  summarize(total = n()) |&gt;\n  mutate(total_elections = sum(total),\n         percentage = round((total/total_elections)*100, 2),\n         percent_labels = paste(percentage, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = total, fill = nation_prop_party)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  geom_text(aes(label = percent_labels),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"National Proportional\") +\n  scale_fill_manual(values = c(\"royalblue1\", \"tomato\"),\n                    name = \"Party\", \n                    guide = guide_legend(reverse = TRUE))\n\ngrid.arrange(state_wide_plot, district_wide_plot, state_prop_plot, nation_prop_plot,\n             top = \"Party Distribution\")\n\n\n\n\n\n\n\n\n\nIn our above pie charts, we find that both of the proportional schemes (state-wide proportional and national proportional) seem to favor the Democratic party over the Republican Party, while the remaining two methods (state-wide winner-takes-all and district-wide winner-take-all + state-wide “at large” votes) seem balanced with a perfect 50/50 split between the two parties. Despite the national proportional method and state-wide proportional method seemingly favoring the Democratic Party, these methods provide a better representation of the voting population, valuing each vote more closely than the other methods, which could perhaps suggest that this is reflective of the nation’s voter preferences."
  },
  {
    "objectID": "mp03.html#final-thoughts",
    "href": "mp03.html#final-thoughts",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nVarious elections would have been different had the electoral college vote allocations. Two examples are the 2000 election and the 2016 election. In the 2000 election, Al Gore won the popular vote and would have won with the national proportional voting scheme but lost in every other method. For the 2016 election, Hillary Clinton also won the national popular vote and would have won with a state-wide proportional vote as well. This leads to the question of fairness among these different allocation schemes.\nEach of the four electoral college vote allocation schemes has its strengths and weaknesses, I will be evaluating the “fairness” of each of these methods below.\nAmong all four options, I believe that the “fairest” ECV allocation method would be the national proportional scheme. The national proportional method takes everyone’s vote into consideration when allocating the electoral college votes. In the past, there have been elections where candidates have won the popular vote (gained the most votes among all US voters), but because of the state-wide winner-take-all allocation scheme, these candidates are left with fewer ECVs and thus, lose the election. The National Proportional allocation scheme would highly reflect voter preferences by allowing the majority popular vote winner to take office. However, to implement this scheme, every state would have to agree to implement this strategy and it may require a change in the Constitution.\nNext, I believe that the state-wide proportional ECV allocation strategy would be the next “fairest” allocation scheme. Similar to the national proportional method, the state-wide proportional method values each person’s vote equally per state. In this scheme, presidential candidates would get votes according to their state-level popularity. Additionally, the impact of swing states would be reduced, rather than a few hundred thousands of votes determining an election, more of an emphasis would be placed on individuals’ votes in each state. As compared to the national proportional vote, the state-wide proportional vote would reflect voter preferences from the respective state, however, since each state may have a different number of ECVs, some people’s votes may weigh less than others in the grand scheme. Implementing the state-wide proportional ECV still would require a state to make legislative changes, but would not be as tedious as what is required in the national proportional method.\nOf the remaining two allocation methods, I believe the district-wide winner-take-all + state-wide “at large” votes would be the next fairest scheme. This method takes the state-wide proportional method a step further by allocating each ECV by the popular vote winner of the respective district, which can better represent a state’s diversity of voting preferences compared to the state-wide winner-take-all method. This method similar to the above two will reduce the chances of candidates winning the popular vote but losing the electoral college, as we saw in some of the past elections. However, with this method of ECV allocation, a possible issue could occur with the bounds of districts (gerrymandering), if certain districts are changed, certain parties may be favored over others, which could skew the outcome of the election.\nLastly, in my opinion, the state-wide winner-take-all approach takes last in the “fairest” rankings. This allocation method only values the votes of the majority of the population in a state, and does not reflect the larger popular vote representation of an entire nation let alone one state. This is especially apparent in swing states, where elections come down to hundreds or even tens of thousands of votes, in which case the candidate that edges out obtains all ECVs. Ultimately, while this ECV allocation method is the simplest to implement, it often does not reflect voter preferences, making it the least “fairest” strategy."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "MP #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMIT Election Data + Science Lab. (n.d.). MIT Election Lab. MIT Election Data + Science Lab. https://electionlab.mit.edu/↩︎"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "As life expectancy continues to increase, there is a greater need for long-term financial security in retirement. Individuals are compelled to start their retirement planning earlier on in their career. A retirement plan is a financial arrangement meant to help individuals save and/or invest money during their career so that this money could support them in retirement. There are various retirement plans (e.g. employer-sponsored plans, individual retirement accounts, etc.) that individuals can choose to contribute to based on various factors.\nAt the City University of New York (CUNY), new faculty members have 30 days to select between two retirement plans. Given the long-term nature and significance of this decision, it is one that cannot be made lightly, as it will impact their financial security for the remainder of their lives during retirement.\n\n\nCUNY offers its employees a choice between two retirement plans: the traditional defined-benefit Teachers Retirement System (TRS) plan and the more recent defined-contribution Optional Retirement Plan (ORP).\nThroughout this project, we will be ignoring the effect of taxes since both of these plans offer pre-tax retirement savings.\n\n\nThe TRS traditional pension plan guarantees that in retirement, the employer (CUNY) will continue paying its employees a portion of their salary until death. This is a “defined-benefit” plan as the retirement benefit the employee ultimately receives is fixed a priori and the employer ends up taking the market risk.\nAt CUNY, the TRS is administered as follows:\n\nEmployees pay a fixed percentage of their paycheck into the pension fund. For CUNY employees joining after March 31, 2012–which you may assume for this project–the so-called “Tier VI” contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe retirement benefit is calculated based on the Final Average Salary of the employee: following 2024 law changes, the FAS is computed based on the final three years salary. (Previously, FAS was computed based on 5 years: since salaries tend to increase monotonically over time, this is a major win for TRS participants.)\nIf N is the number of years served, the annual retirement benefit is:\n\n1.67\\% * \\text{FAS} * N if N \\leq 20\n1.75\\% * \\text{FAS} * N if N = 20\n(35\\% + 2\\% * N) * \\text{FAS} if N \\geq 20\n\nIn each case, the benefit is paid out equally over 12 months.\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\n\n\n\nIn CUNY’s ORP option, the plan is more similar to the more commonly known 401(k) plan. In this plan, both the employee and employer contribute to the retirement account, which will then be invested into the employee’s choice of mutual funds (e.g. US equities, International equities, bonds, short-term debts, etc.). These investments continue to grow “tax-free” over the course of their career and in retirement until they are withdrawn. The employee takes a larger risk in this plan as once they retire and begin withdrawing funds, once the account dries up, there are no more additional funds. However, if their investment grows large enough they may have additional funds to pass down to immediate family upon death. The ORP is a defined-contribution plan since only the contributions during the employee’s career is fixed, the final balance depends on the market.\nDuring retirement, employees have the choice of accessing and withdrawing funds at any rate, but with the nature of this retirement plan it is important to make rational withdrawal decisions. For this project, we will assume that the employee will be withdrawing 4% of the account’s value per year (Schwab discussion).\nThe funds available in a ORP account depend strongly on the investments chosen. For this analysis, you can assume that the ORP participants invest in a Fidelity Freedom Fund with the following asset allocation:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These contributions are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nWe will assume that the contributions from the employee and employer are immediately invested based on the asset allocations above.\nThe goal for this project is to use historical financial data and a bootstrap inference strategy to compare CUNY’s two retirement plans and provide data-driven recommendations for CUNY employees’ retirement planning.\nWe will be using data from two economic and financial data sources, AlphaVantage and the Federal Reserve Economic Data repository. AlphaVantage is a commercial stock market data provider that provides users with APIs for real-time and historical market data. While the Federal Reserve Economic Data (FRED) repository is an online database maintained by the Federal Reserve Bank of St. Louis, allowing users access to various economic data and financial indicators."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "As life expectancy continues to increase, there is a greater need for long-term financial security in retirement. Individuals are compelled to start their retirement planning earlier on in their career. A retirement plan is a financial arrangement meant to help individuals save and/or invest money during their career so that this money could support them in retirement. There are various retirement plans (e.g. employer-sponsored plans, individual retirement accounts, etc.) that individuals can choose to contribute to based on various factors.\nAt the City University of New York (CUNY), new faculty members have 30 days to select between two retirement plans. Given the long-term nature and significance of this decision, it is one that cannot be made lightly, as it will impact their financial security for the remainder of their lives during retirement.\n\n\nCUNY offers its employees a choice between two retirement plans: the traditional defined-benefit Teachers Retirement System (TRS) plan and the more recent defined-contribution Optional Retirement Plan (ORP).\nThroughout this project, we will be ignoring the effect of taxes since both of these plans offer pre-tax retirement savings.\n\n\nThe TRS traditional pension plan guarantees that in retirement, the employer (CUNY) will continue paying its employees a portion of their salary until death. This is a “defined-benefit” plan as the retirement benefit the employee ultimately receives is fixed a priori and the employer ends up taking the market risk.\nAt CUNY, the TRS is administered as follows:\n\nEmployees pay a fixed percentage of their paycheck into the pension fund. For CUNY employees joining after March 31, 2012–which you may assume for this project–the so-called “Tier VI” contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe retirement benefit is calculated based on the Final Average Salary of the employee: following 2024 law changes, the FAS is computed based on the final three years salary. (Previously, FAS was computed based on 5 years: since salaries tend to increase monotonically over time, this is a major win for TRS participants.)\nIf N is the number of years served, the annual retirement benefit is:\n\n1.67\\% * \\text{FAS} * N if N \\leq 20\n1.75\\% * \\text{FAS} * N if N = 20\n(35\\% + 2\\% * N) * \\text{FAS} if N \\geq 20\n\nIn each case, the benefit is paid out equally over 12 months.\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\n\n\n\nIn CUNY’s ORP option, the plan is more similar to the more commonly known 401(k) plan. In this plan, both the employee and employer contribute to the retirement account, which will then be invested into the employee’s choice of mutual funds (e.g. US equities, International equities, bonds, short-term debts, etc.). These investments continue to grow “tax-free” over the course of their career and in retirement until they are withdrawn. The employee takes a larger risk in this plan as once they retire and begin withdrawing funds, once the account dries up, there are no more additional funds. However, if their investment grows large enough they may have additional funds to pass down to immediate family upon death. The ORP is a defined-contribution plan since only the contributions during the employee’s career is fixed, the final balance depends on the market.\nDuring retirement, employees have the choice of accessing and withdrawing funds at any rate, but with the nature of this retirement plan it is important to make rational withdrawal decisions. For this project, we will assume that the employee will be withdrawing 4% of the account’s value per year (Schwab discussion).\nThe funds available in a ORP account depend strongly on the investments chosen. For this analysis, you can assume that the ORP participants invest in a Fidelity Freedom Fund with the following asset allocation:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These contributions are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nWe will assume that the contributions from the employee and employer are immediately invested based on the asset allocations above.\nThe goal for this project is to use historical financial data and a bootstrap inference strategy to compare CUNY’s two retirement plans and provide data-driven recommendations for CUNY employees’ retirement planning.\nWe will be using data from two economic and financial data sources, AlphaVantage and the Federal Reserve Economic Data repository. AlphaVantage is a commercial stock market data provider that provides users with APIs for real-time and historical market data. While the Federal Reserve Economic Data (FRED) repository is an online database maintained by the Federal Reserve Bank of St. Louis, allowing users access to various economic data and financial indicators."
  },
  {
    "objectID": "mp04.html#data-sources",
    "href": "mp04.html#data-sources",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Sources",
    "text": "Data Sources\nFor this project, we will be accessing data from both AlphaVantage and FRED through their password-protected APIs. Before accessing these APIs, we will need to register for API keys for each of the respective APIs: AlphaVantage registration and FRED registration.\nBelow are some useful packages we will need to utilize throughout the project in our retirement planning analysis.\n\n\nCode\n# Install necessary packages\n\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"sf\")) install.packages(\"sf\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"DT\")) install.packages(\"DT\")\nif(!require(\"gt\")) install.packages(\"gt\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif(!require(\"stringr\")) install.packages(\"stringr\")\nif(!require(\"patchwork\")) install.packages(\"patchwork\")\nif(!require(\"gganimate\")) install.packages(\"gganimate\")\nif(!require(\"zoom\")) install.packages(\"zoom\")\nif(!require(\"gridExtra\")) install.packages(\"gridExtra\")\nif(!require(\"httr2\")) install.packages(\"httr2\")\nif(!require(\"readxl\")) install.packages(\"readxl\")\nif(!require(\"ggcorrplot\")) install.packages(\"ggcorrplot\")\n\n# Load packages into R\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(haven)\nlibrary(DT)\nlibrary(gt)\nlibrary(ggplot2)\nlibrary(RColorBrewer) # different color palette options\nlibrary(stringr)\nlibrary(patchwork) # inset plots\nlibrary(gganimate)\nlibrary(zoom) # zoom for plots\nlibrary(gridExtra) # labels outside the plot\nlibrary(httr2)\nlibrary(readxl) # reading excel files\nlibrary(ggcorrplot) # correlation matrices\n\n\n\nLoading API Keys\nOnce we’ve registered and received our unique API keys, we will need to load these keys, so that we can access them later as we send requests to get data from the APIs.\nBelow is the code to load in my personal API keys (replace with your individual key) for each of the APIs.\nPersonal keys are not meant to be displayed.\n\nAlphaVantage API Key\nBelow is my personal AlphaVantage API key.\n\n\nCode\nalphavantage_api_key &lt;- readLines(\"API-Key-alphavantage.txt\")\n\n\n\n\nFRED API Key\nBelow is my personal FRED API key.\n\n\nCode\nfred_api_key &lt;- readLines(\"API-Key-fred.txt\")\n\n\nOnce we’ve registered and received our API keys for each data source, we can now set up and send requests to the APIs to retrieve the necessary data sources for our analysis."
  },
  {
    "objectID": "mp04.html#data-acquisition",
    "href": "mp04.html#data-acquisition",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Acquisition",
    "text": "Data Acquisition\nBefore beginning this project, we will need to retrieve some historical economic and financial data from the AlphaVantage and FRED APIs. More specifically, we will be retrieving data covering:\n\nWage Growth\nInflation\nUS Equities\nInternational Equities\nBond\nShort-Term\n\nFor each of these data set retrievals, we will be utilizing the functions from the httr2 package. First, we will need to set up an API request with the request() function, we will define the URL and set up the respective parameters for the metrics we want to retrieve based on each API’s documentation (AlphaVantage documentation and FRED documentation). Then, after performing the request, we will need to read in the content from the API response. The data from AlphaVantage’s API can be formatted into a csv file, in which case we will use the read_csv() function. Whereas FRED’s API provides a slightly more challenging .zip Excel file format, in which case we will need to unzip the folder and read in the file with read_excel() from the readxl package.\nBelow are the 6 data sets retrieved from either the AlphaVantage or FRED API’s. There are various data sets to choose from for each of these metrics, the choice made is dependent on various factors such as the extent of historical data.\n\nWage Growth\nFor the wage growth data, we will retrieve data from the FRED API. Using the series_id = FRBATLWGT3MMAUMHWGWSFT, we will get the Wage Growth Tracker for Job Switchers, 3-Month Moving Average, Unweighted, Median Hourly Wage. This data set provides information on wage growth from 1997 to 2024.\n\n\nCode\n# Starts from 2006-2024\n\ndir &lt;- getwd()\n\nif(!file.exists('wage.zip')){\n  fred_req_wage &lt;- request(\"https://api.stlouisfed.org/fred/series/observations\") |&gt;\n  req_url_query(series_id = \"FRBATLWGT3MMAUMHWGWSFT\",\n                api_key = fred_api_key,\n                file_type = \"xls\")\n  \n  fred_resp_wage &lt;- fred_req_wage |&gt;\n    req_perform()\n  \n  save_path_wage &lt;- paste0(dir, \"/wage.zip\")\n  \n  fred_resp_wage |&gt;\n    resp_body_raw() |&gt;\n    writeBin(save_path_wage)\n}\n\nsave_path_wage &lt;- paste0(dir, \"/wage.zip\")\n\ntd &lt;- tempdir()\n\nzip_contents_wage &lt;- unzip(save_path_wage,\n      exdir = td)\n\nfred_wage &lt;- read_excel(zip_contents_wage,\n                       sheet = 2)\n\n\nBelow is the code to select the relevant columns we will use in our analysis and rename columns for ease of interpretation.\n\n\nCode\nwage_growth_data &lt;- fred_wage |&gt;\n  select(c('observation_date', 'FRBATLWGT3MMAUMHWGWSFT')) |&gt;\n  rename(wage_growth = 'FRBATLWGT3MMAUMHWGWSFT') # annualized median wage growth rate\n\n\n\n\nInflation\nFor our inflation data set, we will be retrieving the Consumer Price Index (CPI) of the United States data from the AlphaVantage API. The CPI measures the average change over time in the prices for a basket of goods and services purchased by consumers. Later, we will calculate the change in CPI to determine the inflation rates per period.\n\n\nCode\ncpi_alphavantage_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = 'CPI',\n                apikey = alphavantage_api_key,\n                datatype = 'csv')\n\n\ncpi_resp &lt;- req_perform(cpi_alphavantage_req)\n\ncpi_csv &lt;- cpi_resp |&gt;\n  resp_body_string() |&gt;\n  read_csv()\n\n\n\n\nCode\n# Renaming the column for ease of JOINs later\n\ncpi &lt;- cpi_csv |&gt;\n  rename(cpi = 'value')\n\n\n\n\nUS Equity Market Returns\nWe will retrieve data on the US Equity market through the AlphaVantage API. Using AlphaVantage’s TIME_SERIES_MONTHLY_ADJUSTED function, we can retrieve data on the Vanguard Total Stock Market ETF (symbol = VTI) to represent the US Equity historical market returns for our project. This data set provides monthly data from 2001 to 2004 on the US Equity market.\n\n\nCode\nvti_alphavantage_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = 'TIME_SERIES_MONTHLY_ADJUSTED',\n                symbol = 'VTI',\n                apikey = alphavantage_api_key,\n                datatype = 'csv')\n\n\nvti_resp &lt;- req_perform(vti_alphavantage_req)\n\nVTI_csv &lt;- vti_resp |&gt;\n  resp_body_string() |&gt;\n  read_csv()\n\n\n\n\nCode\nus_equity &lt;- VTI_csv |&gt;\n  select(c('timestamp', 'adjusted close')) |&gt;\n  rename(us_equity_adj_close = 'adjusted close')\n\n\n\n\nInternational Equity Market Returns\nWe will also retrieve data on the International Equity market through AlphaVantage. Similarly, using AlphaVantage’s TIME_SERIES_MONTHLY_ADJUSTED function, we can retrieve data on the Vanguard International Growth Fund Investor Shares (symbol = VWIGX) to represent the International Equity historical market returns for our project. This data set provides monthly data from 1999 to 2004 on the US Equity market.\n\n\nCode\nvwigx_alphavantage_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = 'TIME_SERIES_MONTHLY_ADJUSTED',\n                symbol = 'VWIGX',\n                apikey = alphavantage_api_key,\n                datatype = 'csv')\n\n\nvwigx_resp &lt;- req_perform(vwigx_alphavantage_req)\n\nVWIGX_csv &lt;- vwigx_resp |&gt;\n  resp_body_string() |&gt;\n  read_csv()\n\n\n\n\nCode\nint_equity &lt;- VWIGX_csv |&gt;\n  select(c('timestamp', 'adjusted close')) |&gt;\n  rename(int_equity_adj_close = 'adjusted close')\n\n\n\n\nBond market returns\nFor the historical bond market values, we will use AlphaVantage’s TREASURY_YIELD function and set maturity at 10year for our API request. The resulting data set provides monthly data on the interest rates that investors earn on bonds from 1953 to 2024. We will use this data set to represent historical bond values.\n\n\nCode\nbond_alphavantage_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = 'TREASURY_YIELD',\n                maturity = \"10year\",\n                apikey = alphavantage_api_key,\n                datatype = 'csv')\n\n\nbond_resp &lt;- req_perform(bond_alphavantage_req)\n\nbond_csv &lt;- bond_resp |&gt;\n  resp_body_string() |&gt;\n  read_csv()\n\n\n\n\nCode\nbond &lt;- bond_csv |&gt;\n  rename(bond_value = 'value')\n\n\n\n\nShort-Term Debt Returns\nLastly, for short-term debt values, we will also use AlphaVantage’s TREASURY_YIELD function, however, now we will set maturity at 2year for our API request. The resulting data set provides monthly data on the the interest rates that investors earn on short-term debt from 1976 to 2024. We will use this data set to represent historical short-term debt values.\n\n\nCode\nshort_alphavantage_req &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n  req_url_query(`function` = 'TREASURY_YIELD',\n                maturity = \"2year\",\n                apikey = alphavantage_api_key,\n                datatype = 'csv')\n\n\nshort_resp &lt;- req_perform(short_alphavantage_req)\n\nshort_csv &lt;- short_resp |&gt;\n  resp_body_string() |&gt;\n  read_csv()\n\n\n\n\nCode\nshort_term &lt;- short_csv |&gt;\n  rename(short_term_value = 'value')"
  },
  {
    "objectID": "mp04.html#investigation-and-visualization-of-input-data",
    "href": "mp04.html#investigation-and-visualization-of-input-data",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Investigation and Visualization of Input Data",
    "text": "Investigation and Visualization of Input Data\nBefore beginning our retirement plan comparison analysis, we will do some preliminary exploration analysis on the 6 data sets imported to get familiar with our data. For the purposes of this project and to maintain consistency, we will only use data from the last 20 years (2005-2024).\nLet’s first take a look at the wage growth and inflation data sets. Taking a look at the wage growth and inflation data simultaneously gives us a better understanding of the purchasing power and living standards of each specific time period. Both these metrics will be important for individuals’ financial planning throughout their career.\nBelow is the data frame combining the two metrics and calculate the inflation based on the change in CPI per month.\n\n\nCode\ncomparison_inflation_vs_wage_growth &lt;- inner_join(wage_growth_data, cpi, join_by('observation_date' == 'timestamp')) |&gt;\n  arrange(observation_date) |&gt;\n  mutate(inflation = round(((cpi - lag(cpi)) / lag(cpi)) * 100, 2)) |&gt;\n  drop_na() |&gt;\n  filter(year(observation_date) &gt; 2004) # We will only be working with data from the past 20 years\n\n\nTaking a look at the scatter plot between inflation and wage growth, there doesn’t seem to be any correlation between the two metrics, as most of the data is randomly scattered with no obvious pattern or trend.\n\n\nCode\ncomparison_inflation_vs_wage_growth |&gt; \n  ggplot(aes(x = inflation, y = wage_growth)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(title = \"Inflation versus Wage Growth\",\n       x = \"Inflation\",\n       y = \"Wage Growth\")\n\n\n\n\n\n\n\n\n\nNext, let’s take a closer look at the time-series plots for each of these metrics.\nIn the wage growth scatter plot, we can observe that between about 2005 and 2008, the wage growth fluctuates slightly between about 3.5% and 4.5%. However, there is a significant dip in wage growth afterwards until late 2010 to under 2%. The remainder of the wage growth percentages generally increase from 2010 onwards.\nIn the inflation scatter plot, inflation seems to remain consistently between -1% and 1% for most periods, with less of a pattern compared to the wage growth plot. However, there are a few outliers, notably in 2008-10-01, 2008-11-01, 2008-12-01, where the inflation rate drops below -1%. Given that there was a recession in 2008 during the Global Financial Crisis, these values make sense.\n\n\nCode\nwage_growth_scatter &lt;- comparison_inflation_vs_wage_growth |&gt;\n  ggplot(aes(x = observation_date, y = wage_growth)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(\"Wage Growth Rate Over Time (Monthly)\",\n       x = \"Date (Monthly Periods)\",\n       y = \"Wage Growth Rate\")\n\ninflation_scatter &lt;- comparison_inflation_vs_wage_growth |&gt;\n  ggplot(aes(x = observation_date, y = inflation)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(\"Inflation Rate Over Time (Monthly)\",\n       x = \"Date (Monthly Periods)\",\n       y = \"Inflation Rate\")\n\ngrid.arrange(wage_growth_scatter, inflation_scatter, top = \"Time Series: Wage Growth and Inflation\")\n\n\n\n\n\n\n\n\n\nNext, we will take a look at the equities market return fluctuations and compare the US Equity market with the International Equity market from 2005 to 2024.\nBelow is the data frame that joins the two data tables US equity and international equity and calculates the returns for each equity market.\n\n\nCode\n# Join all the monthly market data together\n\n# Adjusting the date in the equities data to the first of the month for simplicity of JOINs (Ex: January 31 will become February 1, and so on)\n\nequities &lt;- inner_join(us_equity, int_equity, by = 'timestamp') |&gt;\n  arrange(timestamp) |&gt;\n  mutate(us_equity_returns = round(((us_equity_adj_close - lag(us_equity_adj_close)) / lag(us_equity_adj_close)) * 100, 2),\n         int_equity_returns = round(((int_equity_adj_close - lag(int_equity_adj_close)) / lag(int_equity_adj_close)) * 100, 2)) |&gt;\n  mutate(timestamp = ceiling_date(timestamp, \"month\")) |&gt;\n  select(-c('us_equity_adj_close', 'int_equity_adj_close')) |&gt;\n  filter(year(timestamp) &gt; 2004)\n\n\nThen, we plot the US Equity market returns against the International Equity market returns. From a preliminary observation of the scatter plot, there seems to be a strong positive correlation between these two metrics. This may indicate a pattern that as the US Equity market returns increase so does the International Equity’s market returns. These markets are likely highly correlated due to factors like global events and/or policies that may impact these markets simultaneously.\n\n\nCode\nequities |&gt;\n  ggplot(aes(x = us_equity_returns,\n             y = int_equity_returns)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(title = \"US Equity Market Returns vs International Equity Market Returns\",\n       x = \"US Equity Market Returns\",\n       y = \"International Equity Market Returns\")\n\n\n\n\n\n\n\n\n\nLet’s also take a look at the time-series for the returns of each equity market. Below, we can observe that each of these markets have a relatively constant fluctuation over time. Both smooth lines plotted for each scatter plot remain around the 0% return line, indicating that the market returns are centered around 0 and a lack of a discernible trend over time. For the US Equity market, most of the returns remain within -10% and 10%. However, there is a noticeable outlier in the US Equity market on 2008-11-01 with a return of -17.48%. Similarly, in the same period, the International Equity market experienced returns of -22.81%. Both of these lulls occurring in 2008 during the Global Financial Crisis.\n\n\nCode\nus_equity_scatter &lt;- equities |&gt;\n  ggplot(aes(x = timestamp, y = us_equity_returns)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(\"US Equity Returns Over Time (Monthly)\",\n       x = \"Date (Monthly Periods)\",\n       y = \"US Equity Returns\")\n\nint_equity_scatter &lt;- equities |&gt;\n  ggplot(aes(x = timestamp, y = int_equity_returns)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(\"International Equity Returns Over Time (Monthly)\",\n       x = \"Date (Monthly Periods)\",\n       y = \"International Equity Returns\")\n\ngrid.arrange(us_equity_scatter, int_equity_scatter, top = \"Time Series: US vs International Equity Returns\")\n\n\n\n\n\n\n\n\n\nLastly, we will be exploring the treasuries market, more specifically, we’ll compare the bond returns against the short-term debt returns from 2005 to 2024.\nBelow is the code joining the two tables together into one us treasuries table.\n\n\nCode\nus_treasury_comparison &lt;- inner_join(bond, short_term, by = \"timestamp\") |&gt;\n  filter(year(timestamp) &gt; 2004) |&gt;\n  arrange(`timestamp`)\n\n\nFirst, we’ll take a look at the bonds vs the short-term debt returns scatter plot. There seems to be, for the most part, a strong positive correlation between these two metrics. Both these returns are sensitive to interest rate changes and market conditions, so their returns may end up synchronized.\n\n\nCode\nus_treasury_comparison |&gt;\n  ggplot(aes(x = bond_value, y = short_term_value)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(title = \"US Treasury Returns Over Time (Monthly)\",\n       x = \"Bonds Return\",\n       y = \"Short-Term Return\")\n\n\n\n\n\n\n\n\n\nThen, taking a look at the time-series for the bond returns and the short-term debt returns, we have that they have a similar general trend of starting off high, then decreasing for a while before increasing again. In both plots, there is an obvious dip in returns between 2020 and 2022, which is likely due to the global pandemic that severely impacted global economies.\n\n\nCode\nbond_scatter &lt;- us_treasury_comparison |&gt;\n  ggplot(aes(x = timestamp, y = bond_value)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(\"Bond Returns Over Time (Monthly)\",\n       x = \"Date (Monthly Periods)\",\n       y = \"Bond Returns\")\n\nshort_term_scatter &lt;- us_treasury_comparison |&gt;\n  ggplot(aes(x = timestamp, y = short_term_value)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw() +\n  labs(\"Short-Term Returns Over Time (Monthly)\",\n       x = \"Date (Monthly Periods)\",\n       y = \"Short-Term Returns\")\n\ngrid.arrange(bond_scatter, short_term_scatter, top = \"Time Series: Bond Market vs Short-Term Debt Returns\")\n\n\n\n\n\n\n\n\n\nThe last thing, I would like to explore is the actual calculated correlation coefficient between each of these metrics. Before running the correlation, we will need to join all the tables into one, all_metrics table for ease of accessibility. Next, using the ggcorrplot package, we can calculate the correlation between each of the factors and plot this onto a correlation heat map matrix.\n\n\nCode\n# All metrics in one data frame to calculate correlations between each metric\n\nall_metrics &lt;- comparison_inflation_vs_wage_growth |&gt;\n  inner_join(equities, join_by(\"observation_date\" == \"timestamp\")) |&gt;\n  inner_join(us_treasury_comparison, join_by(\"observation_date\" == \"timestamp\")) |&gt;\n  select(-c(\"cpi\")) |&gt;\n  drop_na()\n\n# Correlation Matrix\n\ncorr_matrix &lt;- all_metrics |&gt;\n  select(-c(\"observation_date\")) |&gt;\n  rename(`Wage Growth` = wage_growth,\n         Inflation = inflation,\n         `US Equity Returns` = us_equity_returns,\n         `International Equity Returns` = int_equity_returns,\n         `Bond Returns` = bond_value,\n         `Short-Term Debt Returns` = short_term_value) |&gt;\n  cor() |&gt;\n  round(digits = 2)\n\n\ncorr_matrix |&gt;\n  ggcorrplot(lab = TRUE)\n\n\n\n\n\n\n\n\n\nSimilar to the analysis we did prior to this, there is a strong positive correlation between US Equity returns and International Equity returns with a correlation coefficient of 0.86 and also a strong positive correlation between Bond returns and Short-Term Debt returns with a correlation coefficient of 0.83. An additional interesting observation is that there is a relatively strong positive relationship between wage growth and short-term debt returns as well, with a correlation coefficient of 0.69. The remaining values indicate a weak or even non-discernible correlation.\nTo take a closer look at each metric, we will summarize each metric’s monthly data into annual summaries. Since our wage growth data, bond value data, and short-term value data are all annual percentages, we will take the average between the 12-month period for the aggregate value. On the other hand, our inflation, US equity return, and International equity return data are monthly values, so we will follow the formula:\n\nAggregate = (\\prod (1 + value / 100)) - 1\n Where value represents either inflation, US equity return or International equity return.\nBelow is the data table that shows all of our aggregated values for each of the metrics for each year of data we will be utilizing in our calculations. Taking a look at some of the data we saw that stood out in some of the scatter plots earlier, we can see that in 2008, there was an average market return of -38.51% for the US Equity market and a -50.01% for the International equity market. Inflation also took a hit at 0.1% compared to the 4.08% from 2007.\n\n\nCode\nall_metrics_summary &lt;- all_metrics |&gt;\n  mutate(month = month(observation_date),\n         year = year(observation_date)) |&gt;\n  group_by(year) |&gt;\n  summarize(annual_wage_growth = round(mean(wage_growth), 2),\n            annual_inflation = round((prod(1 + (inflation / 100)) - 1) * 100, 2),\n            annual_us_equity_returns = round((prod(1 + (us_equity_returns / 100)) - 1) * 100, 2),\n            annual_int_equity_returns = round((prod(1 + (int_equity_returns / 100)) - 1) * 100, 2),\n            annual_bond_value = round(mean(bond_value), 2),\n            annual_short_term_value = round(mean(short_term_value), 2)) |&gt;\n  arrange(year)\n\n  \nDT::datatable(setNames(all_metrics_summary, c(\"Year\", \"Average Wage Growth\", \"Average Inflation\", \"Average US Equity Returns\", \"Average International Equity\", \"Average Bond Retuns\", \"Average Short-Term Debt Returns\")), \n              caption = \"Table 1: Averages of All Metrics per Year\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\nAdditionally, before we compute estimated retirement projections, we will need to calculate long-run averages for each metric (wage growth, inflation, US equity returns, International equity returns, bond returns, short-term debt returns). Since we do not have information on the future, we will be using the long-run average of each metric to project the CUNY employee’s retirement based on certain assumptions.\nBelow is the data frame containing all the long-run averages for each metric. At first glance, the long-run average market returns for each of the equity markets seem a little aggressive, but based on the averages from the table above, the various global events and market dynamics, it checks out. Comparatively, both the long-run averages for the bond returns and short-term debt returns are lower at 2.91% and 1.91%, which makes sense given that these assets are less risky and typically provide less aggressive returns.\n\n\nCode\nlong_run_averages &lt;- all_metrics_summary |&gt;\n  summarize(long_run_wage_growth = round(mean(annual_wage_growth), 2), # already annualized\n            long_run_inflation = round(mean(annual_inflation), 2),\n            long_run_us_equity_returns = round(mean(annual_us_equity_returns), 2),\n            long_run_int_equity_returns = round(mean(annual_int_equity_returns), 2),\n            long_run_bond_value = round(mean(annual_bond_value), 2), # already annualized\n            long_run_short_term_value = round(mean(annual_short_term_value), 2)) # already annualized\n\nDT::datatable(setNames(long_run_averages, c(\"Average Wage Growth\", \"Average Inflation\", \"Average US Equity Returns\", \"Average International Equity\", \"Average Bond Retuns\", \"Average Short-Term Debt Returns\")), \n              caption = \"Table 2: Long-Run Averages of All Metrics\",\n              rownames = FALSE)"
  },
  {
    "objectID": "mp04.html#historical-comparison-of-trs-and-orp",
    "href": "mp04.html#historical-comparison-of-trs-and-orp",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Historical Comparison of TRS and ORP",
    "text": "Historical Comparison of TRS and ORP\nNow that we’ve retrieved our data sets and performed some preliminary exploratory analysis, we can begin our CUNY retirement plan analysis. For this analysis, we will implement the TRS and ORP formulas introduced earlier, utilizing the data sets we obtained from AlphaVantage and FRED to represent market fluctuations over time. Before beginning analysis we will need to make a few assumptions for our CUNY employee. We will assume that our hypothetical CUNY employee:\n\nJoined CUNY in January of 2005 at age 35 with a starting salary of $50,000\nRetired from CUNY at the end of October 2024 at age 54\nWorked for 20 years in CUNY\nDies at age 77\n\nWe will first calculate the CUNY employee’s salary each year of their career (2005-2024) based on our annual aggregated wage growth and inflation data. To adjust for wage growth, we will follow the following formula:\n\nsalary_w = salary_p * (1 + (WG / 100))\n\nWhere:\n\nsalaryp: salary from the previous year\nWG: wage growth percentage\nsalaryw: salary after wage growth adjustments\n\nThen, to adjust for yearly inflation, we will use:\n\nsalary_f = \\frac{salary_w}{1-(i/100)}\n\nWhere:\n\nsalaryw: salary after wage growth adjustments\ni: inflation rate\nsalaryf: final salary after wage growth and inflation adjustments\n\nBelow is the data frame containing all the final calculations for salary based on the wage growth rate and inflation rate data. Based on our calculations, in about 15 years, the employee will have doubled their starting salary to $102,242.56. By the time the employee retires in 20 years, they would have tripled their initial starting salary at $161,085.94.\n\n\nCode\nyearly_wage_growth &lt;- all_metrics_summary$annual_wage_growth\n\nyearly_inflation &lt;- all_metrics_summary$annual_inflation\n\nsalary_vector &lt;- c(50000)\n\nfor (i in 2:length(yearly_wage_growth)) {\n  start_salary = salary_vector[i-1]\n  salary = (start_salary * (1 + (yearly_wage_growth[i] / 100))) / (1 - (yearly_inflation[i] / 100))\n  salary_vector = c(salary_vector, salary)\n}\n\nyearly_salary_df &lt;- data.frame(year = (2005:2024),\n                               salary = salary_vector) |&gt;\n  mutate(salary = round(salary, 2))\n\nDT::datatable(setNames(yearly_salary_df, c(\"Year\", \"Salary After Adjustments\")), \n              caption = \"Table 3: Employee Salary After Adjustments to Wage Growth and Inflation\",\n              rownames = FALSE)\n\n\n\n\n\n\nNow, based on these salary calculations we will run our simulation for each retirement plan and compare each of values at the first month of retirement.\n\nTRS\nFor the TRS retirement plan, the employee will pay a fixed percentage of their paycheck into the pension fund, based on the following contribution rates and annual salary brackets:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nBelow is the code calculating the total contributions the employee provides to the pension each year of their career at CUNY.\n\n\nCode\n# 1. Employees pay a fixed percentage of their paycheck into the pension fund\n\ntrs_pension &lt;- yearly_salary_df |&gt;\n  mutate(tier_6 = round(case_when(\n    salary &lt;= 45000 ~ salary * 0.03,\n    (salary &gt; 45000 & salary &lt;= 55000) ~ salary * 0.035,\n    (salary &gt; 55000 & salary &lt;= 75000) ~ salary * 0.045,\n    (salary &gt; 75000 & salary &lt;= 100000) ~ salary * 0.0575,\n    salary &gt; 100000 ~ salary * 0.06\n  ), 2))\n\ntotal_contribution &lt;- trs_pension |&gt;\n  summarize(total = sum(tier_6)) |&gt;\n  pull(total)\n\nDT::datatable(setNames(trs_pension, c(\"Year\", \"Salary After Adjustments\", \"Total Contributions\")), \n              caption = \"Table 4: TRS Yearly Pension Contributions\",\n              rownames = FALSE)\n\n\n\n\n\n\nAfter 20 years of employment with CUNY, under the TRS pension plan, this employee would have contributed a total of $97,049.99.\nNext, we’ll need to calculate the employee’s final average salary (FAS) based on their salary in their last three years of employment, so in this case it would be 2022, 2023, and 2024. The final average salary based on these three years of employment came out to $148,763.44. We will then use this FAS towards calculating the employee’s annual retirement benefit. We will consider the following cases when calculating the first year’s retirement benefit:\n\n1.67\\% * \\text{FAS} * N if N \\leq 20\n1.75\\% * \\text{FAS} * N if N = 20\n(35\\% + 2\\% * N) * \\text{FAS} if N \\geq 20\n\nSince our employee worked for 20 years, we will be using the second formula to calculate their initial retirement benefit.\n\n\nCode\n# 2. The retirement benefit is calculated based on the Final Average Salary of the employee (last three years of work)\n\nfinal_average_salary &lt;- yearly_salary_df |&gt;\n  arrange(year) |&gt;\n  slice_tail(n = 3) |&gt;\n  summarize(fas = round(mean(salary), 2)) |&gt;\n  pull(fas)\n\n\nretirement_benefit_trs &lt;- round(case_when(\n  nrow(yearly_salary_df) &lt; 20 ~ 0.0167 * final_average_salary * nrow(yearly_salary_df),\n  nrow(yearly_salary_df) == 20 ~ 0.0175 * final_average_salary * nrow(yearly_salary_df),\n  nrow(yearly_salary_df) &gt; 20 ~ (0.35 + 0.2 * nrow(yearly_salary_df)) * final_average_salary,\n), 2)\n\n\nAfter all calculations, we will find that the employee will receive an annual retirement payment in 2025 of $52,067.2, or a monthly payout of $4,338.93.\nNext, we will take a look at the first month’s payout from CUNY’s Optional Retirement Plan (ORP).\n\n\nORP\nIn the CUNY ORP, the employees contribute at the same rate as the TRS, with the following brackets:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nHowever, now the employee’s are contributing directly to their own retirement into their ORP account. Their employers, CUNY in this case, will also contribute a fraction of their employee’s salary based on these conditions:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nBelow is the code to create the data frame containing all the employee and employer contributions based on salary and years of employment from 2005 to 2024.\n\n\nCode\norp_account &lt;- yearly_salary_df |&gt;\n  mutate(orp_contributions = round(case_when(\n    salary &lt;= 45000 ~ salary * 0.03,\n    (salary &gt; 45000 & salary &lt;= 55000) ~ salary * 0.035,\n    (salary &gt; 55000 & salary &lt;= 75000) ~ salary * 0.045,\n    (salary &gt; 75000 & salary &lt;= 100000) ~ salary * 0.0575,\n    salary &gt; 100000 ~ salary * 0.06\n  ), 2)) |&gt;\n  mutate(employer_contributions = round(case_when(\n    (year - 2005 + 1) &lt;= 7 ~ 0.08 * salary,\n    .default = 0.1 * salary\n  ), 2)) |&gt;\n  mutate(total_contributions = round(orp_contributions + employer_contributions, 2)) |&gt;\n  mutate(age = (35:54))\n\nDT::datatable(setNames(orp_account, c(\"Year\", \"Salary After Adjustments\", \"Employee Contributions\", \"Employer Contributions\", \"Total Contributions\", \"Age\")), \n              caption = \"Table 5: ORP Employee and Employer Yearly Contributions\",\n              rownames = FALSE)\n\n\n\n\n\n\nORP is a retirement plan that invests these contributions into the employee’s choice of mutual funds, for this project, we will assume that our employee invests in a Fidelity Freedom Fund, which allocates assets the following way:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nSo, based on the age of the employee, we will need to allocate the annual contributions accordingly. We will also need to account for the market fluctuation of each of these assets. So we’ll use the annual aggregated data for each of the markets (US Equity, International Equity, Bonds, and Short-Term Debts) to adjust the retirement account balance before the start of the following year.\nBelow is the code to calculate all the above steps and to ultimately calculate the final balance once the employee retires and stops contributing.\n\n\nCode\n# Vectors from the data frame to run a for loop on to calculate total retirement benefit by the time of retirement\n\nage &lt;- orp_account$age\ntotal_contributions &lt;- orp_account$total_contributions\n\n# Vectors for each market's returns\n\nus_equity_yearly &lt;- all_metrics_summary$annual_us_equity_returns\nint_equity_yearly &lt;- all_metrics_summary$annual_int_equity_returns\nbond_yearly &lt;- all_metrics_summary$annual_bond_value\nshort_term_yearly &lt;- all_metrics_summary$annual_short_term_value\n\n# Initially no investment into any of these assets\n\nus_equities_total &lt;- 0\nint_equities_total &lt;- 0\nbonds_total &lt;- 0\nshort_term_total &lt;- 0\n\nfor (i in 1:20) {\n  \n  # Asset allocation based on age of employee\n  \n  if (age[i] &gt;= 25 & age[i] &lt; 50) {\n    us_equities_total &lt;- us_equities_total + total_contributions[i] * 0.54\n    int_equities_total &lt;- int_equities_total + total_contributions[i] * 0.36\n    bonds_total &lt;- bonds_total + total_contributions[i] * 0.1\n    short_term_total &lt;- short_term_total\n  } else if (age[i] &gt;= 50 & age[i] &lt; 60) {\n    us_equities_total &lt;- us_equities_total + total_contributions[i] * 0.47\n    int_equities_total &lt;- int_equities_total + total_contributions[i] * 0.32\n    bonds_total &lt;- bonds_total + total_contributions[i] * 0.21\n    short_term_total &lt;- short_term_total\n  } else if (age[i] &gt;= 60 & age[i] &lt; 75) {\n    us_equities_total &lt;- us_equities_total + total_contributions[i] * 0.34\n    int_equities_total &lt;- int_equities_total + total_contributions[i] * 0.23\n    bonds_total &lt;- bonds_total + total_contributions[i] * 0.43\n    short_term_total &lt;- short_term_total\n  } else if (age[i] &gt;= 75) {\n    us_equities_total &lt;- us_equities_total + total_contributions[i] * 0.19\n    int_equities_total &lt;- int_equities_total + total_contributions[i] * 0.13\n    bonds_total &lt;- bonds_total + total_contributions[i] * 0.62\n    short_term_total &lt;- short_term_total * total_contributions[i] * 0.06\n  }\n  \n  # Need to account for market returns for each asset, will use long run average\n  \n  us_equities_total &lt;- round(us_equities_total * (1 + us_equity_yearly[i] / 100), 2)\n  int_equities_total &lt;- round(int_equities_total * (1 + int_equity_yearly[i] / 100), 2)\n  bonds_total &lt;- round(bonds_total * (1 + bond_yearly[i] / 100), 2)\n  short_term_total &lt;- round(short_term_total * (1 + short_term_yearly[i] / 100), 2)\n  \n}\n\norp_total_investment &lt;- us_equities_total + int_equities_total + bonds_total + short_term_total\n\n\n# General rule of thumb is withdrawing 4% of the value per year\n\nfirst_month_withdrawal &lt;- round((orp_total_investment * 0.04) / 12, 2)\n\n\nAfter working for 20 years, the CUNY employee would end up with a total of $675,728.3 in their ORP retirement account. Typically, individuals would withdraw about 4% of their retirement account’s value per year. So, we will take the assumption that our individual will withdraw 4% of their ORP account every year until they pass away. Thus, for their first month of retirement, they would withdraw $2,252.43.\nComparing the first month’s retirement payout for each plan, we find that the TRS plan provides a bigger payout than the ORP, with $4,338.93 versus $2,252.43 from the ORP. While the TRS plan has shown its benefits in early retirement stages as compared to ORP, it still is not yet clear which plan will be most beneficial in the long term.\nNext, we will take a look at the long-term average analysis, with future projections of our data based on the long-run averages we calculated earlier."
  },
  {
    "objectID": "mp04.html#long-term-average-analysis",
    "href": "mp04.html#long-term-average-analysis",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Long-Term Average Analysis",
    "text": "Long-Term Average Analysis\nThe “first month of retirement” dollar value provides little insight, as it is not representative of the funds that will support the employee throughout the entire retirement period. The TRS will always guarantee an income for the remainder of an employee’s life, while the ORP carries more market risk and can be exhausted if the employee lives a very long time in retirement.\nFor the purposes of this projection, we will take the assumption that our hypothetical employee will die at age 77, 23 years after retirement.\nWe will modify our above simulation to project the employee’s retirement benefit from the TRS plan and the annual withdrawal amount from retirement (55) until death (77). To accommodate the necessary cost-of-living-adjustments for the TRS and the future market returns (ORP), we will use the long-run averages from earlier. As mentioned earlier, this “fixed rate” assumption is not entirely representative of the possible future values, but we will accommodate this in a later section.\nFor the TRS retirement plan, the first year’s retirement benefit is calculated as mentioned above as:\n\n1.67\\% * \\text{FAS} * N if N \\leq 20\n1.75\\% * \\text{FAS} * N if N = 20\n(35\\% + 2\\% * N) * \\text{FAS} if N \\geq 20\n\n(where the FAS is the final average salary of the LAST THREE years of employment).\nFollowing this, for the remainder of retirement, the benefit is increased annually by 50% of the CPI, but this is capped at 1% and 3%.\nAccommodating the above conditions, below is the code calculating the annual and monthly retirement benefit that a CUNY employee would receive in retirement with the TRS plan.\n\n\nCode\n# Current age: 55\n# Estimated death age: 77\n\n# Long Run Averages:\n\ninflation_avg &lt;- long_run_averages$long_run_inflation\n\nyearly_retire_benefit &lt;- retirement_benefit_trs\n\ncol_trs &lt;- c(\"age\", \"annual_retirement_benefit\", \"monthly_payout\")\n\ntrs_retirement &lt;- data.frame(matrix(nrow = 0, ncol = length(col_trs)))\n\nfor (i in 55:77) {\n  \n  monthly_pay &lt;- round(yearly_retire_benefit / 12, 2)\n  \n  trs_retirement &lt;- rbind(trs_retirement, c(i, yearly_retire_benefit, monthly_pay))\n  \n  # CPI adjustment for the next year, using long run inflation average\n  \n  adjusted_benefit &lt;- case_when(\n    inflation_avg / 2 &lt; 1 ~ round(yearly_retire_benefit * 1.01, 2),\n    inflation_avg / 2 &gt; 3 ~ round(yearly_retire_benefit * 1.03, 2),\n    .default = round(yearly_retire_benefit * (1 + ((inflation_avg / 2) / 100))\n  , 2))\n  \n  yearly_retire_benefit &lt;- adjusted_benefit\n}\n\ncolnames(trs_retirement) = col_trs\n\nDT::datatable(setNames(trs_retirement, c(\"Age\", \"Annual Payout\", \"Monthly Payout\")), \n              caption = \"Table 6: Employee's TRS Plan Payout in Retirement\",\n              rownames = FALSE)\n\n\n\n\n\n\nFrom this TRS projection, we can observe that the employee’s annual payout remains between about $52,000 and $70,000 until their time of death. So, while the value of their annual payout does not increase much over time, they are guaranteed a decent share of income each year, about $4,000-$6,000 a month. However, upon death, there are no more benefits for the employee’s family, which is something else one may want to consider when selecting a retirement plan.\nNext, we’ll take a look at the future projections for the CUNY ORP. For this plan, the contributions end once the employee retires, however, the retirement account continues to fluctuate based on market behaviors, which can lead to very positive returns if the market is strong.\nBelow is the code for the data frame which assumes a 4% withdrawal rate for each year and applies the appropriate market fluctuations for each year of retirement based on our long-run averages calculated earlier.\n\n\nCode\n# Current age: 55\n# Estimated death age: 77\n\n# Long Run Averages:\n\nus_equity_avg &lt;- long_run_averages$long_run_us_equity_returns\nint_equity_avg &lt;- long_run_averages$long_run_int_equity_returns\nbond_avg &lt;- long_run_averages$long_run_bond_value\nshort_term_avg &lt;- long_run_averages$long_run_short_term_value\n\n\nremaining_total &lt;- orp_total_investment\n\nremaining_us &lt;- us_equities_total\nremaining_int &lt;- int_equities_total\nremaining_bond &lt;- bonds_total\nremaining_short_term &lt;- short_term_total\n\ncol_orp &lt;- c(\"age\", \"yearly_withdrawal\", \"monthly_withdrawal\", \"remaining_us_equities_amount\", \"remaining_int_equities_amount\", \"remaining_bond_amount\", \"remaining_short_term_amount\", \"remaining_amount\")\n\norp_retirement &lt;- data.frame(matrix(nrow = 0, ncol = length(col_orp)))\n\n\nfor (i in 55:77) {\n  if (remaining_total &gt;= 0) {\n    \n    year_withdrawal &lt;- round(remaining_total * 0.04, 2)\n    month_withdrawal = round(year_withdrawal / 12, 2)\n    \n    remaining_us &lt;- round(remaining_us * 0.96, 2)\n    remaining_int &lt;- round(remaining_int * 0.96, 2)\n    remaining_bond &lt;- round(remaining_bond * 0.96, 2)\n    remaining_short_term &lt;- round(remaining_short_term * 0.96, 2)\n\n    remaining_total &lt;- round(remaining_us + remaining_int + remaining_bond + remaining_short_term, 2)\n    \n    orp_retirement &lt;- rbind(orp_retirement, c(i, year_withdrawal, month_withdrawal, remaining_us, remaining_int, remaining_bond, remaining_short_term, remaining_total))\n    \n    # Need to adjust the total based on market fluctuations\n    \n    remaining_us &lt;- round(remaining_us * (1 + us_equity_avg / 100), 2)\n    remaining_int &lt;- round(remaining_int * (1 + int_equity_avg / 100), 2)\n    remaining_bond &lt;- round(remaining_bond * (1 + bond_avg / 100), 2)\n    remaining_short_term &lt;- round(remaining_short_term * (1 + short_term_avg / 100), 2)\n    \n  } else {\n    print(paste0(\"No More Funds! Stopped at age \", i))\n    break\n  }\n}\n\ncolnames(orp_retirement) = col_orp\n\nDT::datatable(setNames(orp_retirement, c(\"Age\", \"Yearly Withdrawal\", \"Monthly Withdrawal\", \"Remaining in US Equities\", \"Remaining in International Equities\", \"Remaining in Bonds\", \"Remaining in Short-Term Debts\", \"Total Remaining\")), \n              caption = \"Table 7: Employee's ORP Withdrawal in Retirement\",\n              rownames = FALSE)\n\n\n\n\n\n\nCode\nremainder_heirs &lt;- orp_retirement |&gt;\n  arrange(age) |&gt;\n  slice_tail(n = 1) |&gt;\n  pull(remaining_amount)\n\n\nBased on the above table, we observe that the annual and monthly withdrawals initially are less than that of the TRS plan. However, over time, these withdrawals start exceeding the TRS payouts. Additionally, based on our market fluctuation assumptions, we have that at the time of death, the employee’s retirement account has grown to over $2,500,000, money that can be passed down to their family (e.g. spouse, heirs, etc.), something that the TRS plan does not offer. In this above case, the risk taken for the ORP plan has paid off significantly, allowing the employee to live comfortably during retirement and have money leftover for their family upon their death.\nBelow, we will take a closer look at the comparison of minimum, maximum, and average monthly payouts between the TRS and the ORP retirement plans.\n\n\nCode\ncombined_retirement_df &lt;- inner_join(trs_retirement, orp_retirement, by = 'age')\n\nmonthly_amount &lt;- combined_retirement_df |&gt;\n  select(c(\"age\", \"monthly_payout\", \"monthly_withdrawal\")) |&gt;\n  rename(monthly_TRS = monthly_payout,\n         monthly_ORP = monthly_withdrawal)\n\nmonthly_amount_summary &lt;- monthly_amount |&gt;\n  summarize(min_month_trs = min(monthly_TRS),\n            min_month_orp = min(monthly_ORP),\n            max_month_trs = max(monthly_TRS),\n            max_month_orp = max(monthly_ORP),\n            avg_month_trs = round(mean(monthly_TRS), 2),\n            avg_month_orp = round(mean(monthly_ORP), 2))\n\nDT::datatable(setNames(monthly_amount_summary, c(\"Minimum Monthly \\nTRS Payout\", \"Minimum Monthly \\nORP Payout\", \"Maximum Monthly \\nTRS Payout\", \"Maximum Monthly \\nORP Payout\", \"Average Monthly \\nTRS Payout\", \"Average Monthly \\nORP Payout\")), \n              caption = \"Table 8: Summary Monthly Payout for each Retirement Plan\",\n              rownames = FALSE)\n\n\n\n\n\n\nFrom table 8, we find that the minimum monthly payout for TRS is higher than that of the ORP retirement plan, by about $2,000. On the other hand, the maximum ORP monthly withdrawal reaches as high as $8,255.70, compared to only $5752.38 from the TRS, demonstrating an instance of a positive market return over 23 years for the ORP. The average monthly payout between each plan is much closer, with TRS edging out slightly at $5,013.99 compared to ORP’s $4,448.13.\nTo take a closer look at the gaps between monthly payouts between each retirement plan for each year of the employee’s retirement, we will create the following data frame.\n\n\nCode\nmonthly_amount_gaps &lt;- monthly_amount |&gt;\n  mutate(gap = round(abs(monthly_TRS - monthly_ORP), 2),\n         greater = case_when(\n           monthly_TRS &gt; monthly_ORP ~ \"TRS\",\n           monthly_ORP &gt; monthly_TRS ~ \"ORP\",\n           .default = \"EQUAL\"\n         )) |&gt;\n  select(c(\"age\", \"gap\", \"greater\"))\n\nDT::datatable(setNames(monthly_amount_gaps, c(\"Age\", \"Monthly Payment Gap\", \"Plan with Larger Monthly Payout\")), \n              caption = \"Table 9: Monthly Payout Gap per Year\",\n              rownames = FALSE)\n\n\n\n\n\n\nFrom this data table, there is a more evident pattern between the differences in monthly payouts by retirement plan. In the first 16 years of the employee’s retirement, the TRS retirement plan edges ahead of ORP by a range of about $27 to as much as $2,100. After the employee turns 71, the ORP takes the lead until their death from a range of $255 to over $2,500 at the time of death. So while the TRS provides a steady stream of income to their employee’s during retirement, taking the risk with the ORP may pay out better in the long term.\nBelow is a bar plot providing a clearer distinction between the monthly payouts/withdrawals of each plan over the course of the employee’s retirement.\n\n\nCode\nmonthly_amount |&gt;\n  pivot_longer(!age, names_to = \"plan\", values_to = \"monthly_amt\") |&gt;\n  ggplot(aes(x = age, y = monthly_amt, fill = plan)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(title = \"Monthly Amount Received during Retirement for Employee\",\n       subtitle = \"ORP vs TRS\",\n       x = \"Age\",\n       y = \"Monthly Amount ($)\") +\n  theme_bw() +\n  scale_fill_discrete(name = \"Retirement\\nPlan\", labels = c(\"ORP\", \"TRS\"))\n\n\n\n\n\n\n\n\n\nLike we observed earlier, we find that the ORP takes over with the higher amount of monthly pay out from TRS when our employee turns 71, but of course this is with our long-run average assumptions for market returns which can be limiting.\nEach retirement plan has different advantages and disadvantages based on various factors. TRS provides their retirees with a guaranteed comfortable salary until their death, but does not provide any funds beyond this. ORP retirement accounts will fluctuate with the market and does not guarantee a payout every year of retirement. However, if the market works in your favor more times than not, retirees may have money saved even beyond their death to pass down to family.\nBefore providing our recommendation, we will run a few more simulations using bootstrap sampling to gain a larger perspective on each retirement plan."
  },
  {
    "objectID": "mp04.html#bootstrap-monte-carlo-comparison",
    "href": "mp04.html#bootstrap-monte-carlo-comparison",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Bootstrap (Monte Carlo) Comparison",
    "text": "Bootstrap (Monte Carlo) Comparison\nOur findings above are limited based on long-run averages calculated from the original data we retrieved. To mitigate this, we will use our historical data to generate 200 bootstrap histories for a Monte Carlo analysis. We will use bootstrap sampling to generate the values for the “while working” and the “while retired” periods of the model. More specifically, we will be generating values with bootstrapping for all the data we originally retrieved from the APIs (wage growth, inflation, US equity returns, International equity returns, bond returns, short-term debt returns). So we will no longer be assuming the same constant long-term average values for the retirement predictions.\n\nTRS\nBelow is the code used to simulate the TRS retirement plan for our new 200 bootstrap samples. We will set our “seed” to 100, to allow replication of our results in the future. We will store each of our 200 data frames in the list trs_bootstrap_histories, for ease of access later.\n\n\nCode\nset.seed(100) # maintains the same randomness each time we run the code\n\n# Wage Growth\n\nall_metrics_summary_no_yr &lt;- all_metrics_summary |&gt;\n  select(-c(\"year\"))\n\nnum_bootstraps &lt;- 200 # want to run 200 \"bootstrap histories\"\n\ntrs_bootstrap_histories &lt;- list() # Create a list to fill with all the ORP bootstrap data frames\n\nfor (i in 1:num_bootstraps) {\n  \n  # Create new bootstrap sample of the data\n  \n  bootstrap_df &lt;- all_metrics_summary_no_yr |&gt;\n    slice_sample(n = nrow(all_metrics_summary_no_yr), replace = TRUE)\n  \n  # Find the new long run averages based on bootstrapped data\n  \n  long_run_averages_bootstrap &lt;- bootstrap_df |&gt;\n  summarize(long_run_wage_growth = round(mean(annual_wage_growth), 2),\n            long_run_inflation = round(mean(annual_inflation), 2),\n            long_run_us_equity_returns = round(mean(annual_us_equity_returns), 2),\n            long_run_int_equity_returns = round(mean(annual_int_equity_returns), 2),\n            long_run_bond_value = round(mean(annual_bond_value), 2),\n            long_run_short_term_value = round(mean(annual_short_term_value), 2))\n  \n  # Salary adjustment based on inflation and wage growth\n  \n  yearly_wage_growth_bootstrap &lt;- bootstrap_df$annual_wage_growth\n\n  yearly_inflation_bootstrap &lt;- bootstrap_df$annual_inflation\n  \n  salary_vector_bootstrap &lt;- c(50000)\n\n  for (j in 2:length(yearly_wage_growth_bootstrap)) {\n    start_salary = salary_vector_bootstrap[j-1]\n    salary = (start_salary * (1 + (yearly_wage_growth_bootstrap[j] / 100))) / (1 - (yearly_inflation_bootstrap[j] / 100))\n    salary_vector_bootstrap = c(salary_vector_bootstrap, salary)\n  }\n\n  yearly_salary_df_bootstrap &lt;- data.frame(year = (2005:2024),\n                                 salary = salary_vector_bootstrap) |&gt;\n    mutate(salary = round(salary, 2))\n\n  # TRS Simulation\n\n  # 1. Employees pay a fixed percentage of their paycheck into the pension fund\n\n  trs_pension_bootstrap &lt;- yearly_salary_df_bootstrap |&gt;\n    mutate(tier_6 = round(case_when(\n      salary &lt;= 45000 ~ salary * 0.03,\n      (salary &gt; 45000 & salary &lt;= 55000) ~ salary * 0.035,\n      (salary &gt; 55000 & salary &lt;= 75000) ~ salary * 0.045,\n      (salary &gt; 75000 & salary &lt;= 100000) ~ salary * 0.0575,\n      salary &gt; 100000 ~ salary * 0.06\n    ), 2))\n\n  # 2. The retirement benefit is calculated based on the Final Average Salary of the employee (last three years of work)\n\n  final_average_salary_bootstrap &lt;- yearly_salary_df_bootstrap |&gt;\n    arrange(year) |&gt;\n    slice_tail(n = 3) |&gt;\n    summarize(fas = round(mean(salary), 2)) |&gt;\n    pull(fas)\n  \n  retirement_benefit_trs_bootstrap &lt;- round(case_when(\n    nrow(yearly_salary_df_bootstrap) &lt; 20 ~ 0.0167 * final_average_salary_bootstrap * nrow(yearly_salary_df_bootstrap),\n    nrow(yearly_salary_df_bootstrap) == 20 ~ 0.0175 * final_average_salary_bootstrap * nrow(yearly_salary_df_bootstrap),\n    nrow(yearly_salary_df_bootstrap) &gt; 20 ~ (0.35 + 0.2 * nrow(yearly_salary_df_bootstrap)) * final_average_salary_bootstrap,\n  ), 2)\n\n  inflation_avg_bootstrap &lt;- long_run_averages_bootstrap$long_run_inflation\n\n  # TRS: During Retirement\n\n  yearly_retire_benefit_bootstrap &lt;- retirement_benefit_trs_bootstrap\n\n  col_trs_bootstrap &lt;- c(\"age\", \"annual_retirement_benefit\", \"monthly_payout\")\n\n  trs_retirement_bootstrap &lt;- data.frame(matrix(nrow = 0, ncol = length(col_trs_bootstrap)))\n\n  for (k in 55:77) {\n\n    monthly_pay_bootstrap &lt;- round(yearly_retire_benefit_bootstrap / 12, 2)\n\n    trs_retirement_bootstrap &lt;- rbind(trs_retirement_bootstrap, c(k, yearly_retire_benefit_bootstrap, monthly_pay_bootstrap))\n\n    # CPI adjustment for the next year, using long run inflation average\n\n    adjusted_benefit_bootstrap &lt;- case_when(\n      inflation_avg_bootstrap / 2 &lt; 1 ~ round(yearly_retire_benefit_bootstrap * 1.01, 2),\n      inflation_avg_bootstrap / 2 &gt; 3 ~ round(yearly_retire_benefit_bootstrap * 1.03, 2),\n      .default = round(yearly_retire_benefit_bootstrap * (1 + ((inflation_avg_bootstrap / 2) / 100))\n    , 2))\n\n    yearly_retire_benefit_bootstrap &lt;- adjusted_benefit_bootstrap\n  }\n\n  colnames(trs_retirement_bootstrap) = col_trs_bootstrap\n  \n  trs_bootstrap_histories &lt;- append(trs_bootstrap_histories, list(trs_retirement_bootstrap)) # Appends the list with the TRS bootstrap data frame\n\n}\n\n\n\n\nORP\nBelow is the code used to simulate the ORP for our new 200 bootstrap samples. We will set the same “seed” to 100, to allow replication of our results in the future and to keep some consistency by utilizing the same bootstrap as we did in our previous code with TRS. Similarly, we will store each of our 200 data frames in the list orp_bootstrap_histories, for ease of access later.\n\n\nCode\nset.seed(100) # maintains the same randomness each time we run the code\n\n# Wage Growth\n\nall_metrics_summary_no_yr &lt;- all_metrics_summary |&gt;\n  select(-c(\"year\"))\n\nnum_bootstraps &lt;- 200 # want to run 200 \"bootstrap histories\"\n\nran_out &lt;- 0 # number of times the ORP retirement fund ran out before death\n\norp_bootstrap_histories &lt;- list() # Create a list to fill with all the ORP bootstrap data frames\n\nfor (i in 1:num_bootstraps) {\n  \n  # Create new bootstrap sample of the data\n  \n  bootstrap_df &lt;- all_metrics_summary_no_yr |&gt;\n    slice_sample(n = nrow(all_metrics_summary_no_yr), replace = TRUE)\n  \n  # Find the new long run averages based on bootstrapped data\n  \n  long_run_averages_bootstrap &lt;- bootstrap_df |&gt;\n  summarize(long_run_wage_growth = round(mean(annual_wage_growth), 2),\n            long_run_inflation = round(mean(annual_inflation), 2),\n            long_run_us_equity_returns = round(mean(annual_us_equity_returns), 2),\n            long_run_int_equity_returns = round(mean(annual_int_equity_returns), 2),\n            long_run_bond_value = round(mean(annual_bond_value), 2),\n            long_run_short_term_value = round(mean(annual_short_term_value), 2))\n  \n  # Salary adjustment based on inflation and wage growth\n  \n  yearly_wage_growth_bootstrap &lt;- bootstrap_df$annual_wage_growth\n\n  yearly_inflation_bootstrap &lt;- bootstrap_df$annual_inflation\n  \n  salary_vector_bootstrap &lt;- c(50000)\n\n  for (i in 2:length(yearly_wage_growth_bootstrap)) {\n    start_salary = salary_vector_bootstrap[i-1]\n    salary = (start_salary * (1 + (yearly_wage_growth_bootstrap[i] / 100))) / (1 - (yearly_inflation_bootstrap[i] / 100))\n    salary_vector_bootstrap = c(salary_vector_bootstrap, salary)\n  }\n\n  yearly_salary_df_bootstrap &lt;- data.frame(year = (2005:2024),\n                                 salary = salary_vector_bootstrap) |&gt;\n    mutate(salary = round(salary, 2))\n\n\n  # ORP Simulation\n  \n  orp_account_bootstrap &lt;- yearly_salary_df_bootstrap |&gt;\n    mutate(orp_contributions = round(case_when(\n      salary &lt;= 45000 ~ salary * 0.03,\n      (salary &gt; 45000 & salary &lt;= 55000) ~ salary * 0.035,\n      (salary &gt; 55000 & salary &lt;= 75000) ~ salary * 0.045,\n      (salary &gt; 75000 & salary &lt;= 100000) ~ salary * 0.0575,\n      salary &gt; 100000 ~ salary * 0.06\n    ), 2)) |&gt;\n    mutate(employer_contributions = round(case_when(\n      (year - 2005 + 1) &lt;= 7 ~ 0.08 * salary,\n      .default = 0.1 * salary\n    ), 2)) |&gt;\n    mutate(total_contributions = orp_contributions + employer_contributions) |&gt;\n    mutate(age = (35:54))\n  \n  # Vectors from the data frame to run a for loop on to calculate total retirement benefit by the time of retirement\n\n  age_bootstrap &lt;- orp_account_bootstrap$age\n  total_contributions_bootstrap &lt;- orp_account_bootstrap$total_contributions\n\n  # Vectors for each market's returns\n\n  us_equity_yearly_bootstrap &lt;- bootstrap_df$annual_us_equity_returns\n  int_equity_yearly_bootstrap &lt;- bootstrap_df$annual_int_equity_returns\n  bond_yearly_bootstrap &lt;- bootstrap_df$annual_bond_value\n  short_term_yearly_bootstrap &lt;- bootstrap_df$annual_short_term_value\n\n  # Initially no investment into any of these assets\n\n  us_equities_total_bootstrap &lt;- 0\n  int_equities_total_bootstrap &lt;- 0\n  bonds_total_bootstrap &lt;- 0\n  short_term_total_bootstrap &lt;- 0\n\n  for (i in 1:20) {\n\n    # Asset allocation based on age of employee\n\n    if (age[i] &gt;= 25 & age[i] &lt; 50) {\n      us_equities_total_bootstrap &lt;- us_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.54\n      int_equities_total_bootstrap &lt;- int_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.36\n      bonds_total_bootstrap &lt;- bonds_total_bootstrap + total_contributions_bootstrap[i] * 0.1\n      short_term_total_bootstrap &lt;- short_term_total_bootstrap\n    } else if (age[i] &gt;= 50 & age[i] &lt; 60) {\n      us_equities_total_bootstrap &lt;- us_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.47\n      int_equities_total_bootstrap &lt;- int_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.32\n      bonds_total_bootstrap &lt;- bonds_total_bootstrap + total_contributions_bootstrap[i] * 0.21\n      short_term_total_bootstrap &lt;- short_term_total_bootstrap\n    } else if (age[i] &gt;= 60 & age[i] &lt; 75) {\n      us_equities_total_bootstrap &lt;- us_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.34\n      int_equities_total_bootstrap &lt;- int_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.23\n      bonds_total_bootstrap &lt;- bonds_total_bootstrap + total_contributions_bootstrap[i] * 0.43\n      short_term_total_bootstrap &lt;- short_term_total_bootstrap\n    } else if (age[i] &gt;= 75) {\n      us_equities_total_bootstrap &lt;- us_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.19\n      int_equities_total_bootstrap &lt;- int_equities_total_bootstrap + total_contributions_bootstrap[i] * 0.13\n      bonds_total_bootstrap &lt;- bonds_total_bootstrap + total_contributions_bootstrap[i] * 0.62\n      short_term_total_bootstrap &lt;- short_term_total_bootstrap * total_contributions_bootstrap[i] * 0.06\n    }\n    \n    # Need to account for each year's aggregated market return\n\n    us_equities_total_bootstrap &lt;- round(us_equities_total_bootstrap * (1 + us_equity_yearly_bootstrap[i] / 100), 2)\n    int_equities_total_bootstrap &lt;- round(int_equities_total_bootstrap * (1 + int_equity_yearly_bootstrap[i] / 100), 2)\n    bonds_total_bootstrap &lt;- round(bonds_total_bootstrap * (1 + bond_yearly_bootstrap[i] / 100), 2)\n    short_term_total_bootstrap &lt;- round(short_term_total_bootstrap * (1 + short_term_yearly_bootstrap[i] / 100), 2)\n\n  }\n\n  orp_total_investment_bootstrap &lt;- us_equities_total_bootstrap + int_equities_total_bootstrap + bonds_total_bootstrap + short_term_total_bootstrap\n\n  # ORP: During Retirement\n\n  us_equity_avg_bootstrap &lt;- long_run_averages_bootstrap$long_run_us_equity_returns\n  int_equity_avg_bootstrap &lt;- long_run_averages_bootstrap$long_run_int_equity_returns\n  bond_avg_bootstrap &lt;- long_run_averages_bootstrap$long_run_bond_value\n  short_term_avg_bootstrap &lt;- long_run_averages_bootstrap$long_run_short_term_value\n\n  remaining_total_bootstrap &lt;- orp_total_investment_bootstrap\n\n  remaining_us_bootstrap &lt;- us_equities_total_bootstrap\n  remaining_int_bootstrap &lt;- int_equities_total_bootstrap\n  remaining_bond_bootstrap &lt;- bonds_total_bootstrap\n  remaining_short_term_bootstrap &lt;- short_term_total_bootstrap\n\n  col_orp_bootstrap &lt;- c(\"age\", \"yearly_withdrawal\", \"monthly_withdrawal\", \"remaining_us_equities_amount\", \"remaining_int_equities_amount\", \"remaining_bond_amount\", \"remaining_short_term_amount\", \"remaining_amount\")\n\n  orp_retirement_bootstrap &lt;- data.frame(matrix(nrow = 0, ncol = length(col_orp_bootstrap)))\n\n  # Accounting for yearly withdrawals and market fluctuations each year\n\n  for (i in 55:77) {\n    if (remaining_total_bootstrap &gt;= 0) {\n\n      year_withdrawal_bootstrap &lt;- round(remaining_total_bootstrap * 0.04, 2)\n      month_withdrawal_bootstrap = round(year_withdrawal_bootstrap / 12, 2)\n\n      remaining_us_bootstrap &lt;- round(remaining_us_bootstrap * 0.96, 2)\n      remaining_int_bootstrap &lt;- round(remaining_int_bootstrap * 0.96, 2)\n      remaining_bond_bootstrap &lt;- round(remaining_bond_bootstrap * 0.96, 2)\n      remaining_short_term_bootstrap &lt;- round(remaining_short_term_bootstrap * 0.96, 2)\n\n      remaining_total_bootstrap &lt;- round(remaining_us_bootstrap + remaining_int_bootstrap + remaining_bond_bootstrap + remaining_short_term_bootstrap, 2)\n\n      orp_retirement_bootstrap &lt;- rbind(orp_retirement_bootstrap, c(i, year_withdrawal_bootstrap, month_withdrawal_bootstrap, remaining_us_bootstrap, remaining_int_bootstrap, remaining_bond_bootstrap, remaining_short_term_bootstrap, remaining_total_bootstrap))\n\n      # Need to adjust the total based on market fluctuations\n\n      remaining_us_bootstrap &lt;- round(remaining_us_bootstrap * (1 + us_equity_avg_bootstrap / 100), 2)\n      remaining_int_bootstrap &lt;- round(remaining_int_bootstrap * (1 + int_equity_avg_bootstrap / 100), 2)\n      remaining_bond_bootstrap &lt;- round(remaining_bond_bootstrap * (1 + bond_avg_bootstrap / 100), 2)\n      remaining_short_term_bootstrap &lt;- round(remaining_short_term_bootstrap * (1 + short_term_avg_bootstrap / 100), 2)\n\n    } else {\n      \n      # Stops the loop if the retiree runs out of funds before death\n      \n      print(paste0(\"No More Funds! Stopped at age \", i))\n\n      ran_out &lt;- ran_out + 1\n      break\n    }\n  }\n\n  colnames(orp_retirement_bootstrap) = col_orp_bootstrap\n  \n  orp_bootstrap_histories &lt;- append(orp_bootstrap_histories, list(orp_retirement_bootstrap)) # Appends the list with the ORP bootstrap data frame\n}\n\n\nAfter running these simulations, I would like to take a closer look at the various results for each of the retirement plans. Specifically, for the ORP, I would like to find out whether or not there were instances where the employee ran out of funds before death, due to the volatility and unpredictability of the market, it is possible that the ORP retirement account reacts differently in each instance.\nFrom the data, we find that in every bootstrap sample of the ORP, the employee still has funds remaining in their retirement account, so 0% of the time does the employee actually run out of funds to support them in retirement. So, maybe given the market patterns over the past 20 years, it would make sense for employees to consider the ORP over the TRS, given that they will more than likely have enough funds to support them well into retirement and even for their families after they pass.\n\n\n\n\n\n\nNext, I would also like to explore the probability that an employee with the ORP will have a higher monthly income than the employee with a TRS plan in retirement. To do so, I will take an aggregated average across all retirement years from each data frame then compare each ORP value against the respective TRS value for that bootstrap sample.\n\n\nCode\norp_higher &lt;- 0 # counter to see how many times the overall orp monthly payout came out higher than the overall trs monthly payout\n\nfor (i in 1:200) {\n  \n  # Need to calculate the average monthly payout for each ORP and TRS bootstrap and compare them\n  \n  # Monthly Average for TRS Plan\n  \n  trs_monthly_avg &lt;- trs_bootstrap_histories[[i]] |&gt;\n    summarize(avg = round(mean(monthly_payout), 2)) |&gt;\n    pull(avg)\n  \n  # Monthly Average for ORP\n  \n  orp_monthly_avg &lt;- orp_bootstrap_histories[[i]] |&gt;\n    summarize(avg = round(mean(monthly_withdrawal), 2)) |&gt;\n    pull(avg)\n  \n  # Add to counter if the ORP monthly withdrawal was higher than that of TRS for the same bootstrap sample\n  \n  if (orp_monthly_avg &gt; trs_monthly_avg) {\n    orp_higher = orp_higher + 1\n  }\n  \n}\n\n\nFrom the pie chart below, we find that the monthly average for the ORP exceeds that of the TRS retirement plan a majority of the time at 105%. Further indicating that ORP may have an additional leg up on TRS by providing on average a higher monthly payment in retirement than that of TRS.\n\n\nCode\nprob_orp_greater &lt;- round((orp_higher / 200) * 100, 2)\n\nprob_trs_greater &lt;- 100 - prob_orp_greater\n\nretirement_plan &lt;- c(\"ORP\", \"TRS\")\npercentage_greater &lt;- c(prob_orp_greater, prob_trs_greater)\n\ndf_greater &lt;- data.frame(retirement_plan, percentage_greater)\n\ndf_greater |&gt;\n  mutate(percent_labels = paste(percentage_greater, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = percentage_greater, fill = retirement_plan)) +\n    geom_bar(stat = \"identity\", width = 1) +\n    geom_text(aes(label = percent_labels),\n              position = position_stack(vjust = 0.5)) +\n    coord_polar(\"y\", start = 0) +\n    theme_void() +\n    labs(title = \"Retirement Plan Monthly Payout Distribution\") +\n    scale_fill_discrete(name = \"Retirement Plan\")"
  },
  {
    "objectID": "mp04.html#data-driven-decision-recommendation",
    "href": "mp04.html#data-driven-decision-recommendation",
    "title": "MP #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data-Driven Decision Recommendation",
    "text": "Data-Driven Decision Recommendation\nThe City University of New York (CUNY) provides its new employees with a choice between two retirement plan options: the defined-benefit Teachers Retirement System pension plan or the defined-contribution Optional Retirement Plan. When CUNY employees are between retirement plans, it is important to consider the possible factors in their decision.\nIn this project, we examined a hypothetical scenario where an individual started working at CUNY in 2005 at 35 years old with a starting salary of $50,000. This employee would go on to work until 2024 at age 54. We also assumed that the individual lived to the age 77. Taking these assumptions into consideration with historical data on wage growth, inflation, US equity market returns, International equity market returns, bond returns and short-term debt returns, we simulated two scenarios.\nBased on our preliminary analysis, we found that the TRS plan provides a guaranteed steady stream of income for individuals throughout retirement. While ORP does not guarantee the same income that TRS provides, the individual’s account will grow alongside the market and could ultimately provide significant gains in the long term or could run out before the individual dies.\nAfter generating many bootstrapping samples and re-running our retirement plan simulations for a Monte Carlo analysis, we found that the TRS plan provides a higher monthly payout in early retirement stages, but the ORP gains significant traction in later stages of retirement exceeding the monthly payout of the TRS. In all scenarios, we found that the individual always had money remaining in their account even after their death indicating favorable market returns. In 52.5% of the simulations, the average monthly payout of ORP exceeded that of TRS, further indicating an additional advantage for ORP. Our predictions are suggestive, not concrete, due to the limited 20-year data set and factors considered, highlighting the need for broader data and variables to improve accuracy. Past performance is no guarantee of future results.\nBased on our analysis with bootstrap sampling and simulations of each retirement plan, I would recommend that the employee commits to the Optional Retirement Plan (ORP). While at the beginning of retirement, ORP may not meet the same monthly payouts as TRS, this ground is eventually made up over time. Also, one can consider increasing the percentage of withdrawal from their retirement accounts to optimize their monthly payouts with ORP. Being able to have the option of having funds past your death for family members is also something I believe is important to consider, with TRS there is no option for that, with ORP and a favorable market return, individuals could make even more of their money back to pass down to family. Ultimately, decisions for retirement plans will depend on each employee’s unique circumstances and preferences."
  },
  {
    "objectID": "olympics-host.html#introduction",
    "href": "olympics-host.html#introduction",
    "title": "Assessing the ‘Host Country Advantage’ in the Olympics",
    "section": "Introduction",
    "text": "Introduction\nThe Olympic Games, held every two years with alternating Summer and Winter events, unite countries for exciting athletic competition. A host country is selected years in advance by the International Olympic Committee1, and while hosting requires significant investment, the benefits–such as economic growth, tourism, and global visibility–typically outweigh the costs. Hosting has also been linked to greater Olympic success, with countries typically performing better than in non-host years.\nThis is known as the “host country advantage,” where nations typically outperform previous years, possibly due to increased resources, a larger team, and home-field advantage. Understanding this advantage is crucial to evaluating a country’s Olympic performance. So, we’ll explore: Does being the host country impact a country’s performance at the Olympics?"
  },
  {
    "objectID": "olympics-host.html#data-sources",
    "href": "olympics-host.html#data-sources",
    "title": "Assessing the ‘Host Country Advantage’ in the Olympics",
    "section": "Data Sources",
    "text": "Data Sources\n\nHelpful Packages\n\n\nCode\n# Install necessary packages\n\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"DT\")) install.packages(\"DT\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif(!require(\"stringr\")) install.packages(\"stringr\")\nif(!require(\"patchwork\")) install.packages(\"patchwork\")\nif(!require(\"gganimate\")) install.packages(\"gganimate\")\nif(!require(\"ggcorrplot\")) install.packages(\"ggcorrplot\")\nif(!require(\"leaflet\")) install.packages(\"leaflet\")\nif(!require(\"gridExtra\")) install.packages(\"gridExtra\")\nif(!require(\"tidymodels\")) install.packages(\"tidymodels\")\nif(!require(\"readr\")) install.packages(\"readr\")\nif(!require(\"vip\")) install.packages(\"vip\")\nif(!require(\"fable\")) install.packages(\"fable\")\nif(!require(\"tsibble\")) install.packages(\"tsibble\")\nif(!require(\"feasts\")) install.packages(\"feasts\")\nif(!require(\"urca\")) install.packages(\"urca\")\n\n# Load packages into R\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(RColorBrewer) # different color palette options\nlibrary(stringr)\nlibrary(patchwork) # inset plots\nlibrary(gganimate) # animated plots\nlibrary(ggcorrplot) # correlation matrices\nlibrary(leaflet)\nlibrary(gridExtra)\nlibrary(tidymodels)  # statistical modeling\nlibrary(readr)       # for importing data\nlibrary(vip)         # for variable importance plots\nlibrary(fable)  # time series analysis\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(urca)\n\n\n\n\n126 Years of Olympic Data (1896-2022)\nWe will be using a public data source from Kaggle, the 126 Years of Historical Olympic Dataset from Muhammad Ehsan. This collection of datasets contains extensive and unbiased Olympic data from the 1892 Olympic Games in Athens, Greece until the 2022 Winter Olympic Games in Beijing, China. We will only need the following data sets:\n\nOlympic Athlete Event Details: Olympic event results for athletes (e.g. name, sport).\nOlympic Country Profiles: Country National Olympic Committee (NOC) codes.\nOlympic Games Summary: Summary of Olympic Games (e.g. host country, year).\nOlympic Medal Tally History: Historical record of country Olympic medal distributions (gold, silver, bronze).\n\n\n\nCode\n# Historical Olympic \"Master\" Data (1896-2022)\n\nolympic_athlete_event_details &lt;- read_csv(\"Olympic_Athlete_Event_Details.csv\") # used for total athletes\nolympic_country_profiles &lt;- read_csv(\"Olympic_Country_Profiles.csv\") # used for naming conventions\nolympic_games_summary &lt;- read_csv(\"Olympic_Games_Summary.csv\") # used to get host nation information\nolympic_medal_tally_history &lt;- read_csv(\"Olympic_Medal_Tally_History.csv\") # used to get medal total information\n\n\n\n\nParis 2024 Summer Olympics Data\nWe would also like to include the 2024 Summer Olympic Games hosted in Paris, France, by using another Kaggle dataset, the Paris 2024 Olympic Summer Games compiled by Petro. This collection of Paris Olympics data is also an unbiased and extensive overview of the most recent Olympics. We will only need the following data sets:\n\nAthletes: 2024 Paris Summer Olympics athlete information (e.g. name, country, sport).\nMedals Total: Complete country medal counts for 2024 Paris Summer Olympics.\n\n\n\nCode\n# 2024 Paris Summer Olympics Data\n\nparis_medals_total_original &lt;- read_csv(\"medals_total.csv\") # used for medal total information\nparis_athletes_original &lt;- read_csv(\"athletes.csv\") # used for total athletes information"
  },
  {
    "objectID": "olympics-host.html#data-cleaning",
    "href": "olympics-host.html#data-cleaning",
    "title": "Assessing the ‘Host Country Advantage’ in the Olympics",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nBefore proceeding with analysis, to prevent any future difficulties we will need to bind our historical datasets with our Paris datasets.\nBelow we bind the olympic_medal_tally_history data table with the paris_medals_total data, adjusting country names and column names accordingly.\n\n\nCode\n# Paris Data - figure out what data I actually need from here\n\n# Medals Tally Data\n\nparis_edition &lt;- olympic_games_summary |&gt;\n  filter(year == 2024) |&gt;\n  pull(edition)\n\nparis_edition_id &lt;- olympic_games_summary |&gt;\n  filter(year == 2024) |&gt;\n  pull(edition_id)\n\nparis_year &lt;- 2024\n\n# Columns: country_code, country, country_long, Gold Medal, Silver Medal, Bronze Medal, Total\n# Need to adjust the country names based on the original data naming convention, will use the country profiles data to do so\n# Need to add other variables missing: edition, edition_id, and year\n# Need to rename all the columns based on the variables we have in olympic_medal_tally_history data frame\n\nparis_medals_total &lt;- paris_medals_total_original |&gt;\n  left_join(olympic_country_profiles, join_by(\"country_code\" == \"noc\")) |&gt;\n  rename(country_name = country.y) |&gt;\n  select(c(\"country_name\", \"country_code\", \"Gold Medal\", \"Silver Medal\", \"Bronze Medal\", \"Total\")) |&gt;\n  mutate(edition = paris_edition,\n         edition_id = paris_edition_id,\n         year = paris_year) |&gt;\n  rename(country = \"country_name\",\n         country_noc = country_code,\n         gold = \"Gold Medal\",\n         silver = \"Silver Medal\",\n         bronze = \"Bronze Medal\",\n         total = \"Total\")\n\n# need to add this data to our olympic_medal_tally_history data\n\nolympic_medal_tally_history &lt;- rbind(olympic_medal_tally_history, paris_medals_total)\n\n\nNext, we bind olympic_athlete_event_details with the paris_athletes_original data. I first verified that the athlete IDs were unique in each table. The tables stored data differently: olympic_athlete_event_details used a “pivot longer” format, while paris_athletes_original stored events as a string list. I used str_split_fixed(), str_length(), and substr() from stringr, along with pivot_longer(), to adjust the records.\n\n\nCode\n# Paris Olympic Athlete Data\n\n  # Issues with last 3 records \"discipline\" column -- will manually correct\n\nparis_athletes_original &lt;- paris_athletes_original |&gt;\n  mutate(disciplines = case_when(\n    code == 1972077 ~ \"['Athletics']\",\n    code == 1899865 ~ \"['Equestrian']\",\n    code == 1924402 ~ \"['Athletics']\",\n    .default = disciplines\n  )) |&gt;\n  mutate(events = case_when(\n    code == 1972077 ~ \"['4 x 400m Relay Mixed']\",\n    code == 1899865 ~ \"['Jumping Team']\",\n    code == 1924402 ~ \"['Men's 4 x 400m Relay']\",\n    .default = events\n  ))\n\n# Check if there's overlap in the Athlete IDs -- otherwise will be very hard to bind dataframes\n\n# intersect(olympic_athlete_event_details$athlete_id, paris_athletes_original$code) \n  # Returns 0: no intersect, so it's safe to bind the data frames\n\n# Next need to figure out how the olympic_athlete_event_details data frame is organized\n# Columns: edition, edition_id, country_noc, sport, event, result_id, athlete, athlete_id, pos, medal, isTeamSport, year\n  # Really only need to fill the country_noc, sport, event, athlete, athlete_id columns --&gt; as those are the only ones we will be using later\n\nparis_athletes &lt;- paris_athletes_original |&gt;\n  select(c(\"country_code\", \"disciplines\", \"events\", \"name\", \"code\")) |&gt;\n  mutate(edition = paris_edition,\n         edition_id = paris_edition_id,\n         sports_list = substr(disciplines, 2, str_length(disciplines) - 1),\n         events_list = substr(events, 2, str_length(events) - 1))\n\n# Splitting the events column\n\nparis_athletes[c(\"sport_1\", \"sport_2\")] &lt;- str_split_fixed(paris_athletes$sports_list, \", \", 2)\nparis_athletes[c(\"event_1\", \"event_2\", \"event_3\", \"event_4\", \"event_5\", \"event_6\", \"event_7\", \"event_8\")] &lt;- str_split_fixed(paris_athletes$events_list, \", \", 8)\n\nparis_athletes &lt;- paris_athletes |&gt;\n  pivot_longer(!c(\"edition\", \"edition_id\", \"country_code\", \"disciplines\", \"events\", \"events_list\", \"name\", \"code\", \"sports_list\", \"sport_1\", \"sport_2\"), names_to = \"event_number\", values_to = \"event_original\") |&gt;\n  filter(event_original != \"\") |&gt;\n  select(-c(\"events\", \"events_list\", \"event_number\")) |&gt;\n  pivot_longer(!c(\"edition\", \"edition_id\", \"country_code\", \"disciplines\", \"name\", \"code\", \"sports_list\", \"event_original\"), names_to = \"sport_number\", values_to = \"sport_original\") |&gt;\n  filter(sport_original != \"\") |&gt;\n  mutate(event = substr(event_original, 2, str_length(event_original) - 1),\n         sport = substr(sport_original, 2, str_length(sport_original) - 1)) |&gt;\n  select(-c('disciplines', \"sports_list\", \"sport_number\", \"event_original\", \"sport_original\")) |&gt;\n  mutate(result_id = NA,\n         pos = NA,\n         medal = NA,\n         isTeamSport = NA) |&gt;\n  rename(athlete_id = \"code\",\n         athlete = \"name\",\n         country_noc = country_code)\n\nolympic_athlete_event_details &lt;- rbind(olympic_athlete_event_details, paris_athletes)\n\n\nTo avoid possible inaccuracies with older data, we will focus on the Olympics from 1950 onwards. We’ll also create a season column to differentiate between Summer and Winter Olympics.\n\n\nCode\n# For remainder of analysis we will only focus on Olympics from 1950s onwards\n\nolympic_medal_tally_history &lt;- olympic_medal_tally_history |&gt;\n  filter(year &gt;= 1950) |&gt;\n  mutate(season = case_when(\n    str_detect(edition, \"Summer\") ~ \"summer\",\n    str_detect(edition, \"Winter\") ~ \"winter\"\n  ))\n\n\n\n\nCode\n# Include country names into Olympic Games Summary table using the country profile data table\n\nolympic_games_summary &lt;- left_join(olympic_games_summary, olympic_country_profiles, join_by(\"country_noc\" == \"noc\"))\n\n# For remainder of analysis we will only focus on Olympics from 1950s onwards\n\nolympic_games_summary &lt;- olympic_games_summary |&gt;\n  filter(year &gt;= 1950, year &lt;= 2024) |&gt;\n  select(-c(edition_url, country_flag_url, isHeld))\n\n# adds column differentiating type of Olympics\n\nolympic_games_summary &lt;- olympic_games_summary |&gt;\n  mutate(season = case_when(\n    str_detect(edition, \"Summer\") ~ \"summer\",\n    str_detect(edition, \"Winter\") ~ \"winter\"\n  ))\n  \nwinter_olympic_games_summary &lt;- olympic_games_summary |&gt;\n  filter(season == \"winter\")\n\nsummer_olympic_games_summary &lt;- olympic_games_summary |&gt;\n  filter(season == \"summer\")\n\n\n\n\nCode\nolympic_athlete_event_details &lt;- olympic_athlete_event_details |&gt;\n  mutate(year = as.numeric(substr(edition, 1, 4))) |&gt;\n  filter(year &gt;= 1950) |&gt;\n  mutate(season = case_when(\n    str_detect(edition, \"Summer\") ~ \"summer\",\n    str_detect(edition, \"Winter\") ~ \"winter\"\n  ))\n\nsummer_olympic_athlete_event_details &lt;- olympic_athlete_event_details |&gt;\n  filter(season == \"summer\")\n\nwinter_olympic_athlete_event_details &lt;- olympic_athlete_event_details |&gt;\n  filter(season == \"winter\")\n\n\nAs a unique aspect to our analysis, we will use a weighted total medal count metric, where the better the medal, the more weight it carries in the final total.\n\nGold Medals: 3\nSilver Medals: 2\nBronze Medals: 1\n\n\ntotal_w = (gold * 3) + (silver * 2) + (bronze * 1)\n\nWhere,\n\ntotalw: weighted total\ngold: total golds\nsilver: total silvers\nbronze: total bronzes\n\nNext, we’ll proceed with preliminary host country analysis of our Olympic data sets.\n\n\nCode\n# want to include weighted medal count (gold: 3, silver: 2, bronze: 1) and categorize data based on Olympic season\n\nolympic_medal_tally_history &lt;- olympic_medal_tally_history |&gt;\n  mutate(weighted_total = (gold*3) + (silver*2) + (bronze*1))\n\nolympic_medal_tally_history_summer &lt;- olympic_medal_tally_history |&gt;\n  filter(season == \"summer\")\n\nolympic_medal_tally_history_winter &lt;- olympic_medal_tally_history |&gt;\n  filter(season == \"winter\")"
  },
  {
    "objectID": "olympics-host.html#host-analysis",
    "href": "olympics-host.html#host-analysis",
    "title": "Assessing the ‘Host Country Advantage’ in the Olympics",
    "section": "Host Analysis",
    "text": "Host Analysis\nFirst, I would like to take a look at which countries have had the opportunity to host the Olympics since 1950.\nTo build an leaflet interactive map of Olympic host cities, I used latitude and longitude coordinates from the Facts about Olympic cities dataset by Eric Narro.\n\n\nCode\n# Latitude and Longitude for each host city\n\ncity_coord &lt;- read_csv(\"olympic_cities.csv\") |&gt;\n  select(c(\"Olympic_year\", \"Olympic_season\", \"ISO_code_mapping\", \"Latitude\", \"Longitude\"))\n\n# Joining tables to include Olympic host information with location data\n\nolympic_games_summary_coord &lt;- inner_join(olympic_games_summary, city_coord, join_by(\"year\" == \"Olympic_year\", \"season\" == \"Olympic_season\")) |&gt;\n  arrange(year) |&gt;\n  filter(!(country_noc == \"AUS\" & `ISO_code_mapping` == \"SWE\")) |&gt;\n  select(-c(\"ISO_code_mapping\"))\n\n# Summarizing host country information for each Olympic Game\n\nolympic_games_summary_city &lt;- olympic_games_summary_coord |&gt;\n  mutate(season = case_when(\n    city == \"Beijing\" ~ \"both\",\n    .default = season\n  )) |&gt;\n  group_by(city, country, season, `Latitude`, `Longitude`) |&gt;\n  summarize(total_hosted = n(),\n            host_years = as.character(list(edition))) |&gt;\n  ungroup() |&gt;\n  drop_na() |&gt;\n  mutate(popup_info = case_when(\n    substr(host_years, 1, 1) == \"c\" ~ paste(city, \", \", country, \" hosted the \", \"&lt;br/&gt;\", substr(host_years, 4, 23), \" and &lt;br/&gt; the \", substr(host_years, 28, 47)),\n    .default = paste(city, \", \", country, \" hosted the \", host_years)))\n\n\nThe interactive map below shows the cities that have hosted the Olympics. By clicking each of the points, we’ll be able to see the year(s) each city hosted.\n\n\n\n\n\n\nInteractive Map Key\n\n\n\n\nColors:\n\nBlue: Winter Olympic Hosts\nRed: Summer Olympic Hosts\nPurple: Hosted Both\n\n\n\nSize:\n\nSmaller: Less Hosted\nBigger: More Hosted\n\n\n\n\n\n\nCode\npal &lt;- colorFactor(c(\"purple\", \"red\", \"blue\"), domain = c(\"both\", \"summer\", \"winter\"))\n\n\nleaflet() |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(data = olympic_games_summary_city, \n                   lat = ~`Latitude`, \n                   lng = ~`Longitude`, \n                   radius = ~ifelse(total_hosted == 1, 2, 5),\n                   popup = ~popup_info,\n                   color = ~pal(season))\n\n\n\n\n\n\nFrom this map, we can observe that Innsbruck, Austria, Tokyo, Japan and Beijing, China are the only three cities to have hosted the Olympics twice. Innsbruck has hosted the Winter Olympics twice, while Tokyo has hosted the Summer Olympics twice. Beijing is the only city to have hosted the Winter Olympics and Summer Olympics.\nShifting our focus from host cities to host countries, below is an overview of the distribution of Olympic Games hosted by countries for the Summer and Winter Olympics. Since 1950, we find that Australia, Japan, and the United States have hosted the most Summer Olympics with two each.\n\n\nCode\nsummer_olympic_games_summary_host &lt;- summer_olympic_games_summary |&gt;\n  group_by(country, country_noc) |&gt;\n  summarize(total_hosted = n()) |&gt;\n  arrange(desc(total_hosted))\n\nsummer_hosts &lt;- summer_olympic_games_summary_host |&gt;\n  pull(country)\n\nDT::datatable(setNames(summer_olympic_games_summary_host, c(\"Country\", \"Country Code\", \"Total Olympics Hosted\")), \n              caption = \"Table 1: Summer Olympic Host Totals\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\n\n\nCode\nsummer_olympic_games_summary_host |&gt;\n  ggplot(aes(x = reorder(country_noc, -total_hosted), y = total_hosted)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Total Summer Olympics Hosted per Country (1950-2024)\",\n       x = \"Country\",\n       y = \"Host Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nFor the Winter Olympics, we also find the United States taking the top spot for most Winter Olympic Games hosted since 1950, with three Winter Games hosted 1960, 1980, 2002.\n\n\nCode\n# Table showing the number of Olympics hosted per country\n\nwinter_olympic_games_summary_host &lt;- winter_olympic_games_summary |&gt;\n  group_by(country, country_noc) |&gt;\n  summarize(total_hosted = n()) |&gt;\n  arrange(desc(total_hosted))\n\nwinter_hosts &lt;- winter_olympic_games_summary_host |&gt;\n  pull(country)\n\nDT::datatable(setNames(winter_olympic_games_summary_host, c(\"Country\", \"Country Code\", \"Total Olympics Hosted\")), \n              caption = \"Table 2: Winter Olympic Host Totals\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\n\n\nCode\nwinter_olympic_games_summary_host |&gt;\n  ggplot(aes(x = reorder(country_noc, -total_hosted), y = total_hosted)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Total Winter Olympics Hosted per Country (1950-2024)\",\n       x = \"Country\",\n       y = \"Host Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBeing that the Winter Games requires host countries to have a particular climate compatible with the events, it makes sense that only 11 countries have hosted the games, with 7 nations hosting on multiple occasions, compared to 16 countries hosting for the summer, with only 3 countries hosting more than once since 1950.\n\nMedal Progression for Hosts\nNext, we’ll look at the medal progression for host countries over time, to determine any anomalies or patterns during host years.\n\nSummer Olympics\nFrom a preliminary observation of each country’s medal progressions, the weighted medal count for each country during the host year has mostly increased from the weighted medal count from the Olympic Games prior.\n\n\nCode\n# Plotting each summer hosts' medal progression\n\nsummer_hosts_df &lt;- summer_olympic_games_summary |&gt;\n  select(c('country_noc', 'year')) |&gt;\n  mutate(host_country = country_noc,\n         host_year = year) |&gt;\n  select(c('host_country', 'host_year')) |&gt;\n  group_by(host_country) |&gt;\n  summarize(host_years = list(host_year))\n\nsummer_hosts_df_countries &lt;- summer_hosts_df |&gt;\n  pull(host_country) |&gt;\n  unique()\n\n\nmedal_count_summer_hosts &lt;- olympic_medal_tally_history_summer |&gt;\n  filter(country_noc %in% summer_hosts_df_countries) |&gt;\n  left_join(summer_hosts_df, join_by(\"country_noc\" == \"host_country\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  group_by(country_noc) |&gt;\n  mutate(host = (year %in% host_years[[1]]))\n\n# Information about year of olympics, country, and whether they were the host that year\n\nsummer_host_country_cols &lt;- medal_count_summer_hosts |&gt;\n  select(c('year', 'country', 'country_noc', 'host'))\n\nsummer_host_animate &lt;- medal_count_summer_hosts |&gt;\n  ggplot(aes(x = year, y = weighted_total)) +\n  geom_point(aes(group = seq_along(year), color = host), size = 2) +\n  geom_line(aes(group = 1)) +\n  facet_wrap(~country_noc) +\n  theme_bw() +\n  labs(title = \"Host Countries Medal Count Over Time\",\n       subtitle = \"Summer Olympics\",\n       x = \"Year\",\n       y = \"Weighted Medal Total\") +\n  scale_color_manual(name = \"Host Country\",\n                     values = c(\"FALSE\" = \"#F8766D\", \"TRUE\" = \"#619CFF\"),\n                     labels = c(\"No\", \"Yes\")) +\n  guides(color = guide_legend(reverse = TRUE)) +\n  transition_reveal(year)\n  \nanimate(summer_host_animate, duration = 20, end_pause = 30)\n\n\n\n\n\n\n\n\n\nThis is especially obvious in the 1984 Olympic Games for the US, where the country experienced a significant increase of 103.55% in their weighted medal total as compared to the previous year (from 197 to 497).\n\n\nWinter Olympics\nFor the Winter Olympics, the host country usually increased their weighted medal count during host years. One exception to this pattern was in 1988, Canada hosted and earned a weighted total of 7 as compared to their previous showing of 9, both quite low. Otherwise, each country has shown an increase in their weighted totals during host years. One massive increase was for the US in 2000 when they increased their weighted medal count by 139.29%.\n\n\nCode\n# Plotting each winter hosts' medal progression\n\nwinter_hosts_df &lt;- winter_olympic_games_summary |&gt;\n  select(c('country_noc', 'year')) |&gt;\n  mutate(host_country = country_noc,\n         host_year = year) |&gt;\n  select(c('host_country', 'host_year')) |&gt;\n  group_by(host_country) |&gt;\n  summarize(host_years = list(host_year))\n\nwinter_hosts_df_countries &lt;- winter_hosts_df |&gt;\n  pull(host_country) |&gt;\n  unique()\n\n\nmedal_count_winter_hosts &lt;- olympic_medal_tally_history_winter |&gt;\n  filter(country_noc %in% winter_hosts_df_countries) |&gt;\n  left_join(winter_hosts_df, join_by(\"country_noc\" == \"host_country\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  group_by(country_noc) |&gt;\n  mutate(host = (year %in% host_years[[1]]))\n\n# Information about year of olympics, country, and whether they were the host that year\n\nwinter_host_country_cols &lt;- medal_count_winter_hosts |&gt;\n  select(c('year', 'country', 'country_noc', 'host'))\n\nwinter_host_animate &lt;- medal_count_winter_hosts |&gt;\n  ggplot(aes(x = year, y = weighted_total)) +\n  geom_point(aes(group = seq_along(year), color = host), size = 2) +\n  geom_line(aes(group = 1)) +\n  facet_wrap(~country_noc) +\n  theme_bw() +\n  labs(title = \"Host Countries Medal Count Over Time\",\n       subtitle = \"Winter Olympics\",\n       x = \"Year\",\n       y = \"Weighted Medal Total\") +\n  scale_color_manual(name = \"Host Country\",\n                     values = c(\"FALSE\" = \"#F8766D\", \"TRUE\" = \"#619CFF\"),\n                     labels = c(\"No\", \"Yes\")) +\n  guides(color = guide_legend(reverse = TRUE)) +\n  transition_reveal(year)\n  \nanimate(winter_host_animate, duration = 20, end_pause = 30)\n\n\n\n\n\n\n\n\n\nOverall, host countries typically experience more success in their host years as compared to Olympics immediately prior.\nBelow is a quick look at a factor that may contribute to host country success, athlete participation.\n\n\n\n\n\n\nAthlete Participation\n\n\n\n\n\nTypically, host countries have additional athletes representing their country as compared to non-host years.2 This is due to the lower qualifications required for host country athletes as they are guaranteed a spot for each team sport.\n\nSummer Olympics\nThere is a general increasing trend for the amount of athletes each of these countries have sent over time. Additionally, it seems that in every host year, countries are sending more athletes compared to the years before, verifying the pattern of host countries having more athlete representation during host years.\n\n\nCode\nsummer_hosts_noc &lt;- summer_olympic_games_summary |&gt;\n  pull(country_noc) |&gt;\n  unique()\n\nsummer_total_athletes_per_country &lt;- summer_olympic_athlete_event_details |&gt;\n  filter(country_noc %in% summer_hosts_noc) |&gt;\n  group_by(year, country_noc, athlete_id) |&gt;\n  summarize(events_competed = n()) |&gt;\n  ungroup() |&gt;\n  group_by(year, country_noc) |&gt;\n  summarize(total_athletes = n()) |&gt;\n  ungroup()\n\nsummer_host_country_cols |&gt;\n  left_join(summer_total_athletes_per_country, join_by(\"year\" == \"year\", \"country_noc\" == \"country_noc\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  ggplot(aes(x = year, y = total_athletes)) +\n  geom_point(aes(color = host), size = 2) +\n  geom_line() +\n  facet_wrap(~country_noc) +\n  labs(title = \"Total Athletes Competing Over Time\",\n       subtitle = \"Summer Olympics\",\n       x = \"Year\",\n       y = \"Total Athletes\") +\n  theme_bw() +\n  scale_color_discrete(name = \"Host\") +\n  scale_color_manual(name = \"Host Country\",\n                     values = c(\"FALSE\" = \"#F8766D\", \"TRUE\" = \"#619CFF\"),\n                     labels = c(\"No\", \"Yes\")) +\n  guides(color = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\nFurthermore, from the pie charts below, we find that the athletes in every host year exceed that of the Olympics immediately prior for each host country. We also can observe that in 89% of cases, these host year athlete counts also exceed that of the following Olympics.\n\n\nCode\n# Want to check if during host years the number of athletes exceed the totals from immediately before and after the host year\n\n# Check if the athlete participation exceeds the prior Olympics\n\nsummer_less_athletes_before &lt;- summer_host_country_cols |&gt;\n  left_join(summer_total_athletes_per_country, join_by(\"year\" == \"year\", \"country_noc\" == \"country_noc\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  mutate(host_more = case_when(\n    host == \"TRUE\" ~ (total_athletes &gt; lag(total_athletes)),\n    .default = NA\n  )) |&gt;\n  filter(!is.na(host_more)) |&gt;\n  select(c(\"year\", \"country\", \"country_noc\", \"host_more\")) |&gt;\n  arrange(year)\n\n# Pie chart - athlete participation before vs during host year\n\nsummer_before_host &lt;- summer_less_athletes_before |&gt;\n  group_by(host_more) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup() |&gt;\n  mutate(total_olympics = sum(count),\n         percent = round((count / total_olympics) * 100, 2),\n         percent_labels = paste0(percent, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = percent, fill = factor(host_more, levels = c(\"TRUE\", \"FALSE\")))) +\n    geom_bar(stat = \"identity\", width = 1) +\n    geom_text(aes(label = percent_labels),\n              position = position_stack(vjust = 0.5)) +\n    coord_polar(\"y\", start = 0) +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          plot.subtitle = element_text(hjust = 0.5)) +\n    labs(subtitle = \"Before vs During Host Years\") +\n    scale_fill_manual(name = \"Greater\",\n                      values = c(\"TRUE\" = \"skyblue2\", \"FALSE\" = \"indianred2\"),\n                      labels = c(\"Host Year\", \"Previous Year\"))\n\n# Check if the athlete participation exceeds the subsequent Olympics as well\n\nsummer_less_athletes_after &lt;- summer_host_country_cols |&gt;\n  left_join(summer_total_athletes_per_country, join_by(\"year\" == \"year\", \"country_noc\" == \"country_noc\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  mutate(host_more = case_when(\n    host == \"TRUE\" ~ (total_athletes &gt; lead(total_athletes)),\n    .default = NA\n  )) |&gt;\n  filter(!is.na(host_more)) |&gt;\n  select(c(\"year\", \"country\", \"country_noc\", \"host_more\")) |&gt;\n  arrange(year)\n\n# Pie chart - athlete participation after vs during host year\n\nsummer_after_host &lt;- summer_less_athletes_after |&gt;\n  group_by(host_more) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup() |&gt;\n  mutate(total_olympics = sum(count),\n         percent = round((count / total_olympics) * 100, 2),\n         percent_labels = paste0(percent, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = percent, fill = factor(host_more, levels = c(\"TRUE\", \"FALSE\")))) +\n    geom_bar(stat = \"identity\", width = 1) +\n    geom_text(aes(label = percent_labels),\n              position = position_stack(vjust = 0.5)) +\n    coord_polar(\"y\", start = 0) +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          plot.subtitle = element_text(hjust = 0.5)) +\n    labs(subtitle = \"During vs After Host Years\") +\n    scale_fill_manual(name = \"Greater\",\n                      values = c(\"TRUE\" = \"skyblue2\", \"FALSE\" = \"indianred2\"),\n                      labels = c(\"Host Year\", \"Following Year\"))\n\ngrid.arrange(summer_before_host, summer_after_host, ncol = 2, top = \"Comparison of Athletes Sent to Summer Olympics During \\nHost Years vs Immediately Before and After\")\n\n\n\n\n\n\n\n\n\n\n\nWinter Olympics\nContrary to the summer host countries, there is a little less of an obvious extreme athlete participation for host years.\n\n\nCode\nwinter_hosts_noc &lt;- winter_olympic_games_summary |&gt;\n  pull(country_noc) |&gt;\n  unique()\n\nwinter_total_athletes_per_country &lt;- winter_olympic_athlete_event_details |&gt;\n  filter(country_noc %in% winter_hosts_noc) |&gt;\n  group_by(year, country_noc, athlete_id) |&gt;\n  summarize(events_competed = n()) |&gt;\n  ungroup() |&gt;\n  group_by(year, country_noc) |&gt;\n  summarize(total_athletes = n()) |&gt;\n  ungroup()\n\nwinter_host_country_cols |&gt;\n  left_join(winter_total_athletes_per_country, join_by(\"year\" == \"year\", \"country_noc\" == \"country_noc\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  ggplot(aes(x = year, y = total_athletes)) +\n  geom_point(aes(color = host), size = 2) +\n  geom_line() +\n  facet_wrap(~country_noc) +\n  labs(title = \"Total Athletes Competing Over Time\",\n       subtitle = \"Winter Olympics\",\n       x = \"Year\",\n       y = \"Total Athletes\") +\n  theme_bw() +\n  scale_color_discrete(name = \"Host\") +\n  scale_color_manual(name = \"Host Country\",\n                     values = c(\"FALSE\" = \"#F8766D\", \"TRUE\" = \"#619CFF\"),\n                     labels = c(\"No\", \"Yes\")) +\n  guides(color = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\nMore specifically, in the pie charts below, we find that 88% of the time the host year athlete participation exceeded that of the previous year, while only 76% of the host year athlete counts exceeded that of the following year.\n\n\nCode\n# Winter Olympics: Check if during host years the number of athletes exceed the totals from immediately before and after the host year\n\n# Check if the athlete participation exceeds the prior Winter Olympics\n\nwinter_less_athletes_before &lt;- winter_host_country_cols |&gt;\n  left_join(winter_total_athletes_per_country, join_by(\"year\" == \"year\", \"country_noc\" == \"country_noc\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  mutate(host_more = case_when(\n    host == \"TRUE\" ~ (total_athletes &gt; lag(total_athletes)),\n    .default = NA\n  )) |&gt;\n  filter(!is.na(host_more)) |&gt;\n  select(c(\"year\", \"country\", \"country_noc\", \"host_more\")) |&gt;\n  arrange(year)\n\n# Pie chart - athlete participation before vs during host year\n\nwinter_before_host &lt;- winter_less_athletes_before |&gt;\n  group_by(host_more) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup() |&gt;\n  mutate(total_olympics = sum(count),\n         percent = round((count / total_olympics) * 100, 2),\n         percent_labels = paste0(percent, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = percent, fill = factor(host_more, levels = c(\"TRUE\", \"FALSE\")))) +\n    geom_bar(stat = \"identity\", width = 1) +\n    geom_text(aes(label = percent_labels),\n              position = position_stack(vjust = 0.5)) +\n    coord_polar(\"y\", start = 0) +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          plot.subtitle = element_text(hjust = 0.5)) +\n    labs(subtitle = \"Before vs During Host Years\") +\n    scale_fill_manual(name = \"Greater\",\n                      values = c(\"TRUE\" = \"skyblue2\", \"FALSE\" = \"indianred2\"),\n                      labels = c(\"Host Year\", \"Previous Year\"))\n\n# Check if the athlete participation exceeds the subsequent Olympics as well\n\nwinter_less_athletes_after &lt;- winter_host_country_cols |&gt;\n  left_join(winter_total_athletes_per_country, join_by(\"year\" == \"year\", \"country_noc\" == \"country_noc\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  mutate(host_more = case_when(\n    host == \"TRUE\" ~ (total_athletes &gt; lead(total_athletes)),\n    .default = NA\n  )) |&gt;\n  filter(!is.na(host_more)) |&gt;\n  select(c(\"year\", \"country\", \"country_noc\", \"host_more\")) |&gt;\n  arrange(year)\n\n# Pie chart - athlete participation after vs during host year\n\nwinter_after_host &lt;- winter_less_athletes_after |&gt;\n  group_by(host_more) |&gt;\n  summarize(count = n()) |&gt;\n  ungroup() |&gt;\n  mutate(total_olympics = sum(count),\n         percent = round((count / total_olympics) * 100, 2),\n         percent_labels = paste0(percent, \"%\")) |&gt;\n  ggplot(aes(x=\"\", y = percent, fill = factor(host_more, levels = c(\"TRUE\", \"FALSE\")))) +\n    geom_bar(stat = \"identity\", width = 1) +\n    geom_text(aes(label = percent_labels),\n              position = position_stack(vjust = 0.5)) +\n    coord_polar(\"y\", start = 0) +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          plot.subtitle = element_text(hjust = 0.5)) +\n    labs(subtitle = \"During vs After Host Years\") +\n    scale_fill_manual(name = \"Greater\",\n                      values = c(\"TRUE\" = \"skyblue2\", \"FALSE\" = \"indianred2\"),\n                      labels = c(\"Host Year\", \"Following Year\"))\n\ngrid.arrange(winter_before_host, winter_after_host, ncol = 2, top = \"Comparison of Athletes Sent to Winter Olympics During \\nHost Years vs Immediately Before and After\")\n\n\n\n\n\n\n\n\n\nSo while a jump in athlete participation was more obvious for hosts of the Summer Olympics, the pattern is less evident for host countries in the Winter Olympics. Regardless, the increase in athlete participation from host countries as compared to other Olympic years is likely due to the lower barrier of entry for host nation athletes. While this doesn’t guarantee medals, it increases the likelihood of host countries earning medals as compared to other years.\nLet’s look at the correlation between total athletes competing versus the weighted total medal counts for the Olympics.\n\n\nCorrelation: Athlete Participation vs Weighted Medal Counts\nFrom these visuals, we find that athlete participation and weighted medal total have a strong, positive correlation (r=0.74). The correlation between the variables highlights the potential importance of athlete participation in driving overall medal success.\n\n\nCode\n# All metrics in one data frame to calculate correlations between each metric\n\n# Summer Olympics: total athletes sent and total medals won for each country each year\n\nsummer_total_athletes_medals &lt;- inner_join(summer_total_athletes_per_country, olympic_medal_tally_history_summer, by = c(\"year\", \"country_noc\")) |&gt;\n  select(c(\"year\", \"country_noc\", \"season\", \"total_athletes\", \"weighted_total\"))\n\n# Winter Olympics: total athletes sent and total medals won for each country each year\n\nwinter_total_athletes_medals &lt;- inner_join(winter_total_athletes_per_country, olympic_medal_tally_history_winter, by = c(\"year\", \"country_noc\")) |&gt;\n  select(c(\"year\", \"country_noc\", \"season\", \"total_athletes\", \"weighted_total\"))\n\n# Data frame combining above two data frames into one (Summer and Winter)\n\ntotal_athletes_medals &lt;- rbind(summer_total_athletes_medals, winter_total_athletes_medals)\n\n# Scatterplot: Athlete Participation vs Weighted Medal Counts\n\ntotal_athletes_medals |&gt;\n  ggplot(aes(x = total_athletes, y = weighted_total)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Relationship between Total Athletes Sent and Weighted Total Medal Counts\",\n       x = \"Total Athletes Sent to Olympics\",\n       y = \"Weighted Total Medal Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Correlation Matrix: Athletes and Weighted Total Medal Count\n\ncorr_matrix &lt;- total_athletes_medals |&gt;\n  select(c(\"total_athletes\", \"weighted_total\")) |&gt;\n  rename(`Total Athletes` = total_athletes,\n         `Weighted Total Medal Count` = weighted_total) |&gt;\n  cor() |&gt;\n  round(digits = 2)\n\n\ncorr_matrix |&gt;\n  ggcorrplot(lab = TRUE)\n\n\n\n\n\n\n\n\n\nAfter a preliminary look at the historical Olympic athlete data, we found that typically host countries send more athletes during their host years.\n\n\n\n\nSo far, we have found that typically host countries are performing better during their host years. To more confidently determine if the host country advantage phenomenon exists, we will conduct some statistical modeling below."
  },
  {
    "objectID": "olympics-host.html#statistical-modeling",
    "href": "olympics-host.html#statistical-modeling",
    "title": "Assessing the ‘Host Country Advantage’ in the Olympics",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\nWe will take several statistical models to evaluate the existence of the host country advantage. These methods allow us to more thoroughly assess whether the observed patterns are statistically significant or just a consequence of chance.\nFirst, we will employ classification models with tidymodels. More specifically, we will use the variables year, country, season, and weighted_medal_count to predict a country’s host status.\n\n\nCode\n# Create a master data frame containing variables: total medal count, country, season, host, year\n\nall_hosts &lt;- olympic_games_summary |&gt;\n  select(c('country_noc', 'year')) |&gt;\n  mutate(host_country = country_noc,\n         host_year = year) |&gt;\n  select(c('host_country', 'host_year')) |&gt;\n  group_by(host_country) |&gt;\n  summarize(host_years = list(host_year))\n\nhosts_df_countries &lt;- all_hosts |&gt;\n  pull(host_country) |&gt;\n  unique()\n\n\nmedal_count_with_hosts &lt;- olympic_medal_tally_history |&gt;\n  left_join(all_hosts, join_by(\"country_noc\" == \"host_country\")) |&gt;\n  arrange(country_noc, year) |&gt;\n  group_by(country_noc) |&gt;\n  mutate(host = (year %in% host_years[[1]])) |&gt;\n  ungroup() |&gt;\n  select(c('year', 'country_noc', 'weighted_total', 'season', 'host')) |&gt;\n  drop_na() |&gt;\n  mutate(host = as.factor(host),\n         season = as.factor(season))\n\n\n\nLogistic Regression\nFirst, we’ll fit the data into a logistic regression model. Below are the summary results of the logistic regression model.\n\n\nCode\nlr_new &lt;- logistic_reg()\nlr_fit &lt;- fit(lr_new, host ~ ., data = medal_count_with_hosts)\nlr_summary &lt;- tidy(lr_fit)\n\n\nIn this model summary, we will take a look at two values, the coefficient estimate and the p-value for the weighted_total variable.\n\n\nCode\nlr_summary_table &lt;- lr_summary |&gt;\n  arrange(p.value) |&gt;\n  mutate(estimate = round(estimate, 5),\n         std.error = round(std.error, 5),\n         statistic = round(statistic, 5),\n         p.value = round(p.value, 5))\n\nDT::datatable(setNames(lr_summary_table, c(\"Term\", \"Coefficient Estimate\", \"Standard Error\", \"Test Statistic\", \"P-Value\")), \n              caption = \"Table 3: Logistic Regression Summary\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\nUltimately, the coefficient estimate for the weighted_total variable is 0.01753. Although this is a small value, since it is a positive value, this coefficient indicates that countries have a higher weighted medal count during host years. Despite inputting over 150 variables into our model, the weighted_total variable still remains among the most significant with the smallest p-value of 0.00009. This indicates a convincing correlation between weighted total medal count and host country status not due to chance.\nNext, we will employ a random forest model to see if we get similar results for the weighted_total variable.\n\n\nRandom Forest\nFor our random forest model, we select the “best” model among several based on the area under the receiver-operating characteristic curve (ROC AUC).\n\n\nCode\nset.seed(100)\n\nsplits &lt;- initial_split(medal_count_with_hosts, strata = host)\n\n# Non-Test\n\nolympic_other &lt;- training(splits)\n\n# Test\n\nolympic_test &lt;- testing(splits)\n\nset.seed(200)\n\n# Validation Split\n\nval_set &lt;- validation_split(olympic_other,\n                            strata = host,\n                            prop = 0.80)\n# Cores\n\ncores &lt;- parallel::detectCores()\n\n# Random Forest Model\n\nrf_mod &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |&gt; \n  set_engine(\"ranger\", num.threads = cores) |&gt;\n  set_mode(\"classification\")\n\n# Random Forest Recipe (year, season, country, weighted medal counts to predict host status)\n\nrf_recipe &lt;- recipe(host ~ ., data = olympic_other)\n\n# Random Forest Workflow\n\nrf_workflow &lt;- workflow() |&gt; \n  add_model(rf_mod) |&gt; \n  add_recipe(rf_recipe)\n\n# Tuning the Random Forest Model\n\nset.seed(300)\n\nrf_res &lt;- rf_workflow |&gt;\n  tune_grid(val_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(roc_auc))\n\n# Select the best random forest model (based on roc)\n\nrf_best &lt;- rf_res |&gt;\n  select_best(metric = \"roc_auc\")\n\nrf_mtry &lt;- rf_best$mtry\nrf_min_n &lt;- rf_best$min_n\n\n\nAfter fitting the test data on this model, we plot the variable importance scores for each variable in our latest model. The weighted_total variable ranks the highest in our importance plot, with a score of about 34. This indicates that the weighted_total variable is a significant driver of the host status outcome in our model. The strength of the weighted total medal counts in the random forest suggests that host countries are often stronger performers at the Olympics.\n\n\nCode\n# Fitting our test data on our best random forest model\n\n# the last model\nlast_rf_mod &lt;- rand_forest(mtry = rf_mtry, min_n = rf_min_n, trees = 1000) |&gt; \n  set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") |&gt; \n  set_mode(\"classification\")\n\n# the last workflow\nlast_rf_workflow &lt;- \n  rf_workflow |&gt; \n  update_model(last_rf_mod)\n\n# the last fit\nset.seed(300)\n\nlast_rf_fit &lt;- last_rf_workflow |&gt; \n  last_fit(splits)\n\n# Variable Importance Plot from Random Forest Model\n\nlast_rf_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(num_features = 5) +\n  labs(title = \"Variable Importance in Random Forest Model\",\n       x = \"Variables\",\n       y = \"Importance\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIn both classification models, we found the weighted_total variable as the best predictor for our host status models, indicating a conditional correlation between the weighted medal counts and host status even after taking various variables into account.\nClassification models have limitations, one of which is that they are designed to predict outcomes rather than establish causality. In the future, I hope to incorporate additional variables and techniques to further explore the variable importance.\nFor now, to mitigate some of these limitations, we’ll run a time series analysis as a synthetic control to measure the host country effect.\n\n\nTime Series Analysis\nOur goal with the time series analysis is to emulate the Olympic Games hosted at a “neutral” site to avoid any inherent “home field advantage”. We will take each host’s historical weighted medal counts leading up to the host year and build a time series model using the fable package to estimate how that country “would” have performed if it was not the host. Then, we will compare these predicted values with the actual weighted medal counts.\nFor our time series analysis we will calculate these weighted medal count projections using an autoregressive integrated moving average (ARIMA) model. This model allows us to forecast future values while accounting for data trends and patterns.\n\nSummer Olympics\nBelow, we collect each host country’s medal count data from prior to their host year and calculate the ARIMA projected weighted medal counts for the host year. Then we compare these projections to each of the host countries’ actual performance at the Olympics during the host year(s).\n\n\nCode\n# Summer Data Prep\n\n# Data frame with the host information for all Summer Olympic Games hosts since 1950\n\nhosts_countries_summer &lt;- olympic_games_summary |&gt;\n  mutate(season = case_when(\n    str_detect(edition, \"Summer\") ~ \"summer\",\n    str_detect(edition, \"Winter\") ~ \"winter\")) |&gt;\n  filter(season == \"summer\") |&gt;\n  select(c('country_noc', 'year')) |&gt;\n  filter(year &gt;= 1950 & year &lt;= 2024) |&gt;\n  mutate(host_country = country_noc,\n         host_year = year) |&gt;\n  select(c('host_country', 'host_year'))\n\n# Number of Summer Olympic Hosts since 1950\n\nnum_host_summer &lt;- hosts_countries_summer |&gt;\n  pull(host_country) |&gt;\n  length()\n\n\n\n\nCode\n# Time Series Summer Loop\n\n# Create data frame to compare time series predictions to actual medal counts\n\ncol_names_summer &lt;- c(\"year\", \"host country\", \"forecasted\", \"actual\", \"difference\")\n\ntime_series_summer_olympics &lt;- data.frame(matrix(nrow = 0, ncol = length(col_names_summer)))\n\nno_projection_summer &lt;- character()\n\n# Loop to calculate projected medal counts for each host year for each host country and fill time_series_summer_olympics data frame with projected numbers and actual medal counts for host years\n\nfor (i in 1:num_host_summer) {\n  hosts_countries_summer_name &lt;- hosts_countries_summer$host_country\n  hosts_countries_summer_year &lt;- hosts_countries_summer$host_year\n\n  host_year_medal_count &lt;- medal_count_with_hosts |&gt;\n    filter(country_noc %in% hosts_countries_summer_name[[i]]) |&gt;\n    filter(season == \"summer\") |&gt;\n    filter(year == hosts_countries_summer_year[[i]]) |&gt;\n    pull(weighted_total)\n\n  before_host_year_df &lt;- medal_count_with_hosts |&gt;\n    filter(country_noc %in% hosts_countries_summer_name[[i]]) |&gt;\n    filter(season == \"summer\") |&gt;\n    filter(year &lt; hosts_countries_summer_year[[i]]) |&gt;\n    select(c(\"year\", \"weighted_total\"))\n  \n  # Filter out countries that only have one row of historical data\n  \n  if (nrow(before_host_year_df) &gt; 1) {\n    country_ts &lt;- as_tsibble(before_host_year_df, index = year)\n    \n    country_ts &lt;- country_ts |&gt;\n      fill_gaps()\n    \n    fit &lt;- country_ts |&gt;\n      model(arima = ARIMA((weighted_total)))\n  \n    country_forecast &lt;- fit |&gt;\n      forecast(h = \"4 years\")\n    \n    projection &lt;- round(country_forecast$.mean, 0)\n    \n    # Filtering out the host countries that we cannot project (either not enough back dated data)\n    \n    if (!is.na(projection)) {\n      \n      time_series_summer_olympics &lt;- rbind(time_series_summer_olympics, c(hosts_countries_summer_year[[i]], hosts_countries_summer_name[[i]], projection, host_year_medal_count, host_year_medal_count - projection))\n      \n    } else {\n      no_projection_summer &lt;- c(no_projection_summer, hosts_countries_summer_name[[i]])\n    }\n    \n  } else {\n    no_projection_summer &lt;- c(no_projection_summer, hosts_countries_summer_name[[i]])\n  }\n  \n}\n\ncolnames(time_series_summer_olympics) = col_names_summer\n\ntime_series_summer_olympics &lt;- time_series_summer_olympics |&gt;\n  mutate(forecasted = as.numeric(forecasted),\n         actual = as.numeric(actual),\n         difference = as.numeric(difference))\n\n\nTaking a look at the Summer Olympics table, we find that in every instance, the host country has exceeded the ARIMA model projection with a higher actual medal count during their host year. On average, these summer hosts have a 187.14% higher weighted medal count than projected.\n\n\nCode\nDT::datatable(setNames(time_series_summer_olympics, c(\"Year\", \"Host Country\", \"Time Series Forecast\", \"Actual Weighted Medal Count\", \"Difference in Count\")), \n              caption = \"Table 4: Time Series Projection for Summer Olympic Hosts\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\n\n\nWinter Olympics\n\n\nCode\n# Winter Data Prep\n\n# Data frame with the host information for all Summer Olympic Games hosts since 1950\n\nhosts_countries_winter &lt;- olympic_games_summary |&gt;\n  mutate(season = case_when(\n    str_detect(edition, \"Summer\") ~ \"summer\",\n    str_detect(edition, \"Winter\") ~ \"winter\")) |&gt;\n  filter(season == \"winter\") |&gt;\n  select(c('country_noc', 'year')) |&gt;\n  filter(year &gt;= 1950 & year &lt;= 2024) |&gt;\n  mutate(host_country = country_noc,\n         host_year = year) |&gt;\n  select(c('host_country', 'host_year'))\n\n# Number of Winter Olympics hosts since 1950\n\nnum_host_winter &lt;- hosts_countries_winter |&gt;\n  pull(host_country) |&gt;\n  length()\n\n\n\n\nCode\n# Time Series Winter Loop\n\n# Create data frame to compare time series predictions to actual medal counts\n\ncol_names_winter &lt;- c(\"year\", \"host country\", \"forecasted\", \"actual\", \"difference\")\n\ntime_series_winter_olympics &lt;- data.frame(matrix(nrow = 0, ncol = length(col_names_winter)))\n\nno_projection_winter &lt;- character()\n\n# Loop to calculate projected medal counts for each host year for each host country and fill time_series_winter_olympics data frame with projected numbers and actual medal counts for host years\n\nfor (i in 1:num_host_winter) {\n  hosts_countries_winter_name &lt;- hosts_countries_winter$host_country\n  hosts_countries_winter_year &lt;- hosts_countries_winter$host_year\n\n  host_year_medal_count &lt;- medal_count_with_hosts |&gt;\n    filter(country_noc %in% hosts_countries_winter_name[[i]]) |&gt;\n    filter(season == \"winter\") |&gt;\n    filter(year == hosts_countries_winter_year[[i]]) |&gt;\n    pull(weighted_total)\n\n  before_host_year_df &lt;- medal_count_with_hosts |&gt;\n    filter(country_noc %in% hosts_countries_winter_name[[i]]) |&gt;\n    filter(season == \"winter\") |&gt;\n    filter(year &lt; hosts_countries_winter_year[[i]]) |&gt;\n    select(c(\"year\", \"weighted_total\"))\n  \n  # Filter out countries that only have one row of historical data\n  \n  if (nrow(before_host_year_df) &gt; 1) {\n    country_ts &lt;- as_tsibble(before_host_year_df, index = year)\n    \n    country_ts &lt;- country_ts |&gt;\n      fill_gaps()\n    \n    fit &lt;- country_ts |&gt;\n      model(arima = ARIMA((weighted_total)))\n  \n    country_forecast &lt;- fit |&gt;\n      forecast(h = \"4 years\")\n    \n    # Abnormal Winter Olympics in 1992 then 1994, will need to include an adjustment to correct the output of the forecasting for the host of 1998 Olympics\n    \n    if (nrow(country_forecast) &gt; 1) {\n      \n      # We will just take the first projection\n      \n      country_forecast &lt;- country_forecast |&gt;\n        slice_head(n = 1)\n      \n    }\n    \n    projection &lt;- round(country_forecast$.mean, 0)\n    \n    # Filtering out the host countries that we cannot project (either not enough back dated data or gaps within data)\n    \n    if (!is.na(projection)) {\n      \n      time_series_winter_olympics &lt;- rbind(time_series_winter_olympics, c(hosts_countries_winter_year[[i]], hosts_countries_winter_name[[i]], projection, host_year_medal_count, host_year_medal_count - projection))\n      \n    } else {\n      no_projection_winter &lt;- c(no_projection_winter, hosts_countries_winter_name[[i]])\n    }\n    \n  } else {\n    no_projection_winter &lt;- c(no_projection_winter, hosts_countries_winter_name[[i]])\n  }\n  \n}\n\ncolnames(time_series_winter_olympics) = col_names_winter\n\ntime_series_winter_olympics &lt;- time_series_winter_olympics |&gt;\n  mutate(forecasted = as.numeric(forecasted),\n         actual = as.numeric(actual),\n         difference = as.numeric(difference))\n\n\nContrary to the Summer Olympics, there are a couple instances that the projections exceeded the actual weighted medal counts. However, in the majority of the cases (83%), the ARIMA model projected a value less than the actual total. While lower than the summer, winter hosts still have on average a 80.6% higher medal count than projected.\n\n\nCode\nDT::datatable(setNames(time_series_winter_olympics, c(\"Year\", \"Host Country\", \"Time Series Forecast\", \"Actual Weighted Medal Count\", \"Difference in Count\")), \n              caption = \"Table 5: Time Series Projection for Winter Olympic Hosts\",\n              rownames = FALSE,\n              options = list(pageLength = 10))\n\n\n\n\n\n\nBelow is a scatter plot detailing these observations more visually. Most of these points lie above the y = x dashed line, indicating that most of these Olympics host nations performed better than they “should” have in the Olympics during their host year had it been on a “neutral” playing field. Despite having a couple of points (from the Winter Olympics) below our “baseline”, we still have good evidence that being the Olympics host country leads to more success in their weighted total medal count.\n\n\nCode\ntime_series_summer_olympics &lt;- time_series_summer_olympics |&gt;\n  mutate(season = \"summer\")\n\ntime_series_winter_olympics &lt;- time_series_winter_olympics |&gt;\n  mutate(season = \"winter\")\n\ntime_series_combined &lt;- rbind(time_series_summer_olympics, time_series_winter_olympics)\n\ntime_series_combined |&gt;\n  ggplot(aes(x = forecasted, y = actual)) +\n  geom_point(aes(color = factor(season, levels = c(\"summer\", \"winter\"))), size = 2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") + # If forecasted weighted medal count = actual weighted medal count\n  labs(title = \"ARIMA Time Series Projection vs Actual Weighted Medal Counts for Host Countries\",\n       x = \"Projected Weighted Medal Count\",\n       y = \"Actual Weighted Medal Count\") +\n  scale_color_manual(name = \"Olympic Season\",\n                     values = c(\"summer\" = \"indianred2\", \"winter\" = \"skyblue2\"),\n                     labels = c(\"Summer\", \"Winter\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nOverall, in 92% of cases, the actual weighted totals exceeded the ARIMA projections, providing us with evidence that there is some positive “host effect” on the country’s weighted total medal counts."
  },
  {
    "objectID": "olympics-host.html#footnotes",
    "href": "olympics-host.html#footnotes",
    "title": "Assessing the ‘Host Country Advantage’ in the Olympics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more information on the selection process of Olympic host cities: https://www.nwahomepage.com/2024-olympics/how-does-an-olympic-host-city-get-chosen/↩︎\nMore information about possible home field advantages at the Olympics: https://fivethirtyeight.com/features/is-there-home-field-advantage-at-the-olympics/#:~:text=On%20average%2C%20there%20are%20175.8,spot%20in%20each%20team%20sport.↩︎"
  },
  {
    "objectID": "olympics-host.html#conclusion",
    "href": "olympics-host.html#conclusion",
    "title": "Assessing the ‘Host Country Advantage’ in the Olympics",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the beginning of our analysis, we found a general visual trend that host countries typically sent more athletes and performed better during their host years. Through some additional statistical modeling, we found clearer evidence of a positive correlation and association between hosting status and weighted total medal counts.\nThis analysis has some limitations, such as unaccounted variables within each host country that may also influence Olympic success (e.g., economic stability, athlete participation, athletic investment, etc.). Nevertheless, our analysis on the correlation between hosting the Olympics and more medal success cannot be ignored."
  }
]